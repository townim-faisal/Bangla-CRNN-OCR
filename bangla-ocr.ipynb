{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"bangla-ocr.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e1KzxL_BzASM","executionInfo":{"status":"ok","timestamp":1615809349305,"user_tz":-360,"elapsed":16118,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"81f890df-17d5-4e32-8d65-87e4f28307cc"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YcsywByoy65s","executionInfo":{"status":"ok","timestamp":1615809349307,"user_tz":-360,"elapsed":2823,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"4819a1d9-e500-4927-a2c1-130ea9a4c299"},"source":["%cd '/content/drive/MyDrive/Colab Notebooks/bilstm-ctc'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/bilstm-ctc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l6CwQQr1y5uf","executionInfo":{"status":"ok","timestamp":1615809350856,"user_tz":-360,"elapsed":3736,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}}},"source":["import os, sys\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from pathlib import Path\n","from collections import Counter\n","\n","import tensorflow as tf\n","# tf.config.run_functions_eagerly(True)\n","from tensorflow import keras\n","from tensorflow.keras import layers\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oWSoH-Dzy5uk","executionInfo":{"status":"ok","timestamp":1614692347228,"user_tz":-360,"elapsed":1087,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"abeca241-1c8d-4bb8-bdf0-3f539cdb0f83"},"source":["datafile = open('./Data/ICBOCR-D4/Train/Groundtruth.txt', encoding='utf8')\n","line = datafile.readline().split('@')[1].rstrip()\n","print(line, len(\"পিঞ্জরের\"))\n","line = line.encode(\"unicode-escape\").decode()\n","print(line, len(line))\n","print('প'+'ি'+'ঞ'+'্'+'জ'+'র'+'ে'+'র')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["বনের পাখির চেয়ে পিঞ্জরের পাখিটাই বেশি ছটফট করে ! সুরেন্দ্র 8\n","\\u09ac\\u09a8\\u09c7\\u09b0 \\u09aa\\u09be\\u0996\\u09bf\\u09b0 \\u099a\\u09c7\\u09df\\u09c7 \\u09aa\\u09bf\\u099e\\u09cd\\u099c\\u09b0\\u09c7\\u09b0 \\u09aa\\u09be\\u0996\\u09bf\\u099f\\u09be\\u0987 \\u09ac\\u09c7\\u09b6\\u09bf \\u099b\\u099f\\u09ab\\u099f \\u0995\\u09b0\\u09c7 ! \\u09b8\\u09c1\\u09b0\\u09c7\\u09a8\\u09cd\\u09a6\\u09cd\\u09b0 298\n","পিঞ্জরের\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3UuFx8q0y5um","executionInfo":{"status":"ok","timestamp":1615809353465,"user_tz":-360,"elapsed":2600,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"782d5d3b-cd1f-48ae-b9c8-33fe9e459c02"},"source":["train_datafile = open('./Data/ICBOCR-D4/Train/Groundtruth.txt', encoding='utf8')\n","test_datafile = open('./Data/ICBOCR-D4/Test/Groundtruth.txt', encoding='utf8')\n","train_lines = [line.rstrip() for line in train_datafile]\n","test_lines = [line.rstrip() for line in test_datafile]\n","\n","train_image_dir = \"./Data/ICBOCR-D4/Train/Line_Images/\"\n","train_image_paths = [os.path.join(train_image_dir, line.split('@')[0]) for line in train_lines]\n","train_captions = [line.split('@')[1].lstrip() for line in train_lines]\n","\n","test_image_dir = \"./Data/ICBOCR-D4/Test/Line_Images/\"\n","for i in range(len(test_lines)):\n","  try:\n","    train_captions.append(test_lines[i].split('@')[1].lstrip())\n","    train_image_paths.append(os.path.join(test_image_dir, test_lines[i].split('@')[0]))\n","  except:\n","    print(i+1, test_lines[i])\n","\n","# for caption in train_captions:\n","#   print(caption)\n","print(len(train_image_paths), len(train_captions))\n","characters = set(char for label in train_captions for char in label)\n","print(len(characters), characters)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["200 200\n","102 {\"'\", '\\t', 'য', 'ষ', 'r', '-', 'l', '”', 'গ', 'ৃ', 'o', 'y', 'ী', '4', 'ড', 'ঔ', 'া', '—', 'চ', 'হ', '–', 'ছ', 'I', '্', '়', 'h', 'ৈ', 'ু', 'স', '।', '’', 'আ', 'প', 'ড়', 'ঃ', '“', 'ম', 'ে', 'র', '৮', 'ঞ', 'অ', '৯', 'ই', 'এ', 'ঝ', 'n', 'v', 'য়', 'জ', 'e', '.', 's', 'ত', 'ঢ', 'ব', 'ভ', '৫', 'ঙ', 'ঘ', 'খ', 'ৰ', 'ধ', 'ট', '১', 'ৌ', 'w', 'থ', '\"', '?', 'm', 'ি', 'i', '৩', 'শ', 'ও', 'ং', '\\u200c', 't', 'ণ', ',', 'ূ', ';', '!', '৭', 'ল', 'ঠ', 'ো', 'দ', 'f', 'ৎ', 'ঁ', 'u', 'L', 'g', 'ক', ' ', '২', 'ফ', 'উ', 'ন', '০'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kIxkTTg7y5um","executionInfo":{"status":"ok","timestamp":1614678103092,"user_tz":-360,"elapsed":937,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"cde74b55-e41d-4946-e98f-e808a68bb70a"},"source":["f = open('./Dict/AllCharcaters.txt', encoding='utf-8')\n","lines = [line.rstrip().split(',')[1] for line in f]\n","lines = set(lines)\n","print(len(lines))\n","print(lines - characters)\n","print(characters-lines)\n","characters = lines.union(characters)\n","print(len(characters), characters)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["84\n","{'ৎ', '—', 'ঙ', '৬', '•', 'ঋ', '০', 'ড', 'ৌ', '\\u200d', 'ৗ', 'ঁ', 'ঐ', '॥', '\\u200c', 'ঈ', 'ঃ', 'ং', 'ৰ', 'ঊ', '৮', '়', '১', '৪', '‘', '৭', '৯', 'ঔ', '২', 'ঢ', '৩', '৫', '\\ufeff', 'ৱ', 'ৈ', 'ঢ়'}\n","{' ', '?', '!', ',', '-'}\n","89 {'?', 'া', '।', 'ঙ', 'চ', 'ু', 'আ', 'ঋ', '•', '০', 'ঝ', 'ূ', 'ঘ', '্', 'ো', 'ধ', 'য', 'ড', 'গ', 'ভ', 'অ', 'উ', 'ঐ', 'ঈ', 'ং', 'ৰ', '৮', 'প', 'স', 'খ', 'ট', '৪', '‘', 'ছ', 'ঠ', 'ষ', 'ঔ', '২', 'ঢ', 'ত', '\\ufeff', 'ৱ', 'ৈ', 'ম', 'ৎ', ' ', 'ি', '—', '৬', 'ও', 'হ', 'ক', 'ে', 'শ', 'ৌ', 'য়', 'দ', '\\u200d', 'ৗ', 'ল', 'জ', 'ঁ', 'থ', '!', 'ই', '॥', '\\u200c', '-', 'ঃ', 'ী', 'ঊ', 'ড়', 'এ', '–', '়', 'ঞ', '১', '৭', '৯', 'র', 'ৃ', '৩', 'ব', '৫', 'ন', ',', 'ঢ়', 'ণ', 'ফ'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0EpMcw4My5un","executionInfo":{"status":"ok","timestamp":1615809353467,"user_tz":-360,"elapsed":573,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"e6f4895e-d0ad-4175-e2df-1dc28f8485d3"},"source":["# characters= lines\n","print(\"Number of images found: \", len(train_image_paths))\n","print(\"Number of labels found: \", len(train_captions))\n","print(\"Number of unique characters: \", len(characters))\n","print(\"Characters present: \", characters)\n","\n","# Batch size for training and validation\n","batch_size = 16\n","\n","# Desired image dimensions\n","img_width = 2452#224#2452\n","img_height = 144#224#144\n","\n","# Factor by which the image is going to be downsampled\n","# by the convolutional blocks. We will be using two\n","# convolution blocks and each block will have\n","# a pooling layer which downsample the features by a factor of 2.\n","# Hence total downsampling factor would be 4.\n","downsample_factor = 4\n","\n","# Maximum length of any captcha in the dataset\n","max_length = max([len(label) for label in train_captions])\n","print(\"Maximum length: \",max_length)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Number of images found:  200\n","Number of labels found:  200\n","Number of unique characters:  102\n","Characters present:  {\"'\", '\\t', 'য', 'ষ', 'r', '-', 'l', '”', 'গ', 'ৃ', 'o', 'y', 'ী', '4', 'ড', 'ঔ', 'া', '—', 'চ', 'হ', '–', 'ছ', 'I', '্', '়', 'h', 'ৈ', 'ু', 'স', '।', '’', 'আ', 'প', 'ড়', 'ঃ', '“', 'ম', 'ে', 'র', '৮', 'ঞ', 'অ', '৯', 'ই', 'এ', 'ঝ', 'n', 'v', 'য়', 'জ', 'e', '.', 's', 'ত', 'ঢ', 'ব', 'ভ', '৫', 'ঙ', 'ঘ', 'খ', 'ৰ', 'ধ', 'ট', '১', 'ৌ', 'w', 'থ', '\"', '?', 'm', 'ি', 'i', '৩', 'শ', 'ও', 'ং', '\\u200c', 't', 'ণ', ',', 'ূ', ';', '!', '৭', 'ল', 'ঠ', 'ো', 'দ', 'f', 'ৎ', 'ঁ', 'u', 'L', 'g', 'ক', ' ', '২', 'ফ', 'উ', 'ন', '০'}\n","Maximum length:  75\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9n4F5kceseeM","executionInfo":{"status":"ok","timestamp":1615809355238,"user_tz":-360,"elapsed":574,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}}},"source":["def new_py_function(func, inp, Tout, name=None):\r\n","  def wrapped_func(*flat_inp):\r\n","    reconstructed_inp = tf.nest.pack_sequence_as(inp, flat_inp,expand_composites=True)\r\n","    out = func(*reconstructed_inp)\r\n","    return tf.nest.flatten(out, expand_composites=True)\r\n","  flat_Tout = tf.nest.flatten(Tout, expand_composites=True)\r\n","  flat_out = tf.py_function(\r\n","      func=wrapped_func, \r\n","      inp=tf.nest.flatten(inp, expand_composites=True),\r\n","      Tout=[_tensor_spec_to_dtype(v) for v in flat_Tout],\r\n","      name=name)\r\n","  spec_out = tf.nest.map_structure(_dtype_to_tensor_spec, Tout, \r\n","                                   expand_composites=True)\r\n","  out = tf.nest.pack_sequence_as(spec_out, flat_out, expand_composites=True)\r\n","  return out\r\n","\r\n","def _dtype_to_tensor_spec(v):\r\n","  return tf.TensorSpec(None, v) if isinstance(v, tf.dtypes.DType) else v\r\n","\r\n","def _tensor_spec_to_dtype(v):\r\n","  return v.dtype if isinstance(v, tf.TensorSpec) else v"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"6yNv88sMhBxN","executionInfo":{"status":"ok","timestamp":1615810514718,"user_tz":-360,"elapsed":2648,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}}},"source":["# Mapping characters to integers\n","char_to_num = layers.experimental.preprocessing.StringLookup(\n","    vocabulary=list(characters), num_oov_indices=0, mask_token=''\n",")\n","\n","# Mapping integers back to original characters\n","num_to_char = layers.experimental.preprocessing.StringLookup(\n","    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",")\n","\n","def label_to_tensor(st):\n","    f_list = list(characters)\n","    f_list.sort()\n","    st_list = []\n","    for s in st:\n","        s_index = f_list.index(s)\n","        st_list.append(s_index)\n","    return tf.convert_to_tensor(st_list, dtype=tf.float32)\n","\n","def split_data(images, labels, train_size=0.9, shuffle=True):\n","    # 1. Get the total size of the dataset\n","    size = len(images)\n","    # 2. Make an indices array and shuffle it, if required\n","    indices = np.arange(size)\n","    if shuffle:\n","        np.random.shuffle(indices)\n","    # 3. Get the size of training samples\n","    train_samples = int(size * train_size)\n","    # 4. Split data into training and validation sets\n","    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n","    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\n","    return x_train, x_valid, y_train, y_valid\n","\n","\n","# Splitting data into training and validation sets\n","x_train, x_valid, y_train, y_valid = split_data(np.array(train_image_paths), np.array(train_captions))\n","\n","\n","def encode_single_sample(img_path, label):\n","    # 1. Read image\n","    img = tf.io.read_file(img_path)\n","    # 2. Decode and convert to grayscale\n","    img = tf.io.decode_jpeg(img, channels=3)\n","    # 3. Convert to float32 in [0, 1] range\n","    img = tf.image.convert_image_dtype(img, tf.float32)\n","    # 4. Resize to the desired size\n","    img = tf.image.resize(img, [img_height, img_width])\n","    img = tf.image.per_image_standardization(img) # normalize data\n","    # 5. Transpose the image because we want the time dimension to correspond to the width of the image.\n","    img = tf.transpose(img, perm=[1, 0, 2])\n","    # 6. Map the characters in label to numbers\n","    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n","    # label = label.numpy()\n","    label = tf.keras.preprocessing.sequence.pad_sequences([label.numpy()], maxlen=max_length, padding='post').squeeze()\n","    # 7. Return a dict as our model is expecting two inputs\n","    return {\"image\": img, \"label\": label}\n","\n","def pad_map_fn(img_path, label):\n","    return new_py_function(encode_single_sample, inp=(img_path, label), Tout=({\"image\": tf.float32, \"label\": tf.int32}))"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"nLE-Vp-7AeAe"},"source":["sample_train_x, sample_train_y = [], []\r\n","for i in range(len(x_train)):\r\n","  sample = encode_single_sample(x_train[i], y_train[i])\r\n","  sample_train_x.append(np.asarray(sample['image']))\r\n","  label = np.asarray(sample['label'])\r\n","  label = np.expand_dims(label, -1)\r\n","  sample_train_y.append(label)\r\n","sample_train_x = np.asarray(sample_train_x)/255.0\r\n","sample_train_y = np.asarray(sample_train_y)\r\n","sample_train_y = np.expand_dims(sample_train_y, -1)\r\n","\r\n","print(sample_train_x.shape)\r\n","print(sample_train_y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lDFgzzLzhRue"},"source":["y_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nlKmv6cdy5uo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615810518667,"user_tz":-360,"elapsed":1737,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"da35233f-692b-4276-a5e9-75ee3e215a9f"},"source":["batch_size = 16\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","train_dataset = train_dataset.map(\n","        pad_map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n","    ).batch(batch_size)#.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","\n","validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n","validation_dataset = validation_dataset.map(\n","        pad_map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n","    ).batch(batch_size)\n","\n","dataset = train_dataset.take(1)\n","# list(dataset.as_numpy_iterator())\n","a = list(dataset.as_numpy_iterator())\n","for a in list(dataset.as_numpy_iterator()):\n","  print(a['image'].shape, a['label'].shape)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["(16, 2452, 144, 3) (16, 75)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FtVyh4ZOy5uo","executionInfo":{"status":"ok","timestamp":1615455887011,"user_tz":-360,"elapsed":861,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"f30d06c7-6d8c-454a-a189-238fc7dac703"},"source":["# dataset = train_dataset.take(1)\r\n","# # list(dataset.as_numpy_iterator())\r\n","# a = list(dataset.as_numpy_iterator())\r\n","# a[0]['image'].shape, a[0]['label'].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1, 2452, 144, 1), (1, 51))"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"UGTSQr_utfRp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615541261369,"user_tz":-360,"elapsed":857,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"321dfbb4-cabe-41c1-b390-eed1b74fdc77"},"source":["# ragged tensor\r\n","batch_size = 16\r\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n","train_dataset = train_dataset.map(\r\n","        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\r\n","    ).apply(tf.data.experimental.dense_to_ragged_batch(batch_size=batch_size))\r\n","\r\n","\r\n","validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\r\n","validation_dataset = validation_dataset.map(\r\n","        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\r\n","    ).apply(tf.data.experimental.dense_to_ragged_batch(batch_size=batch_size))\r\n","\r\n","dataset = train_dataset.take(1)\r\n","for a in list(dataset.as_numpy_iterator()):\r\n","  print(a['image'].shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(16, 2452, 144, 1)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ragged/ragged_tensor.py:2012: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return np.array(rows)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"kHKp3EwCy5up"},"source":["plt.rc('font')\n","for batch in train_dataset.take(1):\n","    images = batch[\"image\"]\n","    labels = batch[\"label\"]\n","    print(labels)\n","    for i in range(1):\n","        img = (images[i] * 255).numpy().astype(\"uint8\")\n","        print(img.shape) # (200, 50, 1)\n","        label = tf.strings.reduce_join(num_to_char(labels[i])).numpy().decode()\n","        plt.imshow(img[:, :, 0].T, cmap=\"gray\")\n","        plt.title(label)\n","        plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ocAzBnSzzRKD","executionInfo":{"status":"ok","timestamp":1614679263461,"user_tz":-360,"elapsed":983,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"a1fd7471-d8b1-42fe-c42c-783e789acbc9"},"source":["###### TEST EXAMPLE\r\n","paragraph1 = np.random.random((20, 10, 50)).astype(np.float32)\r\n","paragraph2 = np.random.random((20, 10, 50)).astype(np.float32)\r\n","paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)\r\n","\r\n","lstm_layer = layers.Bidirectional(layers.LSTM(512, return_sequences=True, dropout=0.25))\r\n","linear_layer = layers.Dense(512, activation=\"relu\", name=\"dense1\")\r\n","output = lstm_layer(paragraph1)\r\n","output = lstm_layer(paragraph2)\r\n","output = lstm_layer(paragraph3)\r\n","print(paragraph1.shape, output.shape, linear_layer(paragraph1).shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(20, 10, 50) (20, 10, 1024) (20, 10, 512)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sQNyWJVay5up","executionInfo":{"status":"ok","timestamp":1615810527670,"user_tz":-360,"elapsed":2320,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"29913489-f0fe-43b1-f59a-91742103ac75"},"source":["class CTCLayer(layers.Layer):\n","    def __init__(self, name=None):\n","        super().__init__(name=name)\n","        self.loss_fn = keras.backend.ctc_batch_cost\n","\n","    def call(self, y_true, y_pred):\n","        # Compute the training-time loss value and add it to the layer using `self.add_loss()`.\n","        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n","        input_length =  tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n","        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n","        # print(y_true.shape, y_pred.shape)\n","        # batch_len =  nrows(y_true, 'int64')\n","        # input_length = y_pred.uniform_row_length\n","        # label_length = y_true.uniform_row_length\n","\n","        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n","        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n","\n","        loss = self.loss_fn(y_true, y_pred, input_length, label_length) + 0.01\n","        self.add_loss(loss)\n","\n","        # At test time, just return the computed predictions\n","        return y_pred\n","\n","    # def loss(self, y_true, y_pred):\n","    #     \"\"\"The actual loss\"\"\"\n","\n","    #     batch_labels = y_true[:, :, 0]\n","    #     label_length = y_true[:, 0, 1]\n","    #     input_length = y_true[:, 0, 2]\n","\n","    #     #reshape for the loss, add that extra dimension\n","    #     label_length = tf.expand_dims(label_length, -1)\n","    #     input_length = tf.expand_dims(input_length, -1)\n","\n","    #     # use keras backend function for the loss\n","    #     return keras.backend.ctc_batch_cost(batch_labels, y_pred, input_length, label_length)\n","\n","\n","def build_model():\n","    # Inputs to the model\n","    input_img = layers.Input(shape=(img_width, img_height, 3), name=\"image\", dtype=\"float32\")\n","    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n","\n","    # First conv block\n","    # x = layers.Conv2D(\n","    #     32,\n","    #     (3, 3),\n","    #     activation=\"relu\",\n","    #     kernel_initializer=\"he_normal\",\n","    #     padding=\"same\",\n","    #     name=\"Conv1\",\n","    # )(input_img)\n","    # x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n","\n","    # # Second conv block\n","    # x = layers.Conv2D(\n","    #     64,\n","    #     (3, 3),\n","    #     activation=\"relu\",\n","    #     kernel_initializer=\"he_normal\",\n","    #     padding=\"same\",\n","    #     name=\"Conv2\",\n","    # )(x)\n","    # x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n","\n","    x = tf.keras.applications.VGG19(include_top=False,weights=\"imagenet\",input_tensor=input_img)(input_img)\n","    new_shape = (76*4, 512)\n","\n","\n","    # We have used two max pool with pool size and strides 2.\n","    # Hence, downsampled feature maps are 4x smaller. The number of\n","    # filters in the last layer is 64. Reshape accordingly before\n","    # passing the output to the RNN part of the model\n","    # new_shape = ((img_width // 4), (img_height // 4) * 64)\n","    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n","    # x = layers.Dense(1024, activation=\"relu\", name=\"dense1\")(x)\n","    # x = layers.Dropout(0.2)(x)\n","    # x = layers.Dense(512, activation=\"relu\", name=\"dense5\")(x)\n","    # x = layers.Dropout(0.2)(x)\n","    # x = layers.Dense(64, activation=\"relu\", name=\"dense2\")(x)\n","    # x = layers.Dropout(0.2)(x)\n","\n","    # RNNs\n","    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True, dropout=0.25))(x)\n","    # x = layers.Bidirectional(layers.LSTM(512, return_sequences=True, dropout=0.25))(x)\n","    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n","\n","    # Output layer\n","    x = layers.Dense(len(characters) + 2, activation=\"softmax\", name=\"dense3\")(x)\n","\n","    # Add CTC layer for calculating CTC loss at each step\n","    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n","\n","    # Define the model\n","    model = keras.models.Model(\n","        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n","    )\n","    # Optimizer\n","    opt = keras.optimizers.Adam(learning_rate=1e-4, decay=1e-6)\n","    # Compile the model and return\n","    model.compile(optimizer=opt)\n","    return model\n","\n","\n","# Get the model\n","model = build_model()\n","model.summary()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Model: \"ocr_model_v1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","image (InputLayer)              [(None, 2452, 144, 3 0                                            \n","__________________________________________________________________________________________________\n","vgg19 (Functional)              (None, 76, 4, 512)   20024384    image[0][0]                      \n","__________________________________________________________________________________________________\n","reshape (Reshape)               (None, 304, 512)     0           vgg19[0][0]                      \n","__________________________________________________________________________________________________\n","bidirectional_2 (Bidirectional) (None, 304, 512)     1574912     reshape[0][0]                    \n","__________________________________________________________________________________________________\n","bidirectional_3 (Bidirectional) (None, 304, 256)     656384      bidirectional_2[0][0]            \n","__________________________________________________________________________________________________\n","label (InputLayer)              [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","dense3 (Dense)                  (None, 304, 104)     26728       bidirectional_3[0][0]            \n","__________________________________________________________________________________________________\n","ctc_loss (CTCLayer)             (None, 304, 104)     0           label[0][0]                      \n","                                                                 dense3[0][0]                     \n","==================================================================================================\n","Total params: 22,282,408\n","Trainable params: 22,282,408\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ooJF5EcYhBxU","outputId":"00a3dd21-fd63-4e2c-c59d-a4c93454b5b4"},"source":["epochs = 1000\n","early_stopping_patience = 10\n","# Add early stopping\n","early_stopping = keras.callbacks.EarlyStopping(\n","    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",")\n","\n","# Train the model\n","# history = model.fit(\n","#     x=[sample_train_x, sample_train_y], \n","#     y=sample_train_y, \n","#     batch_size=batch_size,\n","#     validation_split=0.1,\n","#     epochs=epochs\n","# )\n","\n","checkpoint_filepath = 'checkpoint/vgg19/cp.ckpt'\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True,\n","    monitor='val_loss',\n","    mode='min', verbose=1,\n","    save_best_only=True)\n","\n","model.load_weights(checkpoint_filepath)\n","history = model.fit(\n","    train_dataset,\n","    validation_data=validation_dataset,\n","    epochs=epochs,\n","    callbacks=[model_checkpoint_callback]\n",")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/1000\n","12/12 [==============================] - 32s 2s/step - loss: 200.5788 - val_loss: 183.3725\n","\n","Epoch 00001: val_loss improved from inf to 183.37247, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 2/1000\n","12/12 [==============================] - 26s 2s/step - loss: 190.1182 - val_loss: 183.9106\n","\n","Epoch 00002: val_loss did not improve from 183.37247\n","Epoch 3/1000\n","12/12 [==============================] - 25s 2s/step - loss: 188.3299 - val_loss: 182.0355\n","\n","Epoch 00003: val_loss improved from 183.37247 to 182.03546, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 4/1000\n","12/12 [==============================] - 25s 2s/step - loss: 186.9123 - val_loss: 181.7336\n","\n","Epoch 00004: val_loss improved from 182.03546 to 181.73361, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 5/1000\n","12/12 [==============================] - 25s 2s/step - loss: 186.6430 - val_loss: 183.6083\n","\n","Epoch 00005: val_loss did not improve from 181.73361\n","Epoch 6/1000\n","12/12 [==============================] - 25s 2s/step - loss: 186.1421 - val_loss: 185.9570\n","\n","Epoch 00006: val_loss did not improve from 181.73361\n","Epoch 7/1000\n","12/12 [==============================] - 25s 2s/step - loss: 185.8124 - val_loss: 182.5697\n","\n","Epoch 00007: val_loss did not improve from 181.73361\n","Epoch 8/1000\n","12/12 [==============================] - 25s 2s/step - loss: 185.7390 - val_loss: 181.3034\n","\n","Epoch 00008: val_loss improved from 181.73361 to 181.30338, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 9/1000\n","12/12 [==============================] - 25s 2s/step - loss: 185.8700 - val_loss: 184.2928\n","\n","Epoch 00009: val_loss did not improve from 181.30338\n","Epoch 10/1000\n","12/12 [==============================] - 25s 2s/step - loss: 185.6217 - val_loss: 189.4526\n","\n","Epoch 00010: val_loss did not improve from 181.30338\n","Epoch 11/1000\n","12/12 [==============================] - 25s 2s/step - loss: 185.6104 - val_loss: 186.1258\n","\n","Epoch 00011: val_loss did not improve from 181.30338\n","Epoch 12/1000\n","12/12 [==============================] - 25s 2s/step - loss: 185.1727 - val_loss: 180.0091\n","\n","Epoch 00012: val_loss improved from 181.30338 to 180.00912, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 13/1000\n","12/12 [==============================] - 25s 2s/step - loss: 184.8982 - val_loss: 179.6063\n","\n","Epoch 00013: val_loss improved from 180.00912 to 179.60628, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 14/1000\n","12/12 [==============================] - 25s 2s/step - loss: 185.1207 - val_loss: 179.4014\n","\n","Epoch 00014: val_loss improved from 179.60628 to 179.40140, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 15/1000\n","12/12 [==============================] - 25s 2s/step - loss: 185.0138 - val_loss: 187.0393\n","\n","Epoch 00015: val_loss did not improve from 179.40140\n","Epoch 16/1000\n","12/12 [==============================] - 25s 2s/step - loss: 184.6283 - val_loss: 179.8511\n","\n","Epoch 00016: val_loss did not improve from 179.40140\n","Epoch 17/1000\n","12/12 [==============================] - 25s 2s/step - loss: 184.4312 - val_loss: 179.9643\n","\n","Epoch 00017: val_loss did not improve from 179.40140\n","Epoch 18/1000\n","12/12 [==============================] - 25s 2s/step - loss: 184.2918 - val_loss: 180.7881\n","\n","Epoch 00018: val_loss did not improve from 179.40140\n","Epoch 19/1000\n","12/12 [==============================] - 25s 2s/step - loss: 183.8853 - val_loss: 179.4850\n","\n","Epoch 00019: val_loss did not improve from 179.40140\n","Epoch 20/1000\n","12/12 [==============================] - 25s 2s/step - loss: 183.7821 - val_loss: 179.6079\n","\n","Epoch 00020: val_loss did not improve from 179.40140\n","Epoch 21/1000\n","12/12 [==============================] - 25s 2s/step - loss: 184.2006 - val_loss: 178.8499\n","\n","Epoch 00021: val_loss improved from 179.40140 to 178.84995, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 22/1000\n","12/12 [==============================] - 25s 2s/step - loss: 183.8798 - val_loss: 181.4046\n","\n","Epoch 00022: val_loss did not improve from 178.84995\n","Epoch 23/1000\n","12/12 [==============================] - 25s 2s/step - loss: 184.0766 - val_loss: 179.1135\n","\n","Epoch 00023: val_loss did not improve from 178.84995\n","Epoch 24/1000\n","12/12 [==============================] - 25s 2s/step - loss: 183.7555 - val_loss: 179.6424\n","\n","Epoch 00024: val_loss did not improve from 178.84995\n","Epoch 25/1000\n","12/12 [==============================] - 25s 2s/step - loss: 183.9327 - val_loss: 182.6472\n","\n","Epoch 00025: val_loss did not improve from 178.84995\n","Epoch 26/1000\n","12/12 [==============================] - 25s 2s/step - loss: 183.4706 - val_loss: 179.0064\n","\n","Epoch 00026: val_loss did not improve from 178.84995\n","Epoch 27/1000\n","12/12 [==============================] - 25s 2s/step - loss: 183.4659 - val_loss: 178.3519\n","\n","Epoch 00027: val_loss improved from 178.84995 to 178.35185, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 28/1000\n","12/12 [==============================] - 25s 2s/step - loss: 183.4069 - val_loss: 178.5728\n","\n","Epoch 00028: val_loss did not improve from 178.35185\n","Epoch 29/1000\n","12/12 [==============================] - 25s 2s/step - loss: 183.1570 - val_loss: 180.3824\n","\n","Epoch 00029: val_loss did not improve from 178.35185\n","Epoch 30/1000\n","12/12 [==============================] - 25s 2s/step - loss: 183.0355 - val_loss: 179.4695\n","\n","Epoch 00030: val_loss did not improve from 178.35185\n","Epoch 31/1000\n","12/12 [==============================] - 25s 2s/step - loss: 182.9311 - val_loss: 181.1096\n","\n","Epoch 00031: val_loss did not improve from 178.35185\n","Epoch 32/1000\n","12/12 [==============================] - 25s 2s/step - loss: 183.8546 - val_loss: 179.1511\n","\n","Epoch 00032: val_loss did not improve from 178.35185\n","Epoch 33/1000\n","12/12 [==============================] - 25s 2s/step - loss: 183.3815 - val_loss: 178.0218\n","\n","Epoch 00033: val_loss improved from 178.35185 to 178.02177, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 34/1000\n","12/12 [==============================] - 25s 2s/step - loss: 183.1323 - val_loss: 177.9586\n","\n","Epoch 00034: val_loss improved from 178.02177 to 177.95862, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 35/1000\n","12/12 [==============================] - 25s 2s/step - loss: 182.8718 - val_loss: 178.4878\n","\n","Epoch 00035: val_loss did not improve from 177.95862\n","Epoch 36/1000\n","12/12 [==============================] - 25s 2s/step - loss: 182.2390 - val_loss: 179.5305\n","\n","Epoch 00036: val_loss did not improve from 177.95862\n","Epoch 37/1000\n","12/12 [==============================] - 25s 2s/step - loss: 182.4409 - val_loss: 179.1477\n","\n","Epoch 00037: val_loss did not improve from 177.95862\n","Epoch 38/1000\n","12/12 [==============================] - 25s 2s/step - loss: 181.9160 - val_loss: 179.2908\n","\n","Epoch 00038: val_loss did not improve from 177.95862\n","Epoch 39/1000\n","12/12 [==============================] - 25s 2s/step - loss: 182.5061 - val_loss: 178.5563\n","\n","Epoch 00039: val_loss did not improve from 177.95862\n","Epoch 40/1000\n","12/12 [==============================] - 25s 2s/step - loss: 182.5699 - val_loss: 177.4211\n","\n","Epoch 00040: val_loss improved from 177.95862 to 177.42113, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 41/1000\n","12/12 [==============================] - 25s 2s/step - loss: 182.1420 - val_loss: 177.0097\n","\n","Epoch 00041: val_loss improved from 177.42113 to 177.00967, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 42/1000\n","12/12 [==============================] - 25s 2s/step - loss: 181.7076 - val_loss: 177.4198\n","\n","Epoch 00042: val_loss did not improve from 177.00967\n","Epoch 43/1000\n","12/12 [==============================] - 25s 2s/step - loss: 181.4718 - val_loss: 178.9444\n","\n","Epoch 00043: val_loss did not improve from 177.00967\n","Epoch 44/1000\n","12/12 [==============================] - 25s 2s/step - loss: 181.7092 - val_loss: 177.4891\n","\n","Epoch 00044: val_loss did not improve from 177.00967\n","Epoch 45/1000\n","12/12 [==============================] - 25s 2s/step - loss: 181.5839 - val_loss: 177.8185\n","\n","Epoch 00045: val_loss did not improve from 177.00967\n","Epoch 46/1000\n","12/12 [==============================] - 25s 2s/step - loss: 181.3078 - val_loss: 178.5251\n","\n","Epoch 00046: val_loss did not improve from 177.00967\n","Epoch 47/1000\n","12/12 [==============================] - 25s 2s/step - loss: 181.6492 - val_loss: 177.4338\n","\n","Epoch 00047: val_loss did not improve from 177.00967\n","Epoch 48/1000\n","12/12 [==============================] - 25s 2s/step - loss: 181.1811 - val_loss: 179.3210\n","\n","Epoch 00048: val_loss did not improve from 177.00967\n","Epoch 49/1000\n","12/12 [==============================] - 25s 2s/step - loss: 181.9456 - val_loss: 178.3997\n","\n","Epoch 00049: val_loss did not improve from 177.00967\n","Epoch 50/1000\n","12/12 [==============================] - 25s 2s/step - loss: 181.8943 - val_loss: 177.1584\n","\n","Epoch 00050: val_loss did not improve from 177.00967\n","Epoch 51/1000\n","12/12 [==============================] - 25s 2s/step - loss: 181.2095 - val_loss: 177.1423\n","\n","Epoch 00051: val_loss did not improve from 177.00967\n","Epoch 52/1000\n","12/12 [==============================] - 25s 2s/step - loss: 180.7714 - val_loss: 177.0036\n","\n","Epoch 00052: val_loss improved from 177.00967 to 177.00363, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 53/1000\n","12/12 [==============================] - 25s 2s/step - loss: 180.6913 - val_loss: 178.1347\n","\n","Epoch 00053: val_loss did not improve from 177.00363\n","Epoch 54/1000\n","12/12 [==============================] - 25s 2s/step - loss: 180.3567 - val_loss: 177.4812\n","\n","Epoch 00054: val_loss did not improve from 177.00363\n","Epoch 55/1000\n","12/12 [==============================] - 25s 2s/step - loss: 180.3503 - val_loss: 177.2811\n","\n","Epoch 00055: val_loss did not improve from 177.00363\n","Epoch 56/1000\n","12/12 [==============================] - 25s 2s/step - loss: 180.3961 - val_loss: 176.5531\n","\n","Epoch 00056: val_loss improved from 177.00363 to 176.55315, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 57/1000\n","12/12 [==============================] - 25s 2s/step - loss: 180.6405 - val_loss: 178.0336\n","\n","Epoch 00057: val_loss did not improve from 176.55315\n","Epoch 58/1000\n","12/12 [==============================] - 25s 2s/step - loss: 180.5191 - val_loss: 177.5162\n","\n","Epoch 00058: val_loss did not improve from 176.55315\n","Epoch 59/1000\n","12/12 [==============================] - 25s 2s/step - loss: 180.3755 - val_loss: 176.8435\n","\n","Epoch 00059: val_loss did not improve from 176.55315\n","Epoch 60/1000\n","12/12 [==============================] - 25s 2s/step - loss: 179.6387 - val_loss: 177.1910\n","\n","Epoch 00060: val_loss did not improve from 176.55315\n","Epoch 61/1000\n","12/12 [==============================] - 25s 2s/step - loss: 179.6195 - val_loss: 178.7580\n","\n","Epoch 00061: val_loss did not improve from 176.55315\n","Epoch 62/1000\n","12/12 [==============================] - 25s 2s/step - loss: 179.7115 - val_loss: 177.7385\n","\n","Epoch 00062: val_loss did not improve from 176.55315\n","Epoch 63/1000\n","12/12 [==============================] - 25s 2s/step - loss: 179.8644 - val_loss: 176.8487\n","\n","Epoch 00063: val_loss did not improve from 176.55315\n","Epoch 64/1000\n","12/12 [==============================] - 25s 2s/step - loss: 179.4518 - val_loss: 176.8884\n","\n","Epoch 00064: val_loss did not improve from 176.55315\n","Epoch 65/1000\n","12/12 [==============================] - 25s 2s/step - loss: 179.2284 - val_loss: 177.4004\n","\n","Epoch 00065: val_loss did not improve from 176.55315\n","Epoch 66/1000\n","12/12 [==============================] - 25s 2s/step - loss: 179.3967 - val_loss: 177.3316\n","\n","Epoch 00066: val_loss did not improve from 176.55315\n","Epoch 67/1000\n","12/12 [==============================] - 25s 2s/step - loss: 179.4595 - val_loss: 177.5780\n","\n","Epoch 00067: val_loss did not improve from 176.55315\n","Epoch 68/1000\n","12/12 [==============================] - 25s 2s/step - loss: 179.0277 - val_loss: 177.8431\n","\n","Epoch 00068: val_loss did not improve from 176.55315\n","Epoch 69/1000\n","12/12 [==============================] - 25s 2s/step - loss: 178.4015 - val_loss: 177.5908\n","\n","Epoch 00069: val_loss did not improve from 176.55315\n","Epoch 70/1000\n","12/12 [==============================] - 25s 2s/step - loss: 178.0196 - val_loss: 177.2805\n","\n","Epoch 00070: val_loss did not improve from 176.55315\n","Epoch 71/1000\n","12/12 [==============================] - 25s 2s/step - loss: 177.2520 - val_loss: 178.2433\n","\n","Epoch 00071: val_loss did not improve from 176.55315\n","Epoch 72/1000\n","12/12 [==============================] - 25s 2s/step - loss: 177.5675 - val_loss: 176.0812\n","\n","Epoch 00072: val_loss improved from 176.55315 to 176.08124, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 73/1000\n","12/12 [==============================] - 25s 2s/step - loss: 178.0796 - val_loss: 180.6241\n","\n","Epoch 00073: val_loss did not improve from 176.08124\n","Epoch 74/1000\n","12/12 [==============================] - 25s 2s/step - loss: 178.4272 - val_loss: 176.2600\n","\n","Epoch 00074: val_loss did not improve from 176.08124\n","Epoch 75/1000\n","12/12 [==============================] - 25s 2s/step - loss: 178.1168 - val_loss: 176.0727\n","\n","Epoch 00075: val_loss improved from 176.08124 to 176.07274, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 76/1000\n","12/12 [==============================] - 25s 2s/step - loss: 177.3558 - val_loss: 179.4837\n","\n","Epoch 00076: val_loss did not improve from 176.07274\n","Epoch 77/1000\n","12/12 [==============================] - 25s 2s/step - loss: 177.4327 - val_loss: 177.1746\n","\n","Epoch 00077: val_loss did not improve from 176.07274\n","Epoch 78/1000\n","12/12 [==============================] - 25s 2s/step - loss: 176.3884 - val_loss: 176.4720\n","\n","Epoch 00078: val_loss did not improve from 176.07274\n","Epoch 79/1000\n","12/12 [==============================] - 25s 2s/step - loss: 175.9931 - val_loss: 176.9029\n","\n","Epoch 00079: val_loss did not improve from 176.07274\n","Epoch 80/1000\n","12/12 [==============================] - 25s 2s/step - loss: 176.0536 - val_loss: 176.2870\n","\n","Epoch 00080: val_loss did not improve from 176.07274\n","Epoch 81/1000\n","12/12 [==============================] - 25s 2s/step - loss: 175.1370 - val_loss: 176.3511\n","\n","Epoch 00081: val_loss did not improve from 176.07274\n","Epoch 82/1000\n","12/12 [==============================] - 25s 2s/step - loss: 174.9041 - val_loss: 177.0290\n","\n","Epoch 00082: val_loss did not improve from 176.07274\n","Epoch 83/1000\n","12/12 [==============================] - 25s 2s/step - loss: 174.4158 - val_loss: 176.9513\n","\n","Epoch 00083: val_loss did not improve from 176.07274\n","Epoch 84/1000\n","12/12 [==============================] - 25s 2s/step - loss: 173.9398 - val_loss: 175.7436\n","\n","Epoch 00084: val_loss improved from 176.07274 to 175.74361, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 85/1000\n","12/12 [==============================] - 25s 2s/step - loss: 174.7741 - val_loss: 175.1775\n","\n","Epoch 00085: val_loss improved from 175.74361 to 175.17749, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 86/1000\n","12/12 [==============================] - 25s 2s/step - loss: 173.9220 - val_loss: 175.5684\n","\n","Epoch 00086: val_loss did not improve from 175.17749\n","Epoch 87/1000\n","12/12 [==============================] - 25s 2s/step - loss: 173.3041 - val_loss: 176.6283\n","\n","Epoch 00087: val_loss did not improve from 175.17749\n","Epoch 88/1000\n","12/12 [==============================] - 25s 2s/step - loss: 173.5755 - val_loss: 176.1982\n","\n","Epoch 00088: val_loss did not improve from 175.17749\n","Epoch 89/1000\n","12/12 [==============================] - 25s 2s/step - loss: 173.3184 - val_loss: 175.6602\n","\n","Epoch 00089: val_loss did not improve from 175.17749\n","Epoch 90/1000\n","12/12 [==============================] - 25s 2s/step - loss: 172.4389 - val_loss: 178.2651\n","\n","Epoch 00090: val_loss did not improve from 175.17749\n","Epoch 91/1000\n","12/12 [==============================] - 25s 2s/step - loss: 172.7204 - val_loss: 175.6551\n","\n","Epoch 00091: val_loss did not improve from 175.17749\n","Epoch 92/1000\n","12/12 [==============================] - 25s 2s/step - loss: 171.7355 - val_loss: 176.9934\n","\n","Epoch 00092: val_loss did not improve from 175.17749\n","Epoch 93/1000\n","12/12 [==============================] - 25s 2s/step - loss: 171.9318 - val_loss: 176.9027\n","\n","Epoch 00093: val_loss did not improve from 175.17749\n","Epoch 94/1000\n","12/12 [==============================] - 25s 2s/step - loss: 172.7932 - val_loss: 177.3479\n","\n","Epoch 00094: val_loss did not improve from 175.17749\n","Epoch 95/1000\n","12/12 [==============================] - 25s 2s/step - loss: 171.9771 - val_loss: 174.5969\n","\n","Epoch 00095: val_loss improved from 175.17749 to 174.59692, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 96/1000\n","12/12 [==============================] - 25s 2s/step - loss: 171.0103 - val_loss: 177.4350\n","\n","Epoch 00096: val_loss did not improve from 174.59692\n","Epoch 97/1000\n","12/12 [==============================] - 25s 2s/step - loss: 171.1235 - val_loss: 175.3513\n","\n","Epoch 00097: val_loss did not improve from 174.59692\n","Epoch 98/1000\n","12/12 [==============================] - 25s 2s/step - loss: 170.5895 - val_loss: 181.0828\n","\n","Epoch 00098: val_loss did not improve from 174.59692\n","Epoch 99/1000\n","12/12 [==============================] - 25s 2s/step - loss: 171.0324 - val_loss: 178.4894\n","\n","Epoch 00099: val_loss did not improve from 174.59692\n","Epoch 100/1000\n","12/12 [==============================] - 25s 2s/step - loss: 170.6597 - val_loss: 177.4532\n","\n","Epoch 00100: val_loss did not improve from 174.59692\n","Epoch 101/1000\n","12/12 [==============================] - 25s 2s/step - loss: 170.7032 - val_loss: 177.9427\n","\n","Epoch 00101: val_loss did not improve from 174.59692\n","Epoch 102/1000\n","12/12 [==============================] - 25s 2s/step - loss: 171.7511 - val_loss: 177.1557\n","\n","Epoch 00102: val_loss did not improve from 174.59692\n","Epoch 103/1000\n","12/12 [==============================] - 25s 2s/step - loss: 171.1717 - val_loss: 179.6782\n","\n","Epoch 00103: val_loss did not improve from 174.59692\n","Epoch 104/1000\n","12/12 [==============================] - 25s 2s/step - loss: 170.9270 - val_loss: 184.0788\n","\n","Epoch 00104: val_loss did not improve from 174.59692\n","Epoch 105/1000\n","12/12 [==============================] - 25s 2s/step - loss: 171.9447 - val_loss: 190.8279\n","\n","Epoch 00105: val_loss did not improve from 174.59692\n","Epoch 106/1000\n","12/12 [==============================] - 25s 2s/step - loss: 171.2047 - val_loss: 179.8951\n","\n","Epoch 00106: val_loss did not improve from 174.59692\n","Epoch 107/1000\n","12/12 [==============================] - 25s 2s/step - loss: 170.3583 - val_loss: 176.3588\n","\n","Epoch 00107: val_loss did not improve from 174.59692\n","Epoch 108/1000\n","12/12 [==============================] - 25s 2s/step - loss: 170.3618 - val_loss: 179.3976\n","\n","Epoch 00108: val_loss did not improve from 174.59692\n","Epoch 109/1000\n","12/12 [==============================] - 25s 2s/step - loss: 170.3277 - val_loss: 178.7014\n","\n","Epoch 00109: val_loss did not improve from 174.59692\n","Epoch 110/1000\n","12/12 [==============================] - 25s 2s/step - loss: 171.5000 - val_loss: 175.3126\n","\n","Epoch 00110: val_loss did not improve from 174.59692\n","Epoch 111/1000\n","12/12 [==============================] - 25s 2s/step - loss: 169.5881 - val_loss: 178.2264\n","\n","Epoch 00111: val_loss did not improve from 174.59692\n","Epoch 112/1000\n","12/12 [==============================] - 25s 2s/step - loss: 168.5835 - val_loss: 175.9108\n","\n","Epoch 00112: val_loss did not improve from 174.59692\n","Epoch 113/1000\n","12/12 [==============================] - 25s 2s/step - loss: 167.9449 - val_loss: 176.2864\n","\n","Epoch 00113: val_loss did not improve from 174.59692\n","Epoch 114/1000\n","12/12 [==============================] - 25s 2s/step - loss: 167.7863 - val_loss: 174.2356\n","\n","Epoch 00114: val_loss improved from 174.59692 to 174.23558, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 115/1000\n","12/12 [==============================] - 25s 2s/step - loss: 167.2620 - val_loss: 173.6930\n","\n","Epoch 00115: val_loss improved from 174.23558 to 173.69304, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 116/1000\n","12/12 [==============================] - 25s 2s/step - loss: 166.8780 - val_loss: 175.7242\n","\n","Epoch 00116: val_loss did not improve from 173.69304\n","Epoch 117/1000\n","12/12 [==============================] - 25s 2s/step - loss: 167.2677 - val_loss: 174.7197\n","\n","Epoch 00117: val_loss did not improve from 173.69304\n","Epoch 118/1000\n","12/12 [==============================] - 25s 2s/step - loss: 166.6504 - val_loss: 174.0876\n","\n","Epoch 00118: val_loss did not improve from 173.69304\n","Epoch 119/1000\n","12/12 [==============================] - 25s 2s/step - loss: 167.0438 - val_loss: 174.1464\n","\n","Epoch 00119: val_loss did not improve from 173.69304\n","Epoch 120/1000\n","12/12 [==============================] - 25s 2s/step - loss: 166.6726 - val_loss: 174.3933\n","\n","Epoch 00120: val_loss did not improve from 173.69304\n","Epoch 121/1000\n","12/12 [==============================] - 25s 2s/step - loss: 165.8850 - val_loss: 174.7101\n","\n","Epoch 00121: val_loss did not improve from 173.69304\n","Epoch 122/1000\n","12/12 [==============================] - 25s 2s/step - loss: 166.1850 - val_loss: 176.7956\n","\n","Epoch 00122: val_loss did not improve from 173.69304\n","Epoch 123/1000\n","12/12 [==============================] - 25s 2s/step - loss: 166.4413 - val_loss: 175.5980\n","\n","Epoch 00123: val_loss did not improve from 173.69304\n","Epoch 124/1000\n","12/12 [==============================] - 25s 2s/step - loss: 165.6465 - val_loss: 175.3477\n","\n","Epoch 00124: val_loss did not improve from 173.69304\n","Epoch 125/1000\n","12/12 [==============================] - 25s 2s/step - loss: 165.6184 - val_loss: 174.5738\n","\n","Epoch 00125: val_loss did not improve from 173.69304\n","Epoch 126/1000\n","12/12 [==============================] - 25s 2s/step - loss: 164.4169 - val_loss: 174.7057\n","\n","Epoch 00126: val_loss did not improve from 173.69304\n","Epoch 127/1000\n","12/12 [==============================] - 25s 2s/step - loss: 164.7865 - val_loss: 176.0541\n","\n","Epoch 00127: val_loss did not improve from 173.69304\n","Epoch 128/1000\n","12/12 [==============================] - 25s 2s/step - loss: 164.1534 - val_loss: 175.0519\n","\n","Epoch 00128: val_loss did not improve from 173.69304\n","Epoch 129/1000\n","12/12 [==============================] - 25s 2s/step - loss: 164.1349 - val_loss: 173.4922\n","\n","Epoch 00129: val_loss improved from 173.69304 to 173.49219, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 130/1000\n","12/12 [==============================] - 25s 2s/step - loss: 163.1109 - val_loss: 172.8958\n","\n","Epoch 00130: val_loss improved from 173.49219 to 172.89584, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 131/1000\n","12/12 [==============================] - 25s 2s/step - loss: 162.0787 - val_loss: 174.4306\n","\n","Epoch 00131: val_loss did not improve from 172.89584\n","Epoch 132/1000\n","12/12 [==============================] - 25s 2s/step - loss: 161.6468 - val_loss: 172.4290\n","\n","Epoch 00132: val_loss improved from 172.89584 to 172.42897, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 133/1000\n","12/12 [==============================] - 25s 2s/step - loss: 160.7874 - val_loss: 173.5606\n","\n","Epoch 00133: val_loss did not improve from 172.42897\n","Epoch 134/1000\n","12/12 [==============================] - 25s 2s/step - loss: 160.7530 - val_loss: 173.7922\n","\n","Epoch 00134: val_loss did not improve from 172.42897\n","Epoch 135/1000\n","12/12 [==============================] - 25s 2s/step - loss: 161.8091 - val_loss: 172.2492\n","\n","Epoch 00135: val_loss improved from 172.42897 to 172.24919, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 136/1000\n","12/12 [==============================] - 25s 2s/step - loss: 160.7846 - val_loss: 173.7429\n","\n","Epoch 00136: val_loss did not improve from 172.24919\n","Epoch 137/1000\n","12/12 [==============================] - 25s 2s/step - loss: 160.3297 - val_loss: 172.2913\n","\n","Epoch 00137: val_loss did not improve from 172.24919\n","Epoch 138/1000\n","12/12 [==============================] - 25s 2s/step - loss: 159.6052 - val_loss: 172.0501\n","\n","Epoch 00138: val_loss improved from 172.24919 to 172.05008, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 139/1000\n","12/12 [==============================] - 25s 2s/step - loss: 158.5071 - val_loss: 172.7948\n","\n","Epoch 00139: val_loss did not improve from 172.05008\n","Epoch 140/1000\n","12/12 [==============================] - 25s 2s/step - loss: 157.8000 - val_loss: 172.2616\n","\n","Epoch 00140: val_loss did not improve from 172.05008\n","Epoch 141/1000\n","12/12 [==============================] - 25s 2s/step - loss: 157.5871 - val_loss: 172.8438\n","\n","Epoch 00141: val_loss did not improve from 172.05008\n","Epoch 142/1000\n","12/12 [==============================] - 25s 2s/step - loss: 156.9838 - val_loss: 176.0655\n","\n","Epoch 00142: val_loss did not improve from 172.05008\n","Epoch 143/1000\n","12/12 [==============================] - 25s 2s/step - loss: 157.2371 - val_loss: 173.5426\n","\n","Epoch 00143: val_loss did not improve from 172.05008\n","Epoch 144/1000\n","12/12 [==============================] - 25s 2s/step - loss: 156.9466 - val_loss: 175.0660\n","\n","Epoch 00144: val_loss did not improve from 172.05008\n","Epoch 145/1000\n","12/12 [==============================] - 25s 2s/step - loss: 157.1197 - val_loss: 171.1505\n","\n","Epoch 00145: val_loss improved from 172.05008 to 171.15048, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 146/1000\n","12/12 [==============================] - 25s 2s/step - loss: 156.8052 - val_loss: 172.5763\n","\n","Epoch 00146: val_loss did not improve from 171.15048\n","Epoch 147/1000\n","12/12 [==============================] - 25s 2s/step - loss: 155.9711 - val_loss: 173.0049\n","\n","Epoch 00147: val_loss did not improve from 171.15048\n","Epoch 148/1000\n","12/12 [==============================] - 25s 2s/step - loss: 156.0041 - val_loss: 175.3456\n","\n","Epoch 00148: val_loss did not improve from 171.15048\n","Epoch 149/1000\n","12/12 [==============================] - 25s 2s/step - loss: 157.6721 - val_loss: 173.7577\n","\n","Epoch 00149: val_loss did not improve from 171.15048\n","Epoch 150/1000\n","12/12 [==============================] - 25s 2s/step - loss: 156.2760 - val_loss: 172.4315\n","\n","Epoch 00150: val_loss did not improve from 171.15048\n","Epoch 151/1000\n","12/12 [==============================] - 25s 2s/step - loss: 155.3846 - val_loss: 172.9501\n","\n","Epoch 00151: val_loss did not improve from 171.15048\n","Epoch 152/1000\n","12/12 [==============================] - 25s 2s/step - loss: 154.5052 - val_loss: 171.5110\n","\n","Epoch 00152: val_loss did not improve from 171.15048\n","Epoch 153/1000\n","12/12 [==============================] - 25s 2s/step - loss: 154.1612 - val_loss: 170.5807\n","\n","Epoch 00153: val_loss improved from 171.15048 to 170.58066, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 154/1000\n","12/12 [==============================] - 25s 2s/step - loss: 152.4083 - val_loss: 171.6685\n","\n","Epoch 00154: val_loss did not improve from 170.58066\n","Epoch 155/1000\n","12/12 [==============================] - 25s 2s/step - loss: 152.6134 - val_loss: 170.4897\n","\n","Epoch 00155: val_loss improved from 170.58066 to 170.48965, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 156/1000\n","12/12 [==============================] - 25s 2s/step - loss: 150.7684 - val_loss: 169.7751\n","\n","Epoch 00156: val_loss improved from 170.48965 to 169.77505, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 157/1000\n","12/12 [==============================] - 25s 2s/step - loss: 151.3979 - val_loss: 170.8605\n","\n","Epoch 00157: val_loss did not improve from 169.77505\n","Epoch 158/1000\n","12/12 [==============================] - 25s 2s/step - loss: 151.9564 - val_loss: 169.0388\n","\n","Epoch 00158: val_loss improved from 169.77505 to 169.03885, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 159/1000\n","12/12 [==============================] - 25s 2s/step - loss: 150.3418 - val_loss: 169.8927\n","\n","Epoch 00159: val_loss did not improve from 169.03885\n","Epoch 160/1000\n","12/12 [==============================] - 25s 2s/step - loss: 149.7691 - val_loss: 169.8997\n","\n","Epoch 00160: val_loss did not improve from 169.03885\n","Epoch 161/1000\n","12/12 [==============================] - 25s 2s/step - loss: 149.0561 - val_loss: 173.4487\n","\n","Epoch 00161: val_loss did not improve from 169.03885\n","Epoch 162/1000\n","12/12 [==============================] - 25s 2s/step - loss: 151.7457 - val_loss: 168.8263\n","\n","Epoch 00162: val_loss improved from 169.03885 to 168.82631, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 163/1000\n","12/12 [==============================] - 25s 2s/step - loss: 148.8997 - val_loss: 176.8823\n","\n","Epoch 00163: val_loss did not improve from 168.82631\n","Epoch 164/1000\n","12/12 [==============================] - 25s 2s/step - loss: 151.9114 - val_loss: 171.2522\n","\n","Epoch 00164: val_loss did not improve from 168.82631\n","Epoch 165/1000\n","12/12 [==============================] - 25s 2s/step - loss: 148.1023 - val_loss: 171.1698\n","\n","Epoch 00165: val_loss did not improve from 168.82631\n","Epoch 166/1000\n","12/12 [==============================] - 25s 2s/step - loss: 147.5220 - val_loss: 168.7375\n","\n","Epoch 00166: val_loss improved from 168.82631 to 168.73747, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 167/1000\n","12/12 [==============================] - 25s 2s/step - loss: 146.5200 - val_loss: 170.7769\n","\n","Epoch 00167: val_loss did not improve from 168.73747\n","Epoch 168/1000\n","12/12 [==============================] - 25s 2s/step - loss: 148.1881 - val_loss: 170.7441\n","\n","Epoch 00168: val_loss did not improve from 168.73747\n","Epoch 169/1000\n","12/12 [==============================] - 25s 2s/step - loss: 148.9129 - val_loss: 170.1104\n","\n","Epoch 00169: val_loss did not improve from 168.73747\n","Epoch 170/1000\n","12/12 [==============================] - 25s 2s/step - loss: 147.2672 - val_loss: 170.0113\n","\n","Epoch 00170: val_loss did not improve from 168.73747\n","Epoch 171/1000\n","12/12 [==============================] - 25s 2s/step - loss: 143.9723 - val_loss: 168.4341\n","\n","Epoch 00171: val_loss improved from 168.73747 to 168.43411, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 172/1000\n","12/12 [==============================] - 25s 2s/step - loss: 141.9387 - val_loss: 167.1993\n","\n","Epoch 00172: val_loss improved from 168.43411 to 167.19930, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 173/1000\n","12/12 [==============================] - 25s 2s/step - loss: 140.7148 - val_loss: 167.2611\n","\n","Epoch 00173: val_loss did not improve from 167.19930\n","Epoch 174/1000\n","12/12 [==============================] - 25s 2s/step - loss: 139.1638 - val_loss: 167.1276\n","\n","Epoch 00174: val_loss improved from 167.19930 to 167.12759, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 175/1000\n","12/12 [==============================] - 25s 2s/step - loss: 138.7722 - val_loss: 166.5554\n","\n","Epoch 00175: val_loss improved from 167.12759 to 166.55539, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 176/1000\n","12/12 [==============================] - 25s 2s/step - loss: 138.6817 - val_loss: 166.6545\n","\n","Epoch 00176: val_loss did not improve from 166.55539\n","Epoch 177/1000\n","12/12 [==============================] - 25s 2s/step - loss: 137.1116 - val_loss: 167.0288\n","\n","Epoch 00177: val_loss did not improve from 166.55539\n","Epoch 178/1000\n","12/12 [==============================] - 25s 2s/step - loss: 135.6781 - val_loss: 165.6642\n","\n","Epoch 00178: val_loss improved from 166.55539 to 165.66422, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 179/1000\n","12/12 [==============================] - 25s 2s/step - loss: 133.6105 - val_loss: 165.3053\n","\n","Epoch 00179: val_loss improved from 165.66422 to 165.30527, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 180/1000\n","12/12 [==============================] - 25s 2s/step - loss: 133.2289 - val_loss: 165.4972\n","\n","Epoch 00180: val_loss did not improve from 165.30527\n","Epoch 181/1000\n","12/12 [==============================] - 25s 2s/step - loss: 132.2516 - val_loss: 165.1242\n","\n","Epoch 00181: val_loss improved from 165.30527 to 165.12424, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 182/1000\n","12/12 [==============================] - 25s 2s/step - loss: 132.7184 - val_loss: 167.0665\n","\n","Epoch 00182: val_loss did not improve from 165.12424\n","Epoch 183/1000\n","12/12 [==============================] - 25s 2s/step - loss: 132.4108 - val_loss: 168.1791\n","\n","Epoch 00183: val_loss did not improve from 165.12424\n","Epoch 184/1000\n","12/12 [==============================] - 25s 2s/step - loss: 132.2065 - val_loss: 169.3116\n","\n","Epoch 00184: val_loss did not improve from 165.12424\n","Epoch 185/1000\n","12/12 [==============================] - 25s 2s/step - loss: 132.9305 - val_loss: 170.0432\n","\n","Epoch 00185: val_loss did not improve from 165.12424\n","Epoch 186/1000\n","12/12 [==============================] - 25s 2s/step - loss: 132.9027 - val_loss: 166.7158\n","\n","Epoch 00186: val_loss did not improve from 165.12424\n","Epoch 187/1000\n","12/12 [==============================] - 25s 2s/step - loss: 132.0044 - val_loss: 166.3332\n","\n","Epoch 00187: val_loss did not improve from 165.12424\n","Epoch 188/1000\n","12/12 [==============================] - 25s 2s/step - loss: 129.8560 - val_loss: 162.5085\n","\n","Epoch 00188: val_loss improved from 165.12424 to 162.50848, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 189/1000\n","12/12 [==============================] - 25s 2s/step - loss: 127.9384 - val_loss: 163.6867\n","\n","Epoch 00189: val_loss did not improve from 162.50848\n","Epoch 190/1000\n","12/12 [==============================] - 25s 2s/step - loss: 126.0042 - val_loss: 162.9667\n","\n","Epoch 00190: val_loss did not improve from 162.50848\n","Epoch 191/1000\n","12/12 [==============================] - 25s 2s/step - loss: 124.0414 - val_loss: 162.7020\n","\n","Epoch 00191: val_loss did not improve from 162.50848\n","Epoch 192/1000\n","12/12 [==============================] - 25s 2s/step - loss: 122.7771 - val_loss: 162.5162\n","\n","Epoch 00192: val_loss did not improve from 162.50848\n","Epoch 193/1000\n","12/12 [==============================] - 25s 2s/step - loss: 121.8580 - val_loss: 164.8993\n","\n","Epoch 00193: val_loss did not improve from 162.50848\n","Epoch 194/1000\n","12/12 [==============================] - 25s 2s/step - loss: 120.9664 - val_loss: 162.8159\n","\n","Epoch 00194: val_loss did not improve from 162.50848\n","Epoch 195/1000\n","12/12 [==============================] - 25s 2s/step - loss: 120.0635 - val_loss: 163.0336\n","\n","Epoch 00195: val_loss did not improve from 162.50848\n","Epoch 196/1000\n","12/12 [==============================] - 25s 2s/step - loss: 119.3614 - val_loss: 162.1811\n","\n","Epoch 00196: val_loss improved from 162.50848 to 162.18114, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 197/1000\n","12/12 [==============================] - 25s 2s/step - loss: 118.5333 - val_loss: 160.2463\n","\n","Epoch 00197: val_loss improved from 162.18114 to 160.24626, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 198/1000\n","12/12 [==============================] - 25s 2s/step - loss: 116.5251 - val_loss: 160.3586\n","\n","Epoch 00198: val_loss did not improve from 160.24626\n","Epoch 199/1000\n","12/12 [==============================] - 25s 2s/step - loss: 116.2315 - val_loss: 162.1615\n","\n","Epoch 00199: val_loss did not improve from 160.24626\n","Epoch 200/1000\n","12/12 [==============================] - 25s 2s/step - loss: 115.9355 - val_loss: 163.0268\n","\n","Epoch 00200: val_loss did not improve from 160.24626\n","Epoch 201/1000\n","12/12 [==============================] - 25s 2s/step - loss: 114.2161 - val_loss: 162.6895\n","\n","Epoch 00201: val_loss did not improve from 160.24626\n","Epoch 202/1000\n","12/12 [==============================] - 25s 2s/step - loss: 113.7367 - val_loss: 160.1835\n","\n","Epoch 00202: val_loss improved from 160.24626 to 160.18350, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 203/1000\n","12/12 [==============================] - 25s 2s/step - loss: 113.5268 - val_loss: 160.4786\n","\n","Epoch 00203: val_loss did not improve from 160.18350\n","Epoch 204/1000\n","12/12 [==============================] - 25s 2s/step - loss: 111.2159 - val_loss: 159.7766\n","\n","Epoch 00204: val_loss improved from 160.18350 to 159.77658, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 205/1000\n","12/12 [==============================] - 25s 2s/step - loss: 109.6500 - val_loss: 159.1903\n","\n","Epoch 00205: val_loss improved from 159.77658 to 159.19029, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 206/1000\n","12/12 [==============================] - 25s 2s/step - loss: 108.0937 - val_loss: 159.0462\n","\n","Epoch 00206: val_loss improved from 159.19029 to 159.04623, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 207/1000\n","12/12 [==============================] - 25s 2s/step - loss: 107.5129 - val_loss: 159.9157\n","\n","Epoch 00207: val_loss did not improve from 159.04623\n","Epoch 208/1000\n","12/12 [==============================] - 25s 2s/step - loss: 106.6034 - val_loss: 158.4636\n","\n","Epoch 00208: val_loss improved from 159.04623 to 158.46356, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 209/1000\n","12/12 [==============================] - 25s 2s/step - loss: 105.3131 - val_loss: 160.8526\n","\n","Epoch 00209: val_loss did not improve from 158.46356\n","Epoch 210/1000\n","12/12 [==============================] - 25s 2s/step - loss: 104.5028 - val_loss: 159.9658\n","\n","Epoch 00210: val_loss did not improve from 158.46356\n","Epoch 211/1000\n","12/12 [==============================] - 25s 2s/step - loss: 103.6135 - val_loss: 162.2198\n","\n","Epoch 00211: val_loss did not improve from 158.46356\n","Epoch 212/1000\n","12/12 [==============================] - 25s 2s/step - loss: 102.4451 - val_loss: 158.9324\n","\n","Epoch 00212: val_loss did not improve from 158.46356\n","Epoch 213/1000\n","12/12 [==============================] - 25s 2s/step - loss: 101.5434 - val_loss: 158.5696\n","\n","Epoch 00213: val_loss did not improve from 158.46356\n","Epoch 214/1000\n","12/12 [==============================] - 25s 2s/step - loss: 99.3061 - val_loss: 157.2277\n","\n","Epoch 00214: val_loss improved from 158.46356 to 157.22772, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 215/1000\n","12/12 [==============================] - 25s 2s/step - loss: 97.6350 - val_loss: 159.7321\n","\n","Epoch 00215: val_loss did not improve from 157.22772\n","Epoch 216/1000\n","12/12 [==============================] - 25s 2s/step - loss: 96.3387 - val_loss: 157.5026\n","\n","Epoch 00216: val_loss did not improve from 157.22772\n","Epoch 217/1000\n","12/12 [==============================] - 25s 2s/step - loss: 95.2187 - val_loss: 155.4109\n","\n","Epoch 00217: val_loss improved from 157.22772 to 155.41092, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 218/1000\n","12/12 [==============================] - 25s 2s/step - loss: 93.6914 - val_loss: 155.8405\n","\n","Epoch 00218: val_loss did not improve from 155.41092\n","Epoch 219/1000\n","12/12 [==============================] - 25s 2s/step - loss: 92.3756 - val_loss: 153.8323\n","\n","Epoch 00219: val_loss improved from 155.41092 to 153.83226, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 220/1000\n","12/12 [==============================] - 25s 2s/step - loss: 90.7358 - val_loss: 155.1101\n","\n","Epoch 00220: val_loss did not improve from 153.83226\n","Epoch 221/1000\n","12/12 [==============================] - 25s 2s/step - loss: 89.5414 - val_loss: 155.2415\n","\n","Epoch 00221: val_loss did not improve from 153.83226\n","Epoch 222/1000\n","12/12 [==============================] - 25s 2s/step - loss: 87.8188 - val_loss: 155.5454\n","\n","Epoch 00222: val_loss did not improve from 153.83226\n","Epoch 223/1000\n","12/12 [==============================] - 25s 2s/step - loss: 86.8987 - val_loss: 156.9834\n","\n","Epoch 00223: val_loss did not improve from 153.83226\n","Epoch 224/1000\n","12/12 [==============================] - 25s 2s/step - loss: 86.2592 - val_loss: 155.2459\n","\n","Epoch 00224: val_loss did not improve from 153.83226\n","Epoch 225/1000\n","12/12 [==============================] - 25s 2s/step - loss: 85.0210 - val_loss: 155.8023\n","\n","Epoch 00225: val_loss did not improve from 153.83226\n","Epoch 226/1000\n","12/12 [==============================] - 25s 2s/step - loss: 83.7289 - val_loss: 155.4928\n","\n","Epoch 00226: val_loss did not improve from 153.83226\n","Epoch 227/1000\n","12/12 [==============================] - 25s 2s/step - loss: 84.0517 - val_loss: 155.6675\n","\n","Epoch 00227: val_loss did not improve from 153.83226\n","Epoch 228/1000\n","12/12 [==============================] - 25s 2s/step - loss: 82.3505 - val_loss: 151.6523\n","\n","Epoch 00228: val_loss improved from 153.83226 to 151.65228, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 229/1000\n","12/12 [==============================] - 25s 2s/step - loss: 80.5256 - val_loss: 156.6338\n","\n","Epoch 00229: val_loss did not improve from 151.65228\n","Epoch 230/1000\n","12/12 [==============================] - 25s 2s/step - loss: 80.4323 - val_loss: 157.7122\n","\n","Epoch 00230: val_loss did not improve from 151.65228\n","Epoch 231/1000\n","12/12 [==============================] - 25s 2s/step - loss: 80.2273 - val_loss: 157.7240\n","\n","Epoch 00231: val_loss did not improve from 151.65228\n","Epoch 232/1000\n","12/12 [==============================] - 25s 2s/step - loss: 78.1839 - val_loss: 154.9520\n","\n","Epoch 00232: val_loss did not improve from 151.65228\n","Epoch 233/1000\n","12/12 [==============================] - 25s 2s/step - loss: 76.4710 - val_loss: 155.8708\n","\n","Epoch 00233: val_loss did not improve from 151.65228\n","Epoch 234/1000\n","12/12 [==============================] - 25s 2s/step - loss: 74.4309 - val_loss: 152.1225\n","\n","Epoch 00234: val_loss did not improve from 151.65228\n","Epoch 235/1000\n","12/12 [==============================] - 25s 2s/step - loss: 71.5584 - val_loss: 153.7489\n","\n","Epoch 00235: val_loss did not improve from 151.65228\n","Epoch 236/1000\n","12/12 [==============================] - 25s 2s/step - loss: 71.5586 - val_loss: 153.3289\n","\n","Epoch 00236: val_loss did not improve from 151.65228\n","Epoch 237/1000\n","12/12 [==============================] - 25s 2s/step - loss: 69.0747 - val_loss: 152.8014\n","\n","Epoch 00237: val_loss did not improve from 151.65228\n","Epoch 238/1000\n","12/12 [==============================] - 25s 2s/step - loss: 69.6208 - val_loss: 154.3080\n","\n","Epoch 00238: val_loss did not improve from 151.65228\n","Epoch 239/1000\n","12/12 [==============================] - 25s 2s/step - loss: 69.4555 - val_loss: 159.4225\n","\n","Epoch 00239: val_loss did not improve from 151.65228\n","Epoch 240/1000\n","12/12 [==============================] - 25s 2s/step - loss: 70.1435 - val_loss: 162.2099\n","\n","Epoch 00240: val_loss did not improve from 151.65228\n","Epoch 241/1000\n","12/12 [==============================] - 25s 2s/step - loss: 69.6869 - val_loss: 152.6644\n","\n","Epoch 00241: val_loss did not improve from 151.65228\n","Epoch 242/1000\n","12/12 [==============================] - 25s 2s/step - loss: 67.4904 - val_loss: 148.4188\n","\n","Epoch 00242: val_loss improved from 151.65228 to 148.41882, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 243/1000\n","12/12 [==============================] - 25s 2s/step - loss: 63.9042 - val_loss: 150.5690\n","\n","Epoch 00243: val_loss did not improve from 148.41882\n","Epoch 244/1000\n","12/12 [==============================] - 25s 2s/step - loss: 60.4331 - val_loss: 150.7119\n","\n","Epoch 00244: val_loss did not improve from 148.41882\n","Epoch 245/1000\n","12/12 [==============================] - 25s 2s/step - loss: 57.1396 - val_loss: 147.6314\n","\n","Epoch 00245: val_loss improved from 148.41882 to 147.63142, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 246/1000\n","12/12 [==============================] - 25s 2s/step - loss: 55.2147 - val_loss: 150.1912\n","\n","Epoch 00246: val_loss did not improve from 147.63142\n","Epoch 247/1000\n","12/12 [==============================] - 25s 2s/step - loss: 54.1483 - val_loss: 152.5427\n","\n","Epoch 00247: val_loss did not improve from 147.63142\n","Epoch 248/1000\n","12/12 [==============================] - 25s 2s/step - loss: 53.6585 - val_loss: 149.4749\n","\n","Epoch 00248: val_loss did not improve from 147.63142\n","Epoch 249/1000\n","12/12 [==============================] - 25s 2s/step - loss: 51.9614 - val_loss: 148.7024\n","\n","Epoch 00249: val_loss did not improve from 147.63142\n","Epoch 250/1000\n","12/12 [==============================] - 25s 2s/step - loss: 52.2912 - val_loss: 150.6381\n","\n","Epoch 00250: val_loss did not improve from 147.63142\n","Epoch 251/1000\n","12/12 [==============================] - 25s 2s/step - loss: 51.5443 - val_loss: 149.0665\n","\n","Epoch 00251: val_loss did not improve from 147.63142\n","Epoch 252/1000\n","12/12 [==============================] - 25s 2s/step - loss: 49.5655 - val_loss: 146.7196\n","\n","Epoch 00252: val_loss improved from 147.63142 to 146.71956, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 253/1000\n","12/12 [==============================] - 25s 2s/step - loss: 47.1088 - val_loss: 148.6646\n","\n","Epoch 00253: val_loss did not improve from 146.71956\n","Epoch 254/1000\n","12/12 [==============================] - 25s 2s/step - loss: 45.6342 - val_loss: 147.0932\n","\n","Epoch 00254: val_loss did not improve from 146.71956\n","Epoch 255/1000\n","12/12 [==============================] - 25s 2s/step - loss: 43.4977 - val_loss: 148.0217\n","\n","Epoch 00255: val_loss did not improve from 146.71956\n","Epoch 256/1000\n","12/12 [==============================] - 25s 2s/step - loss: 43.1656 - val_loss: 146.8428\n","\n","Epoch 00256: val_loss did not improve from 146.71956\n","Epoch 257/1000\n","12/12 [==============================] - 25s 2s/step - loss: 41.0972 - val_loss: 151.1182\n","\n","Epoch 00257: val_loss did not improve from 146.71956\n","Epoch 258/1000\n","12/12 [==============================] - 25s 2s/step - loss: 40.9943 - val_loss: 146.3914\n","\n","Epoch 00258: val_loss improved from 146.71956 to 146.39136, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 259/1000\n","12/12 [==============================] - 25s 2s/step - loss: 39.2364 - val_loss: 146.8647\n","\n","Epoch 00259: val_loss did not improve from 146.39136\n","Epoch 260/1000\n","12/12 [==============================] - 25s 2s/step - loss: 39.2528 - val_loss: 144.7511\n","\n","Epoch 00260: val_loss improved from 146.39136 to 144.75113, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 261/1000\n","12/12 [==============================] - 25s 2s/step - loss: 37.3997 - val_loss: 147.2188\n","\n","Epoch 00261: val_loss did not improve from 144.75113\n","Epoch 262/1000\n","12/12 [==============================] - 25s 2s/step - loss: 36.3472 - val_loss: 147.6133\n","\n","Epoch 00262: val_loss did not improve from 144.75113\n","Epoch 263/1000\n","12/12 [==============================] - 25s 2s/step - loss: 35.1720 - val_loss: 144.0122\n","\n","Epoch 00263: val_loss improved from 144.75113 to 144.01224, saving model to checkpoint/vgg19/cp.ckpt\n","Epoch 264/1000\n","12/12 [==============================] - 25s 2s/step - loss: 33.5742 - val_loss: 145.0408\n","\n","Epoch 00264: val_loss did not improve from 144.01224\n","Epoch 265/1000\n","12/12 [==============================] - 25s 2s/step - loss: 32.3547 - val_loss: 144.2243\n","\n","Epoch 00265: val_loss did not improve from 144.01224\n","Epoch 266/1000\n","12/12 [==============================] - 25s 2s/step - loss: 31.1494 - val_loss: 146.2576\n","\n","Epoch 00266: val_loss did not improve from 144.01224\n","Epoch 267/1000\n","12/12 [==============================] - 25s 2s/step - loss: 30.3352 - val_loss: 145.5154\n","\n","Epoch 00267: val_loss did not improve from 144.01224\n","Epoch 268/1000\n","12/12 [==============================] - 25s 2s/step - loss: 29.2984 - val_loss: 145.4948\n","\n","Epoch 00268: val_loss did not improve from 144.01224\n","Epoch 269/1000\n","12/12 [==============================] - 25s 2s/step - loss: 28.2087 - val_loss: 146.7780\n","\n","Epoch 00269: val_loss did not improve from 144.01224\n","Epoch 270/1000\n","12/12 [==============================] - 25s 2s/step - loss: 27.8013 - val_loss: 146.0949\n","\n","Epoch 00270: val_loss did not improve from 144.01224\n","Epoch 271/1000\n","12/12 [==============================] - 25s 2s/step - loss: 26.8892 - val_loss: 145.7769\n","\n","Epoch 00271: val_loss did not improve from 144.01224\n","Epoch 272/1000\n","12/12 [==============================] - 25s 2s/step - loss: 26.3181 - val_loss: 146.5290\n","\n","Epoch 00272: val_loss did not improve from 144.01224\n","Epoch 273/1000\n","12/12 [==============================] - 25s 2s/step - loss: 25.1987 - val_loss: 145.7322\n","\n","Epoch 00273: val_loss did not improve from 144.01224\n","Epoch 274/1000\n","12/12 [==============================] - 25s 2s/step - loss: 24.5865 - val_loss: 145.6484\n","\n","Epoch 00274: val_loss did not improve from 144.01224\n","Epoch 275/1000\n","12/12 [==============================] - 25s 2s/step - loss: 23.8430 - val_loss: 147.8253\n","\n","Epoch 00275: val_loss did not improve from 144.01224\n","Epoch 276/1000\n","12/12 [==============================] - 25s 2s/step - loss: 23.0689 - val_loss: 146.0370\n","\n","Epoch 00276: val_loss did not improve from 144.01224\n","Epoch 277/1000\n","12/12 [==============================] - 25s 2s/step - loss: 22.3469 - val_loss: 146.7587\n","\n","Epoch 00277: val_loss did not improve from 144.01224\n","Epoch 278/1000\n","12/12 [==============================] - 25s 2s/step - loss: 21.7372 - val_loss: 150.1108\n","\n","Epoch 00278: val_loss did not improve from 144.01224\n","Epoch 279/1000\n","12/12 [==============================] - 25s 2s/step - loss: 21.0794 - val_loss: 147.7234\n","\n","Epoch 00279: val_loss did not improve from 144.01224\n","Epoch 280/1000\n","12/12 [==============================] - 25s 2s/step - loss: 20.4711 - val_loss: 149.4575\n","\n","Epoch 00280: val_loss did not improve from 144.01224\n","Epoch 281/1000\n","12/12 [==============================] - 25s 2s/step - loss: 19.8771 - val_loss: 148.7148\n","\n","Epoch 00281: val_loss did not improve from 144.01224\n","Epoch 282/1000\n","12/12 [==============================] - 25s 2s/step - loss: 19.1061 - val_loss: 149.3592\n","\n","Epoch 00282: val_loss did not improve from 144.01224\n","Epoch 283/1000\n","12/12 [==============================] - 25s 2s/step - loss: 18.7897 - val_loss: 149.3103\n","\n","Epoch 00283: val_loss did not improve from 144.01224\n","Epoch 284/1000\n","12/12 [==============================] - 25s 2s/step - loss: 18.1834 - val_loss: 149.4646\n","\n","Epoch 00284: val_loss did not improve from 144.01224\n","Epoch 285/1000\n","12/12 [==============================] - 25s 2s/step - loss: 17.7142 - val_loss: 150.7463\n","\n","Epoch 00285: val_loss did not improve from 144.01224\n","Epoch 286/1000\n","12/12 [==============================] - 25s 2s/step - loss: 17.2953 - val_loss: 151.3906\n","\n","Epoch 00286: val_loss did not improve from 144.01224\n","Epoch 287/1000\n","12/12 [==============================] - 25s 2s/step - loss: 16.8750 - val_loss: 149.0670\n","\n","Epoch 00287: val_loss did not improve from 144.01224\n","Epoch 288/1000\n","12/12 [==============================] - 25s 2s/step - loss: 16.5920 - val_loss: 150.9610\n","\n","Epoch 00288: val_loss did not improve from 144.01224\n","Epoch 289/1000\n","12/12 [==============================] - 25s 2s/step - loss: 15.9706 - val_loss: 149.9413\n","\n","Epoch 00289: val_loss did not improve from 144.01224\n","Epoch 290/1000\n","12/12 [==============================] - 25s 2s/step - loss: 15.3345 - val_loss: 149.9257\n","\n","Epoch 00290: val_loss did not improve from 144.01224\n","Epoch 291/1000\n","12/12 [==============================] - 25s 2s/step - loss: 15.1557 - val_loss: 152.0695\n","\n","Epoch 00291: val_loss did not improve from 144.01224\n","Epoch 292/1000\n","12/12 [==============================] - 25s 2s/step - loss: 14.6948 - val_loss: 151.7487\n","\n","Epoch 00292: val_loss did not improve from 144.01224\n","Epoch 293/1000\n","12/12 [==============================] - 25s 2s/step - loss: 14.3036 - val_loss: 154.1628\n","\n","Epoch 00293: val_loss did not improve from 144.01224\n","Epoch 294/1000\n","12/12 [==============================] - 25s 2s/step - loss: 13.8821 - val_loss: 150.5250\n","\n","Epoch 00294: val_loss did not improve from 144.01224\n","Epoch 295/1000\n","12/12 [==============================] - 25s 2s/step - loss: 13.5219 - val_loss: 151.5610\n","\n","Epoch 00295: val_loss did not improve from 144.01224\n","Epoch 296/1000\n","12/12 [==============================] - 25s 2s/step - loss: 13.1345 - val_loss: 154.3631\n","\n","Epoch 00296: val_loss did not improve from 144.01224\n","Epoch 297/1000\n","12/12 [==============================] - 25s 2s/step - loss: 12.8200 - val_loss: 150.2220\n","\n","Epoch 00297: val_loss did not improve from 144.01224\n","Epoch 298/1000\n","12/12 [==============================] - 25s 2s/step - loss: 12.5881 - val_loss: 154.3140\n","\n","Epoch 00298: val_loss did not improve from 144.01224\n","Epoch 299/1000\n","12/12 [==============================] - 25s 2s/step - loss: 12.1949 - val_loss: 155.3991\n","\n","Epoch 00299: val_loss did not improve from 144.01224\n","Epoch 300/1000\n","12/12 [==============================] - 25s 2s/step - loss: 11.9515 - val_loss: 153.1290\n","\n","Epoch 00300: val_loss did not improve from 144.01224\n","Epoch 301/1000\n","12/12 [==============================] - 25s 2s/step - loss: 11.6873 - val_loss: 153.0591\n","\n","Epoch 00301: val_loss did not improve from 144.01224\n","Epoch 302/1000\n","12/12 [==============================] - 25s 2s/step - loss: 11.3762 - val_loss: 153.4305\n","\n","Epoch 00302: val_loss did not improve from 144.01224\n","Epoch 303/1000\n","12/12 [==============================] - 25s 2s/step - loss: 11.1020 - val_loss: 155.6416\n","\n","Epoch 00303: val_loss did not improve from 144.01224\n","Epoch 304/1000\n","12/12 [==============================] - 25s 2s/step - loss: 10.8641 - val_loss: 156.3364\n","\n","Epoch 00304: val_loss did not improve from 144.01224\n","Epoch 305/1000\n","12/12 [==============================] - 25s 2s/step - loss: 10.5447 - val_loss: 152.0935\n","\n","Epoch 00305: val_loss did not improve from 144.01224\n","Epoch 306/1000\n","12/12 [==============================] - 25s 2s/step - loss: 10.3273 - val_loss: 154.2635\n","\n","Epoch 00306: val_loss did not improve from 144.01224\n","Epoch 307/1000\n","12/12 [==============================] - 25s 2s/step - loss: 10.1271 - val_loss: 159.1576\n","\n","Epoch 00307: val_loss did not improve from 144.01224\n","Epoch 308/1000\n","12/12 [==============================] - 25s 2s/step - loss: 9.9555 - val_loss: 155.9062\n","\n","Epoch 00308: val_loss did not improve from 144.01224\n","Epoch 309/1000\n","12/12 [==============================] - 25s 2s/step - loss: 9.8188 - val_loss: 158.0468\n","\n","Epoch 00309: val_loss did not improve from 144.01224\n","Epoch 310/1000\n","12/12 [==============================] - 25s 2s/step - loss: 9.4584 - val_loss: 158.3018\n","\n","Epoch 00310: val_loss did not improve from 144.01224\n","Epoch 311/1000\n","12/12 [==============================] - 25s 2s/step - loss: 9.2088 - val_loss: 155.5218\n","\n","Epoch 00311: val_loss did not improve from 144.01224\n","Epoch 312/1000\n","12/12 [==============================] - 25s 2s/step - loss: 9.0824 - val_loss: 159.7567\n","\n","Epoch 00312: val_loss did not improve from 144.01224\n","Epoch 313/1000\n","12/12 [==============================] - 25s 2s/step - loss: 8.8327 - val_loss: 156.1539\n","\n","Epoch 00313: val_loss did not improve from 144.01224\n","Epoch 314/1000\n","12/12 [==============================] - 25s 2s/step - loss: 8.6456 - val_loss: 155.7822\n","\n","Epoch 00314: val_loss did not improve from 144.01224\n","Epoch 315/1000\n","12/12 [==============================] - 25s 2s/step - loss: 8.5896 - val_loss: 162.4397\n","\n","Epoch 00315: val_loss did not improve from 144.01224\n","Epoch 316/1000\n","12/12 [==============================] - 25s 2s/step - loss: 8.3411 - val_loss: 156.7862\n","\n","Epoch 00316: val_loss did not improve from 144.01224\n","Epoch 317/1000\n","12/12 [==============================] - 25s 2s/step - loss: 8.0410 - val_loss: 158.9216\n","\n","Epoch 00317: val_loss did not improve from 144.01224\n","Epoch 318/1000\n","12/12 [==============================] - 25s 2s/step - loss: 7.9148 - val_loss: 157.5761\n","\n","Epoch 00318: val_loss did not improve from 144.01224\n","Epoch 319/1000\n","12/12 [==============================] - 25s 2s/step - loss: 7.8438 - val_loss: 161.2059\n","\n","Epoch 00319: val_loss did not improve from 144.01224\n","Epoch 320/1000\n","12/12 [==============================] - 25s 2s/step - loss: 7.6735 - val_loss: 162.0030\n","\n","Epoch 00320: val_loss did not improve from 144.01224\n","Epoch 321/1000\n","12/12 [==============================] - 25s 2s/step - loss: 7.5129 - val_loss: 160.9972\n","\n","Epoch 00321: val_loss did not improve from 144.01224\n","Epoch 322/1000\n","12/12 [==============================] - 25s 2s/step - loss: 7.3684 - val_loss: 159.6842\n","\n","Epoch 00322: val_loss did not improve from 144.01224\n","Epoch 323/1000\n","12/12 [==============================] - 25s 2s/step - loss: 7.2140 - val_loss: 162.2553\n","\n","Epoch 00323: val_loss did not improve from 144.01224\n","Epoch 324/1000\n","12/12 [==============================] - 25s 2s/step - loss: 7.0980 - val_loss: 160.0146\n","\n","Epoch 00324: val_loss did not improve from 144.01224\n","Epoch 325/1000\n","12/12 [==============================] - 25s 2s/step - loss: 6.9531 - val_loss: 161.5261\n","\n","Epoch 00325: val_loss did not improve from 144.01224\n","Epoch 326/1000\n","12/12 [==============================] - 25s 2s/step - loss: 6.9544 - val_loss: 163.4080\n","\n","Epoch 00326: val_loss did not improve from 144.01224\n","Epoch 327/1000\n","12/12 [==============================] - 25s 2s/step - loss: 6.7091 - val_loss: 162.8224\n","\n","Epoch 00327: val_loss did not improve from 144.01224\n","Epoch 328/1000\n","12/12 [==============================] - 25s 2s/step - loss: 6.6275 - val_loss: 162.3796\n","\n","Epoch 00328: val_loss did not improve from 144.01224\n","Epoch 329/1000\n","12/12 [==============================] - 25s 2s/step - loss: 6.5478 - val_loss: 164.6921\n","\n","Epoch 00329: val_loss did not improve from 144.01224\n","Epoch 330/1000\n","12/12 [==============================] - 25s 2s/step - loss: 6.4738 - val_loss: 161.9323\n","\n","Epoch 00330: val_loss did not improve from 144.01224\n","Epoch 331/1000\n","12/12 [==============================] - 25s 2s/step - loss: 6.3023 - val_loss: 164.5130\n","\n","Epoch 00331: val_loss did not improve from 144.01224\n","Epoch 332/1000\n","12/12 [==============================] - 25s 2s/step - loss: 6.2010 - val_loss: 164.5287\n","\n","Epoch 00332: val_loss did not improve from 144.01224\n","Epoch 333/1000\n","12/12 [==============================] - 25s 2s/step - loss: 6.2327 - val_loss: 163.7099\n","\n","Epoch 00333: val_loss did not improve from 144.01224\n","Epoch 334/1000\n","12/12 [==============================] - 25s 2s/step - loss: 6.0566 - val_loss: 163.8220\n","\n","Epoch 00334: val_loss did not improve from 144.01224\n","Epoch 335/1000\n","12/12 [==============================] - 25s 2s/step - loss: 5.9970 - val_loss: 165.6314\n","\n","Epoch 00335: val_loss did not improve from 144.01224\n","Epoch 336/1000\n","12/12 [==============================] - 25s 2s/step - loss: 5.8220 - val_loss: 165.2177\n","\n","Epoch 00336: val_loss did not improve from 144.01224\n","Epoch 337/1000\n","12/12 [==============================] - 25s 2s/step - loss: 5.7661 - val_loss: 167.1566\n","\n","Epoch 00337: val_loss did not improve from 144.01224\n","Epoch 338/1000\n","12/12 [==============================] - 25s 2s/step - loss: 5.6617 - val_loss: 164.1054\n","\n","Epoch 00338: val_loss did not improve from 144.01224\n","Epoch 339/1000\n","12/12 [==============================] - 25s 2s/step - loss: 5.6170 - val_loss: 164.8200\n","\n","Epoch 00339: val_loss did not improve from 144.01224\n","Epoch 340/1000\n","12/12 [==============================] - 25s 2s/step - loss: 5.5151 - val_loss: 164.0693\n","\n","Epoch 00340: val_loss did not improve from 144.01224\n","Epoch 341/1000\n","12/12 [==============================] - 25s 2s/step - loss: 5.4766 - val_loss: 172.2163\n","\n","Epoch 00341: val_loss did not improve from 144.01224\n","Epoch 342/1000\n","12/12 [==============================] - 25s 2s/step - loss: 5.3703 - val_loss: 166.7239\n","\n","Epoch 00342: val_loss did not improve from 144.01224\n","Epoch 343/1000\n","12/12 [==============================] - 25s 2s/step - loss: 5.3132 - val_loss: 168.1598\n","\n","Epoch 00343: val_loss did not improve from 144.01224\n","Epoch 344/1000\n","12/12 [==============================] - 25s 2s/step - loss: 5.1916 - val_loss: 168.1685\n","\n","Epoch 00344: val_loss did not improve from 144.01224\n","Epoch 345/1000\n","12/12 [==============================] - 25s 2s/step - loss: 5.1073 - val_loss: 169.0534\n","\n","Epoch 00345: val_loss did not improve from 144.01224\n","Epoch 346/1000\n","12/12 [==============================] - 25s 2s/step - loss: 5.0489 - val_loss: 171.5223\n","\n","Epoch 00346: val_loss did not improve from 144.01224\n","Epoch 347/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.9660 - val_loss: 166.8028\n","\n","Epoch 00347: val_loss did not improve from 144.01224\n","Epoch 348/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.9380 - val_loss: 169.1609\n","\n","Epoch 00348: val_loss did not improve from 144.01224\n","Epoch 349/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.8545 - val_loss: 169.0589\n","\n","Epoch 00349: val_loss did not improve from 144.01224\n","Epoch 350/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.7856 - val_loss: 170.1564\n","\n","Epoch 00350: val_loss did not improve from 144.01224\n","Epoch 351/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.7458 - val_loss: 170.2345\n","\n","Epoch 00351: val_loss did not improve from 144.01224\n","Epoch 352/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.6857 - val_loss: 172.9656\n","\n","Epoch 00352: val_loss did not improve from 144.01224\n","Epoch 353/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.6440 - val_loss: 168.7420\n","\n","Epoch 00353: val_loss did not improve from 144.01224\n","Epoch 354/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.5864 - val_loss: 171.6326\n","\n","Epoch 00354: val_loss did not improve from 144.01224\n","Epoch 355/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.5348 - val_loss: 172.8205\n","\n","Epoch 00355: val_loss did not improve from 144.01224\n","Epoch 356/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.4500 - val_loss: 172.1970\n","\n","Epoch 00356: val_loss did not improve from 144.01224\n","Epoch 357/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.4436 - val_loss: 174.2744\n","\n","Epoch 00357: val_loss did not improve from 144.01224\n","Epoch 358/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.4008 - val_loss: 172.1807\n","\n","Epoch 00358: val_loss did not improve from 144.01224\n","Epoch 359/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.3557 - val_loss: 173.1086\n","\n","Epoch 00359: val_loss did not improve from 144.01224\n","Epoch 360/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.2881 - val_loss: 169.5846\n","\n","Epoch 00360: val_loss did not improve from 144.01224\n","Epoch 361/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.3188 - val_loss: 177.2783\n","\n","Epoch 00361: val_loss did not improve from 144.01224\n","Epoch 362/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.2061 - val_loss: 172.9620\n","\n","Epoch 00362: val_loss did not improve from 144.01224\n","Epoch 363/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.2057 - val_loss: 176.0211\n","\n","Epoch 00363: val_loss did not improve from 144.01224\n","Epoch 364/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.1794 - val_loss: 173.5670\n","\n","Epoch 00364: val_loss did not improve from 144.01224\n","Epoch 365/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.1775 - val_loss: 173.7511\n","\n","Epoch 00365: val_loss did not improve from 144.01224\n","Epoch 366/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.2025 - val_loss: 176.1977\n","\n","Epoch 00366: val_loss did not improve from 144.01224\n","Epoch 367/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.1663 - val_loss: 176.4320\n","\n","Epoch 00367: val_loss did not improve from 144.01224\n","Epoch 368/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.1147 - val_loss: 177.5565\n","\n","Epoch 00368: val_loss did not improve from 144.01224\n","Epoch 369/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.0756 - val_loss: 172.8164\n","\n","Epoch 00369: val_loss did not improve from 144.01224\n","Epoch 370/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.9657 - val_loss: 178.5339\n","\n","Epoch 00370: val_loss did not improve from 144.01224\n","Epoch 371/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.0176 - val_loss: 182.5412\n","\n","Epoch 00371: val_loss did not improve from 144.01224\n","Epoch 372/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.2424 - val_loss: 165.0469\n","\n","Epoch 00372: val_loss did not improve from 144.01224\n","Epoch 373/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.7036 - val_loss: 183.7390\n","\n","Epoch 00373: val_loss did not improve from 144.01224\n","Epoch 374/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.7357 - val_loss: 169.3044\n","\n","Epoch 00374: val_loss did not improve from 144.01224\n","Epoch 375/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.8691 - val_loss: 167.6493\n","\n","Epoch 00375: val_loss did not improve from 144.01224\n","Epoch 376/1000\n","12/12 [==============================] - 25s 2s/step - loss: 5.0369 - val_loss: 172.5031\n","\n","Epoch 00376: val_loss did not improve from 144.01224\n","Epoch 377/1000\n","12/12 [==============================] - 25s 2s/step - loss: 5.1550 - val_loss: 172.1036\n","\n","Epoch 00377: val_loss did not improve from 144.01224\n","Epoch 378/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.6429 - val_loss: 174.5385\n","\n","Epoch 00378: val_loss did not improve from 144.01224\n","Epoch 379/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.3289 - val_loss: 173.3051\n","\n","Epoch 00379: val_loss did not improve from 144.01224\n","Epoch 380/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.2550 - val_loss: 169.5504\n","\n","Epoch 00380: val_loss did not improve from 144.01224\n","Epoch 381/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.2339 - val_loss: 179.7622\n","\n","Epoch 00381: val_loss did not improve from 144.01224\n","Epoch 382/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.0945 - val_loss: 170.9278\n","\n","Epoch 00382: val_loss did not improve from 144.01224\n","Epoch 383/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.8575 - val_loss: 174.8888\n","\n","Epoch 00383: val_loss did not improve from 144.01224\n","Epoch 384/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.7477 - val_loss: 174.0531\n","\n","Epoch 00384: val_loss did not improve from 144.01224\n","Epoch 385/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.6841 - val_loss: 177.3484\n","\n","Epoch 00385: val_loss did not improve from 144.01224\n","Epoch 386/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.6010 - val_loss: 178.8449\n","\n","Epoch 00386: val_loss did not improve from 144.01224\n","Epoch 387/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.5587 - val_loss: 178.7202\n","\n","Epoch 00387: val_loss did not improve from 144.01224\n","Epoch 388/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.4520 - val_loss: 179.1177\n","\n","Epoch 00388: val_loss did not improve from 144.01224\n","Epoch 389/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.4436 - val_loss: 180.3117\n","\n","Epoch 00389: val_loss did not improve from 144.01224\n","Epoch 390/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.3687 - val_loss: 180.5744\n","\n","Epoch 00390: val_loss did not improve from 144.01224\n","Epoch 391/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.3583 - val_loss: 178.9845\n","\n","Epoch 00391: val_loss did not improve from 144.01224\n","Epoch 392/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.3430 - val_loss: 178.2129\n","\n","Epoch 00392: val_loss did not improve from 144.01224\n","Epoch 393/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.3653 - val_loss: 182.9294\n","\n","Epoch 00393: val_loss did not improve from 144.01224\n","Epoch 394/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.2879 - val_loss: 180.8091\n","\n","Epoch 00394: val_loss did not improve from 144.01224\n","Epoch 395/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.2177 - val_loss: 179.5808\n","\n","Epoch 00395: val_loss did not improve from 144.01224\n","Epoch 396/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.2445 - val_loss: 181.0876\n","\n","Epoch 00396: val_loss did not improve from 144.01224\n","Epoch 397/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.1861 - val_loss: 179.6883\n","\n","Epoch 00397: val_loss did not improve from 144.01224\n","Epoch 398/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.1573 - val_loss: 182.2263\n","\n","Epoch 00398: val_loss did not improve from 144.01224\n","Epoch 399/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.1490 - val_loss: 180.7784\n","\n","Epoch 00399: val_loss did not improve from 144.01224\n","Epoch 400/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.0930 - val_loss: 180.7937\n","\n","Epoch 00400: val_loss did not improve from 144.01224\n","Epoch 401/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.1002 - val_loss: 184.4523\n","\n","Epoch 00401: val_loss did not improve from 144.01224\n","Epoch 402/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.0699 - val_loss: 185.0092\n","\n","Epoch 00402: val_loss did not improve from 144.01224\n","Epoch 403/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.0428 - val_loss: 182.1342\n","\n","Epoch 00403: val_loss did not improve from 144.01224\n","Epoch 404/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.0469 - val_loss: 185.0897\n","\n","Epoch 00404: val_loss did not improve from 144.01224\n","Epoch 405/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.0198 - val_loss: 181.9439\n","\n","Epoch 00405: val_loss did not improve from 144.01224\n","Epoch 406/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.9991 - val_loss: 184.0492\n","\n","Epoch 00406: val_loss did not improve from 144.01224\n","Epoch 407/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.9654 - val_loss: 185.6249\n","\n","Epoch 00407: val_loss did not improve from 144.01224\n","Epoch 408/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.9623 - val_loss: 184.4478\n","\n","Epoch 00408: val_loss did not improve from 144.01224\n","Epoch 409/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.9564 - val_loss: 185.2026\n","\n","Epoch 00409: val_loss did not improve from 144.01224\n","Epoch 410/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.9341 - val_loss: 184.2858\n","\n","Epoch 00410: val_loss did not improve from 144.01224\n","Epoch 411/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.9119 - val_loss: 185.0531\n","\n","Epoch 00411: val_loss did not improve from 144.01224\n","Epoch 412/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.9096 - val_loss: 185.8189\n","\n","Epoch 00412: val_loss did not improve from 144.01224\n","Epoch 413/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.8819 - val_loss: 190.1813\n","\n","Epoch 00413: val_loss did not improve from 144.01224\n","Epoch 414/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.8560 - val_loss: 188.6225\n","\n","Epoch 00414: val_loss did not improve from 144.01224\n","Epoch 415/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.8486 - val_loss: 188.5355\n","\n","Epoch 00415: val_loss did not improve from 144.01224\n","Epoch 416/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.8200 - val_loss: 187.7904\n","\n","Epoch 00416: val_loss did not improve from 144.01224\n","Epoch 417/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.7998 - val_loss: 186.0563\n","\n","Epoch 00417: val_loss did not improve from 144.01224\n","Epoch 418/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.8220 - val_loss: 186.8145\n","\n","Epoch 00418: val_loss did not improve from 144.01224\n","Epoch 419/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.7849 - val_loss: 189.2082\n","\n","Epoch 00419: val_loss did not improve from 144.01224\n","Epoch 420/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.7879 - val_loss: 188.2544\n","\n","Epoch 00420: val_loss did not improve from 144.01224\n","Epoch 421/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.7681 - val_loss: 187.8919\n","\n","Epoch 00421: val_loss did not improve from 144.01224\n","Epoch 422/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.7406 - val_loss: 188.3114\n","\n","Epoch 00422: val_loss did not improve from 144.01224\n","Epoch 423/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.7726 - val_loss: 184.6504\n","\n","Epoch 00423: val_loss did not improve from 144.01224\n","Epoch 424/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.7423 - val_loss: 188.6878\n","\n","Epoch 00424: val_loss did not improve from 144.01224\n","Epoch 425/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.7315 - val_loss: 186.8415\n","\n","Epoch 00425: val_loss did not improve from 144.01224\n","Epoch 426/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.7140 - val_loss: 190.0271\n","\n","Epoch 00426: val_loss did not improve from 144.01224\n","Epoch 427/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.6742 - val_loss: 191.1289\n","\n","Epoch 00427: val_loss did not improve from 144.01224\n","Epoch 428/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.6640 - val_loss: 187.8332\n","\n","Epoch 00428: val_loss did not improve from 144.01224\n","Epoch 429/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.6731 - val_loss: 191.2025\n","\n","Epoch 00429: val_loss did not improve from 144.01224\n","Epoch 430/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.6404 - val_loss: 192.1367\n","\n","Epoch 00430: val_loss did not improve from 144.01224\n","Epoch 431/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.6391 - val_loss: 190.3380\n","\n","Epoch 00431: val_loss did not improve from 144.01224\n","Epoch 432/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.6282 - val_loss: 188.4696\n","\n","Epoch 00432: val_loss did not improve from 144.01224\n","Epoch 433/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.6279 - val_loss: 191.2718\n","\n","Epoch 00433: val_loss did not improve from 144.01224\n","Epoch 434/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.6258 - val_loss: 191.2168\n","\n","Epoch 00434: val_loss did not improve from 144.01224\n","Epoch 435/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.6122 - val_loss: 190.2208\n","\n","Epoch 00435: val_loss did not improve from 144.01224\n","Epoch 436/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.5890 - val_loss: 188.8838\n","\n","Epoch 00436: val_loss did not improve from 144.01224\n","Epoch 437/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.5861 - val_loss: 190.6493\n","\n","Epoch 00437: val_loss did not improve from 144.01224\n","Epoch 438/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.5889 - val_loss: 191.3668\n","\n","Epoch 00438: val_loss did not improve from 144.01224\n","Epoch 439/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.5897 - val_loss: 192.0689\n","\n","Epoch 00439: val_loss did not improve from 144.01224\n","Epoch 440/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.5738 - val_loss: 193.0324\n","\n","Epoch 00440: val_loss did not improve from 144.01224\n","Epoch 441/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.5383 - val_loss: 194.9987\n","\n","Epoch 00441: val_loss did not improve from 144.01224\n","Epoch 442/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.5346 - val_loss: 194.9739\n","\n","Epoch 00442: val_loss did not improve from 144.01224\n","Epoch 443/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.5317 - val_loss: 194.2135\n","\n","Epoch 00443: val_loss did not improve from 144.01224\n","Epoch 444/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.5052 - val_loss: 193.5036\n","\n","Epoch 00444: val_loss did not improve from 144.01224\n","Epoch 445/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4954 - val_loss: 193.0156\n","\n","Epoch 00445: val_loss did not improve from 144.01224\n","Epoch 446/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.5189 - val_loss: 194.6160\n","\n","Epoch 00446: val_loss did not improve from 144.01224\n","Epoch 447/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4881 - val_loss: 193.2739\n","\n","Epoch 00447: val_loss did not improve from 144.01224\n","Epoch 448/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4697 - val_loss: 193.3527\n","\n","Epoch 00448: val_loss did not improve from 144.01224\n","Epoch 449/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4889 - val_loss: 195.4564\n","\n","Epoch 00449: val_loss did not improve from 144.01224\n","Epoch 450/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4809 - val_loss: 193.6440\n","\n","Epoch 00450: val_loss did not improve from 144.01224\n","Epoch 451/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4436 - val_loss: 196.0846\n","\n","Epoch 00451: val_loss did not improve from 144.01224\n","Epoch 452/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4480 - val_loss: 195.1505\n","\n","Epoch 00452: val_loss did not improve from 144.01224\n","Epoch 453/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4386 - val_loss: 196.0776\n","\n","Epoch 00453: val_loss did not improve from 144.01224\n","Epoch 454/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4452 - val_loss: 197.3526\n","\n","Epoch 00454: val_loss did not improve from 144.01224\n","Epoch 455/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4365 - val_loss: 193.7645\n","\n","Epoch 00455: val_loss did not improve from 144.01224\n","Epoch 456/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4238 - val_loss: 195.9231\n","\n","Epoch 00456: val_loss did not improve from 144.01224\n","Epoch 457/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4293 - val_loss: 196.4258\n","\n","Epoch 00457: val_loss did not improve from 144.01224\n","Epoch 458/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4226 - val_loss: 195.7281\n","\n","Epoch 00458: val_loss did not improve from 144.01224\n","Epoch 459/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4107 - val_loss: 194.9212\n","\n","Epoch 00459: val_loss did not improve from 144.01224\n","Epoch 460/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3957 - val_loss: 194.0728\n","\n","Epoch 00460: val_loss did not improve from 144.01224\n","Epoch 461/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3995 - val_loss: 193.9628\n","\n","Epoch 00461: val_loss did not improve from 144.01224\n","Epoch 462/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3918 - val_loss: 198.3418\n","\n","Epoch 00462: val_loss did not improve from 144.01224\n","Epoch 463/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3677 - val_loss: 196.7615\n","\n","Epoch 00463: val_loss did not improve from 144.01224\n","Epoch 464/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3663 - val_loss: 196.9405\n","\n","Epoch 00464: val_loss did not improve from 144.01224\n","Epoch 465/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3888 - val_loss: 194.9954\n","\n","Epoch 00465: val_loss did not improve from 144.01224\n","Epoch 466/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3769 - val_loss: 197.5091\n","\n","Epoch 00466: val_loss did not improve from 144.01224\n","Epoch 467/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3516 - val_loss: 197.5408\n","\n","Epoch 00467: val_loss did not improve from 144.01224\n","Epoch 468/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3483 - val_loss: 199.2179\n","\n","Epoch 00468: val_loss did not improve from 144.01224\n","Epoch 469/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3532 - val_loss: 197.2539\n","\n","Epoch 00469: val_loss did not improve from 144.01224\n","Epoch 470/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3275 - val_loss: 196.1596\n","\n","Epoch 00470: val_loss did not improve from 144.01224\n","Epoch 471/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3138 - val_loss: 194.7041\n","\n","Epoch 00471: val_loss did not improve from 144.01224\n","Epoch 472/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3138 - val_loss: 197.3064\n","\n","Epoch 00472: val_loss did not improve from 144.01224\n","Epoch 473/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3154 - val_loss: 197.3322\n","\n","Epoch 00473: val_loss did not improve from 144.01224\n","Epoch 474/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3219 - val_loss: 196.9346\n","\n","Epoch 00474: val_loss did not improve from 144.01224\n","Epoch 475/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2904 - val_loss: 199.0939\n","\n","Epoch 00475: val_loss did not improve from 144.01224\n","Epoch 476/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3063 - val_loss: 196.2615\n","\n","Epoch 00476: val_loss did not improve from 144.01224\n","Epoch 477/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3087 - val_loss: 200.3539\n","\n","Epoch 00477: val_loss did not improve from 144.01224\n","Epoch 478/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3083 - val_loss: 201.3740\n","\n","Epoch 00478: val_loss did not improve from 144.01224\n","Epoch 479/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3134 - val_loss: 195.4209\n","\n","Epoch 00479: val_loss did not improve from 144.01224\n","Epoch 480/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2767 - val_loss: 198.4088\n","\n","Epoch 00480: val_loss did not improve from 144.01224\n","Epoch 481/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2794 - val_loss: 199.7988\n","\n","Epoch 00481: val_loss did not improve from 144.01224\n","Epoch 482/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2937 - val_loss: 202.2848\n","\n","Epoch 00482: val_loss did not improve from 144.01224\n","Epoch 483/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2939 - val_loss: 200.6075\n","\n","Epoch 00483: val_loss did not improve from 144.01224\n","Epoch 484/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2800 - val_loss: 198.3836\n","\n","Epoch 00484: val_loss did not improve from 144.01224\n","Epoch 485/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2561 - val_loss: 199.6604\n","\n","Epoch 00485: val_loss did not improve from 144.01224\n","Epoch 486/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2680 - val_loss: 199.3201\n","\n","Epoch 00486: val_loss did not improve from 144.01224\n","Epoch 487/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2460 - val_loss: 198.8033\n","\n","Epoch 00487: val_loss did not improve from 144.01224\n","Epoch 488/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2751 - val_loss: 199.9062\n","\n","Epoch 00488: val_loss did not improve from 144.01224\n","Epoch 489/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2592 - val_loss: 204.9893\n","\n","Epoch 00489: val_loss did not improve from 144.01224\n","Epoch 490/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4074 - val_loss: 176.4494\n","\n","Epoch 00490: val_loss did not improve from 144.01224\n","Epoch 491/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.2446 - val_loss: 188.5130\n","\n","Epoch 00491: val_loss did not improve from 144.01224\n","Epoch 492/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.1789 - val_loss: 197.9585\n","\n","Epoch 00492: val_loss did not improve from 144.01224\n","Epoch 493/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.3052 - val_loss: 183.1862\n","\n","Epoch 00493: val_loss did not improve from 144.01224\n","Epoch 494/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.3926 - val_loss: 186.2478\n","\n","Epoch 00494: val_loss did not improve from 144.01224\n","Epoch 495/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.5309 - val_loss: 177.1746\n","\n","Epoch 00495: val_loss did not improve from 144.01224\n","Epoch 496/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.7879 - val_loss: 175.2083\n","\n","Epoch 00496: val_loss did not improve from 144.01224\n","Epoch 497/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.2399 - val_loss: 189.5194\n","\n","Epoch 00497: val_loss did not improve from 144.01224\n","Epoch 498/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.7914 - val_loss: 182.1508\n","\n","Epoch 00498: val_loss did not improve from 144.01224\n","Epoch 499/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.9945 - val_loss: 217.4379\n","\n","Epoch 00499: val_loss did not improve from 144.01224\n","Epoch 500/1000\n","12/12 [==============================] - 25s 2s/step - loss: 5.9640 - val_loss: 217.8499\n","\n","Epoch 00500: val_loss did not improve from 144.01224\n","Epoch 501/1000\n","12/12 [==============================] - 25s 2s/step - loss: 7.4071 - val_loss: 202.4966\n","\n","Epoch 00501: val_loss did not improve from 144.01224\n","Epoch 502/1000\n","12/12 [==============================] - 25s 2s/step - loss: 10.7714 - val_loss: 238.1400\n","\n","Epoch 00502: val_loss did not improve from 144.01224\n","Epoch 503/1000\n","12/12 [==============================] - 25s 2s/step - loss: 17.3378 - val_loss: 230.7724\n","\n","Epoch 00503: val_loss did not improve from 144.01224\n","Epoch 504/1000\n","12/12 [==============================] - 25s 2s/step - loss: 22.0031 - val_loss: 170.4041\n","\n","Epoch 00504: val_loss did not improve from 144.01224\n","Epoch 505/1000\n","12/12 [==============================] - 25s 2s/step - loss: 18.5417 - val_loss: 175.9733\n","\n","Epoch 00505: val_loss did not improve from 144.01224\n","Epoch 506/1000\n","12/12 [==============================] - 25s 2s/step - loss: 14.0802 - val_loss: 166.8619\n","\n","Epoch 00506: val_loss did not improve from 144.01224\n","Epoch 507/1000\n","12/12 [==============================] - 25s 2s/step - loss: 9.9482 - val_loss: 174.7161\n","\n","Epoch 00507: val_loss did not improve from 144.01224\n","Epoch 508/1000\n","12/12 [==============================] - 25s 2s/step - loss: 8.3659 - val_loss: 176.2344\n","\n","Epoch 00508: val_loss did not improve from 144.01224\n","Epoch 509/1000\n","12/12 [==============================] - 25s 2s/step - loss: 6.2009 - val_loss: 199.6673\n","\n","Epoch 00509: val_loss did not improve from 144.01224\n","Epoch 510/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.8844 - val_loss: 179.6342\n","\n","Epoch 00510: val_loss did not improve from 144.01224\n","Epoch 511/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.8759 - val_loss: 190.7571\n","\n","Epoch 00511: val_loss did not improve from 144.01224\n","Epoch 512/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.3591 - val_loss: 194.8808\n","\n","Epoch 00512: val_loss did not improve from 144.01224\n","Epoch 513/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.2665 - val_loss: 194.2905\n","\n","Epoch 00513: val_loss did not improve from 144.01224\n","Epoch 514/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.9504 - val_loss: 193.7216\n","\n","Epoch 00514: val_loss did not improve from 144.01224\n","Epoch 515/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.8681 - val_loss: 193.6478\n","\n","Epoch 00515: val_loss did not improve from 144.01224\n","Epoch 516/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.7183 - val_loss: 195.4160\n","\n","Epoch 00516: val_loss did not improve from 144.01224\n","Epoch 517/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.6720 - val_loss: 197.3609\n","\n","Epoch 00517: val_loss did not improve from 144.01224\n","Epoch 518/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.6466 - val_loss: 198.5239\n","\n","Epoch 00518: val_loss did not improve from 144.01224\n","Epoch 519/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.5966 - val_loss: 198.7824\n","\n","Epoch 00519: val_loss did not improve from 144.01224\n","Epoch 520/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.5559 - val_loss: 198.5094\n","\n","Epoch 00520: val_loss did not improve from 144.01224\n","Epoch 521/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.5435 - val_loss: 198.7146\n","\n","Epoch 00521: val_loss did not improve from 144.01224\n","Epoch 522/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4708 - val_loss: 198.8396\n","\n","Epoch 00522: val_loss did not improve from 144.01224\n","Epoch 523/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4358 - val_loss: 199.8178\n","\n","Epoch 00523: val_loss did not improve from 144.01224\n","Epoch 524/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4252 - val_loss: 200.4936\n","\n","Epoch 00524: val_loss did not improve from 144.01224\n","Epoch 525/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4261 - val_loss: 201.3226\n","\n","Epoch 00525: val_loss did not improve from 144.01224\n","Epoch 526/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3991 - val_loss: 201.1218\n","\n","Epoch 00526: val_loss did not improve from 144.01224\n","Epoch 527/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3611 - val_loss: 201.3519\n","\n","Epoch 00527: val_loss did not improve from 144.01224\n","Epoch 528/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3649 - val_loss: 201.7286\n","\n","Epoch 00528: val_loss did not improve from 144.01224\n","Epoch 529/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3372 - val_loss: 202.2748\n","\n","Epoch 00529: val_loss did not improve from 144.01224\n","Epoch 530/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3434 - val_loss: 202.2790\n","\n","Epoch 00530: val_loss did not improve from 144.01224\n","Epoch 531/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.3091 - val_loss: 202.8687\n","\n","Epoch 00531: val_loss did not improve from 144.01224\n","Epoch 532/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2905 - val_loss: 203.4292\n","\n","Epoch 00532: val_loss did not improve from 144.01224\n","Epoch 533/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2817 - val_loss: 203.9739\n","\n","Epoch 00533: val_loss did not improve from 144.01224\n","Epoch 534/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2844 - val_loss: 203.8589\n","\n","Epoch 00534: val_loss did not improve from 144.01224\n","Epoch 535/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2650 - val_loss: 204.0835\n","\n","Epoch 00535: val_loss did not improve from 144.01224\n","Epoch 536/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2560 - val_loss: 204.5154\n","\n","Epoch 00536: val_loss did not improve from 144.01224\n","Epoch 537/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2428 - val_loss: 204.6708\n","\n","Epoch 00537: val_loss did not improve from 144.01224\n","Epoch 538/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2308 - val_loss: 204.9214\n","\n","Epoch 00538: val_loss did not improve from 144.01224\n","Epoch 539/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2106 - val_loss: 205.1342\n","\n","Epoch 00539: val_loss did not improve from 144.01224\n","Epoch 540/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2258 - val_loss: 205.7623\n","\n","Epoch 00540: val_loss did not improve from 144.01224\n","Epoch 541/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2038 - val_loss: 205.7243\n","\n","Epoch 00541: val_loss did not improve from 144.01224\n","Epoch 542/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.2028 - val_loss: 206.4240\n","\n","Epoch 00542: val_loss did not improve from 144.01224\n","Epoch 543/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.1876 - val_loss: 206.6064\n","\n","Epoch 00543: val_loss did not improve from 144.01224\n","Epoch 544/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.1748 - val_loss: 206.6016\n","\n","Epoch 00544: val_loss did not improve from 144.01224\n","Epoch 545/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.1703 - val_loss: 206.6954\n","\n","Epoch 00545: val_loss did not improve from 144.01224\n","Epoch 546/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.1703 - val_loss: 206.9895\n","\n","Epoch 00546: val_loss did not improve from 144.01224\n","Epoch 547/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.1507 - val_loss: 207.3693\n","\n","Epoch 00547: val_loss did not improve from 144.01224\n","Epoch 548/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.1354 - val_loss: 207.9603\n","\n","Epoch 00548: val_loss did not improve from 144.01224\n","Epoch 549/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.1426 - val_loss: 206.9670\n","\n","Epoch 00549: val_loss did not improve from 144.01224\n","Epoch 550/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.1571 - val_loss: 206.8032\n","\n","Epoch 00550: val_loss did not improve from 144.01224\n","Epoch 551/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.1369 - val_loss: 207.0195\n","\n","Epoch 00551: val_loss did not improve from 144.01224\n","Epoch 552/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.1326 - val_loss: 207.2729\n","\n","Epoch 00552: val_loss did not improve from 144.01224\n","Epoch 553/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.1141 - val_loss: 208.3392\n","\n","Epoch 00553: val_loss did not improve from 144.01224\n","Epoch 554/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.1103 - val_loss: 208.6932\n","\n","Epoch 00554: val_loss did not improve from 144.01224\n","Epoch 555/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0959 - val_loss: 208.9692\n","\n","Epoch 00555: val_loss did not improve from 144.01224\n","Epoch 556/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.1104 - val_loss: 209.2030\n","\n","Epoch 00556: val_loss did not improve from 144.01224\n","Epoch 557/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0965 - val_loss: 209.2638\n","\n","Epoch 00557: val_loss did not improve from 144.01224\n","Epoch 558/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0872 - val_loss: 208.4422\n","\n","Epoch 00558: val_loss did not improve from 144.01224\n","Epoch 559/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0854 - val_loss: 209.8745\n","\n","Epoch 00559: val_loss did not improve from 144.01224\n","Epoch 560/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0931 - val_loss: 210.4155\n","\n","Epoch 00560: val_loss did not improve from 144.01224\n","Epoch 561/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0754 - val_loss: 210.4937\n","\n","Epoch 00561: val_loss did not improve from 144.01224\n","Epoch 562/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0565 - val_loss: 210.3551\n","\n","Epoch 00562: val_loss did not improve from 144.01224\n","Epoch 563/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0654 - val_loss: 209.8612\n","\n","Epoch 00563: val_loss did not improve from 144.01224\n","Epoch 564/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0558 - val_loss: 210.5285\n","\n","Epoch 00564: val_loss did not improve from 144.01224\n","Epoch 565/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0429 - val_loss: 210.2992\n","\n","Epoch 00565: val_loss did not improve from 144.01224\n","Epoch 566/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0497 - val_loss: 209.2638\n","\n","Epoch 00566: val_loss did not improve from 144.01224\n","Epoch 567/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0345 - val_loss: 210.1111\n","\n","Epoch 00567: val_loss did not improve from 144.01224\n","Epoch 568/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0338 - val_loss: 210.6424\n","\n","Epoch 00568: val_loss did not improve from 144.01224\n","Epoch 569/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0417 - val_loss: 210.8335\n","\n","Epoch 00569: val_loss did not improve from 144.01224\n","Epoch 570/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0106 - val_loss: 210.6380\n","\n","Epoch 00570: val_loss did not improve from 144.01224\n","Epoch 571/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0372 - val_loss: 210.7775\n","\n","Epoch 00571: val_loss did not improve from 144.01224\n","Epoch 572/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0144 - val_loss: 211.4752\n","\n","Epoch 00572: val_loss did not improve from 144.01224\n","Epoch 573/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0279 - val_loss: 211.7349\n","\n","Epoch 00573: val_loss did not improve from 144.01224\n","Epoch 574/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0088 - val_loss: 210.6741\n","\n","Epoch 00574: val_loss did not improve from 144.01224\n","Epoch 575/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0133 - val_loss: 211.7246\n","\n","Epoch 00575: val_loss did not improve from 144.01224\n","Epoch 576/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9953 - val_loss: 211.3892\n","\n","Epoch 00576: val_loss did not improve from 144.01224\n","Epoch 577/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.0017 - val_loss: 212.5272\n","\n","Epoch 00577: val_loss did not improve from 144.01224\n","Epoch 578/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9966 - val_loss: 211.8646\n","\n","Epoch 00578: val_loss did not improve from 144.01224\n","Epoch 579/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9783 - val_loss: 211.8778\n","\n","Epoch 00579: val_loss did not improve from 144.01224\n","Epoch 580/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9941 - val_loss: 210.6616\n","\n","Epoch 00580: val_loss did not improve from 144.01224\n","Epoch 581/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9736 - val_loss: 211.5505\n","\n","Epoch 00581: val_loss did not improve from 144.01224\n","Epoch 582/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9734 - val_loss: 212.0646\n","\n","Epoch 00582: val_loss did not improve from 144.01224\n","Epoch 583/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9622 - val_loss: 212.5746\n","\n","Epoch 00583: val_loss did not improve from 144.01224\n","Epoch 584/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9761 - val_loss: 212.4481\n","\n","Epoch 00584: val_loss did not improve from 144.01224\n","Epoch 585/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9541 - val_loss: 212.3498\n","\n","Epoch 00585: val_loss did not improve from 144.01224\n","Epoch 586/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9585 - val_loss: 211.1659\n","\n","Epoch 00586: val_loss did not improve from 144.01224\n","Epoch 587/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9468 - val_loss: 210.9283\n","\n","Epoch 00587: val_loss did not improve from 144.01224\n","Epoch 588/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9548 - val_loss: 212.2004\n","\n","Epoch 00588: val_loss did not improve from 144.01224\n","Epoch 589/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9596 - val_loss: 212.1325\n","\n","Epoch 00589: val_loss did not improve from 144.01224\n","Epoch 590/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9201 - val_loss: 212.2751\n","\n","Epoch 00590: val_loss did not improve from 144.01224\n","Epoch 591/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9534 - val_loss: 211.8801\n","\n","Epoch 00591: val_loss did not improve from 144.01224\n","Epoch 592/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9273 - val_loss: 211.9086\n","\n","Epoch 00592: val_loss did not improve from 144.01224\n","Epoch 593/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9322 - val_loss: 211.1713\n","\n","Epoch 00593: val_loss did not improve from 144.01224\n","Epoch 594/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9285 - val_loss: 212.2286\n","\n","Epoch 00594: val_loss did not improve from 144.01224\n","Epoch 595/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9190 - val_loss: 212.6090\n","\n","Epoch 00595: val_loss did not improve from 144.01224\n","Epoch 596/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9164 - val_loss: 211.9879\n","\n","Epoch 00596: val_loss did not improve from 144.01224\n","Epoch 597/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9128 - val_loss: 211.4917\n","\n","Epoch 00597: val_loss did not improve from 144.01224\n","Epoch 598/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9008 - val_loss: 211.5612\n","\n","Epoch 00598: val_loss did not improve from 144.01224\n","Epoch 599/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.8920 - val_loss: 210.6249\n","\n","Epoch 00599: val_loss did not improve from 144.01224\n","Epoch 600/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.8812 - val_loss: 210.0475\n","\n","Epoch 00600: val_loss did not improve from 144.01224\n","Epoch 601/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.8957 - val_loss: 210.9776\n","\n","Epoch 00601: val_loss did not improve from 144.01224\n","Epoch 602/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.8812 - val_loss: 210.7306\n","\n","Epoch 00602: val_loss did not improve from 144.01224\n","Epoch 603/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.8669 - val_loss: 211.0284\n","\n","Epoch 00603: val_loss did not improve from 144.01224\n","Epoch 604/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.8733 - val_loss: 210.8870\n","\n","Epoch 00604: val_loss did not improve from 144.01224\n","Epoch 605/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.8705 - val_loss: 210.5241\n","\n","Epoch 00605: val_loss did not improve from 144.01224\n","Epoch 606/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.8490 - val_loss: 212.0803\n","\n","Epoch 00606: val_loss did not improve from 144.01224\n","Epoch 607/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.8435 - val_loss: 210.5214\n","\n","Epoch 00607: val_loss did not improve from 144.01224\n","Epoch 608/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.8264 - val_loss: 210.6166\n","\n","Epoch 00608: val_loss did not improve from 144.01224\n","Epoch 609/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.8233 - val_loss: 211.5430\n","\n","Epoch 00609: val_loss did not improve from 144.01224\n","Epoch 610/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.8001 - val_loss: 209.0345\n","\n","Epoch 00610: val_loss did not improve from 144.01224\n","Epoch 611/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.7784 - val_loss: 208.5562\n","\n","Epoch 00611: val_loss did not improve from 144.01224\n","Epoch 612/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.7635 - val_loss: 208.3909\n","\n","Epoch 00612: val_loss did not improve from 144.01224\n","Epoch 613/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.7641 - val_loss: 208.2118\n","\n","Epoch 00613: val_loss did not improve from 144.01224\n","Epoch 614/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.7374 - val_loss: 207.7260\n","\n","Epoch 00614: val_loss did not improve from 144.01224\n","Epoch 615/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.7323 - val_loss: 207.4861\n","\n","Epoch 00615: val_loss did not improve from 144.01224\n","Epoch 616/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.7052 - val_loss: 206.1866\n","\n","Epoch 00616: val_loss did not improve from 144.01224\n","Epoch 617/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.6663 - val_loss: 206.2585\n","\n","Epoch 00617: val_loss did not improve from 144.01224\n","Epoch 618/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.6581 - val_loss: 205.5925\n","\n","Epoch 00618: val_loss did not improve from 144.01224\n","Epoch 619/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.5963 - val_loss: 203.8241\n","\n","Epoch 00619: val_loss did not improve from 144.01224\n","Epoch 620/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.5804 - val_loss: 202.2133\n","\n","Epoch 00620: val_loss did not improve from 144.01224\n","Epoch 621/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.5370 - val_loss: 201.1641\n","\n","Epoch 00621: val_loss did not improve from 144.01224\n","Epoch 622/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.5059 - val_loss: 201.8098\n","\n","Epoch 00622: val_loss did not improve from 144.01224\n","Epoch 623/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.4818 - val_loss: 207.8011\n","\n","Epoch 00623: val_loss did not improve from 144.01224\n","Epoch 624/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.5940 - val_loss: 209.2240\n","\n","Epoch 00624: val_loss did not improve from 144.01224\n","Epoch 625/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.5859 - val_loss: 198.4710\n","\n","Epoch 00625: val_loss did not improve from 144.01224\n","Epoch 626/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.6374 - val_loss: 197.0269\n","\n","Epoch 00626: val_loss did not improve from 144.01224\n","Epoch 627/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.7993 - val_loss: 197.9749\n","\n","Epoch 00627: val_loss did not improve from 144.01224\n","Epoch 628/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.9423 - val_loss: 211.2799\n","\n","Epoch 00628: val_loss did not improve from 144.01224\n","Epoch 629/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.7965 - val_loss: 201.0820\n","\n","Epoch 00629: val_loss did not improve from 144.01224\n","Epoch 630/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.1825 - val_loss: 191.5490\n","\n","Epoch 00630: val_loss did not improve from 144.01224\n","Epoch 631/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.8782 - val_loss: 210.4418\n","\n","Epoch 00631: val_loss did not improve from 144.01224\n","Epoch 632/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.8794 - val_loss: 202.4650\n","\n","Epoch 00632: val_loss did not improve from 144.01224\n","Epoch 633/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.9307 - val_loss: 198.6857\n","\n","Epoch 00633: val_loss did not improve from 144.01224\n","Epoch 634/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.1412 - val_loss: 206.3112\n","\n","Epoch 00634: val_loss did not improve from 144.01224\n","Epoch 635/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.7831 - val_loss: 201.3999\n","\n","Epoch 00635: val_loss did not improve from 144.01224\n","Epoch 636/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.5781 - val_loss: 200.4261\n","\n","Epoch 00636: val_loss did not improve from 144.01224\n","Epoch 637/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.4606 - val_loss: 203.0526\n","\n","Epoch 00637: val_loss did not improve from 144.01224\n","Epoch 638/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.3334 - val_loss: 200.8071\n","\n","Epoch 00638: val_loss did not improve from 144.01224\n","Epoch 639/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.2485 - val_loss: 197.4992\n","\n","Epoch 00639: val_loss did not improve from 144.01224\n","Epoch 640/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.1665 - val_loss: 198.4427\n","\n","Epoch 00640: val_loss did not improve from 144.01224\n","Epoch 641/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.1025 - val_loss: 199.9043\n","\n","Epoch 00641: val_loss did not improve from 144.01224\n","Epoch 642/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.0348 - val_loss: 200.6430\n","\n","Epoch 00642: val_loss did not improve from 144.01224\n","Epoch 643/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.9600 - val_loss: 198.5709\n","\n","Epoch 00643: val_loss did not improve from 144.01224\n","Epoch 644/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.9096 - val_loss: 200.8630\n","\n","Epoch 00644: val_loss did not improve from 144.01224\n","Epoch 645/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.8654 - val_loss: 202.4656\n","\n","Epoch 00645: val_loss did not improve from 144.01224\n","Epoch 646/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.8309 - val_loss: 198.9958\n","\n","Epoch 00646: val_loss did not improve from 144.01224\n","Epoch 647/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.7990 - val_loss: 203.3233\n","\n","Epoch 00647: val_loss did not improve from 144.01224\n","Epoch 648/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.7434 - val_loss: 200.8724\n","\n","Epoch 00648: val_loss did not improve from 144.01224\n","Epoch 649/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.7000 - val_loss: 203.2222\n","\n","Epoch 00649: val_loss did not improve from 144.01224\n","Epoch 650/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.7019 - val_loss: 199.9889\n","\n","Epoch 00650: val_loss did not improve from 144.01224\n","Epoch 651/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.6923 - val_loss: 210.1463\n","\n","Epoch 00651: val_loss did not improve from 144.01224\n","Epoch 652/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.8188 - val_loss: 203.2937\n","\n","Epoch 00652: val_loss did not improve from 144.01224\n","Epoch 653/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.1189 - val_loss: 197.7443\n","\n","Epoch 00653: val_loss did not improve from 144.01224\n","Epoch 654/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.4966 - val_loss: 211.9426\n","\n","Epoch 00654: val_loss did not improve from 144.01224\n","Epoch 655/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.3887 - val_loss: 195.2815\n","\n","Epoch 00655: val_loss did not improve from 144.01224\n","Epoch 656/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.1926 - val_loss: 197.0830\n","\n","Epoch 00656: val_loss did not improve from 144.01224\n","Epoch 657/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.3647 - val_loss: 194.1817\n","\n","Epoch 00657: val_loss did not improve from 144.01224\n","Epoch 658/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.0825 - val_loss: 204.7735\n","\n","Epoch 00658: val_loss did not improve from 144.01224\n","Epoch 659/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.9795 - val_loss: 196.6606\n","\n","Epoch 00659: val_loss did not improve from 144.01224\n","Epoch 660/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.8519 - val_loss: 201.6434\n","\n","Epoch 00660: val_loss did not improve from 144.01224\n","Epoch 661/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.7882 - val_loss: 202.1131\n","\n","Epoch 00661: val_loss did not improve from 144.01224\n","Epoch 662/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.7714 - val_loss: 198.6913\n","\n","Epoch 00662: val_loss did not improve from 144.01224\n","Epoch 663/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.6852 - val_loss: 195.1764\n","\n","Epoch 00663: val_loss did not improve from 144.01224\n","Epoch 664/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.7316 - val_loss: 197.0109\n","\n","Epoch 00664: val_loss did not improve from 144.01224\n","Epoch 665/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.6488 - val_loss: 200.1322\n","\n","Epoch 00665: val_loss did not improve from 144.01224\n","Epoch 666/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.6050 - val_loss: 205.8557\n","\n","Epoch 00666: val_loss did not improve from 144.01224\n","Epoch 667/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.5489 - val_loss: 206.9872\n","\n","Epoch 00667: val_loss did not improve from 144.01224\n","Epoch 668/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.5436 - val_loss: 206.1724\n","\n","Epoch 00668: val_loss did not improve from 144.01224\n","Epoch 669/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.4884 - val_loss: 205.3892\n","\n","Epoch 00669: val_loss did not improve from 144.01224\n","Epoch 670/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.4863 - val_loss: 204.8459\n","\n","Epoch 00670: val_loss did not improve from 144.01224\n","Epoch 671/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.4743 - val_loss: 201.0357\n","\n","Epoch 00671: val_loss did not improve from 144.01224\n","Epoch 672/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.4731 - val_loss: 201.9425\n","\n","Epoch 00672: val_loss did not improve from 144.01224\n","Epoch 673/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.4763 - val_loss: 201.8328\n","\n","Epoch 00673: val_loss did not improve from 144.01224\n","Epoch 674/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.5267 - val_loss: 210.0746\n","\n","Epoch 00674: val_loss did not improve from 144.01224\n","Epoch 675/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.6430 - val_loss: 198.4741\n","\n","Epoch 00675: val_loss did not improve from 144.01224\n","Epoch 676/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.8226 - val_loss: 210.4377\n","\n","Epoch 00676: val_loss did not improve from 144.01224\n","Epoch 677/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.8122 - val_loss: 211.6081\n","\n","Epoch 00677: val_loss did not improve from 144.01224\n","Epoch 678/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.3031 - val_loss: 193.5322\n","\n","Epoch 00678: val_loss did not improve from 144.01224\n","Epoch 679/1000\n","12/12 [==============================] - 25s 2s/step - loss: 4.1540 - val_loss: 196.7634\n","\n","Epoch 00679: val_loss did not improve from 144.01224\n","Epoch 680/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.1882 - val_loss: 184.3150\n","\n","Epoch 00680: val_loss did not improve from 144.01224\n","Epoch 681/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.8867 - val_loss: 196.0027\n","\n","Epoch 00681: val_loss did not improve from 144.01224\n","Epoch 682/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.1826 - val_loss: 204.8521\n","\n","Epoch 00682: val_loss did not improve from 144.01224\n","Epoch 683/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.4380 - val_loss: 204.8734\n","\n","Epoch 00683: val_loss did not improve from 144.01224\n","Epoch 684/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.1461 - val_loss: 200.3554\n","\n","Epoch 00684: val_loss did not improve from 144.01224\n","Epoch 685/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.0146 - val_loss: 213.6474\n","\n","Epoch 00685: val_loss did not improve from 144.01224\n","Epoch 686/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.7684 - val_loss: 203.9940\n","\n","Epoch 00686: val_loss did not improve from 144.01224\n","Epoch 687/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.6495 - val_loss: 200.5465\n","\n","Epoch 00687: val_loss did not improve from 144.01224\n","Epoch 688/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.5688 - val_loss: 206.3085\n","\n","Epoch 00688: val_loss did not improve from 144.01224\n","Epoch 689/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.5174 - val_loss: 205.9751\n","\n","Epoch 00689: val_loss did not improve from 144.01224\n","Epoch 690/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.4740 - val_loss: 206.5545\n","\n","Epoch 00690: val_loss did not improve from 144.01224\n","Epoch 691/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.4547 - val_loss: 206.3922\n","\n","Epoch 00691: val_loss did not improve from 144.01224\n","Epoch 692/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.4198 - val_loss: 207.1125\n","\n","Epoch 00692: val_loss did not improve from 144.01224\n","Epoch 693/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.3613 - val_loss: 206.4084\n","\n","Epoch 00693: val_loss did not improve from 144.01224\n","Epoch 694/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.3534 - val_loss: 207.5535\n","\n","Epoch 00694: val_loss did not improve from 144.01224\n","Epoch 695/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.3252 - val_loss: 210.2332\n","\n","Epoch 00695: val_loss did not improve from 144.01224\n","Epoch 696/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.3148 - val_loss: 210.4500\n","\n","Epoch 00696: val_loss did not improve from 144.01224\n","Epoch 697/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2922 - val_loss: 209.9080\n","\n","Epoch 00697: val_loss did not improve from 144.01224\n","Epoch 698/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.3057 - val_loss: 209.8425\n","\n","Epoch 00698: val_loss did not improve from 144.01224\n","Epoch 699/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.3100 - val_loss: 210.9762\n","\n","Epoch 00699: val_loss did not improve from 144.01224\n","Epoch 700/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2969 - val_loss: 212.5836\n","\n","Epoch 00700: val_loss did not improve from 144.01224\n","Epoch 701/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.3261 - val_loss: 210.8798\n","\n","Epoch 00701: val_loss did not improve from 144.01224\n","Epoch 702/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.3237 - val_loss: 209.2137\n","\n","Epoch 00702: val_loss did not improve from 144.01224\n","Epoch 703/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2869 - val_loss: 208.3269\n","\n","Epoch 00703: val_loss did not improve from 144.01224\n","Epoch 704/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2704 - val_loss: 209.4478\n","\n","Epoch 00704: val_loss did not improve from 144.01224\n","Epoch 705/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2444 - val_loss: 210.2981\n","\n","Epoch 00705: val_loss did not improve from 144.01224\n","Epoch 706/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2418 - val_loss: 211.1638\n","\n","Epoch 00706: val_loss did not improve from 144.01224\n","Epoch 707/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2703 - val_loss: 212.6717\n","\n","Epoch 00707: val_loss did not improve from 144.01224\n","Epoch 708/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2679 - val_loss: 213.6092\n","\n","Epoch 00708: val_loss did not improve from 144.01224\n","Epoch 709/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2463 - val_loss: 215.6171\n","\n","Epoch 00709: val_loss did not improve from 144.01224\n","Epoch 710/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2414 - val_loss: 214.4331\n","\n","Epoch 00710: val_loss did not improve from 144.01224\n","Epoch 711/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2219 - val_loss: 214.3361\n","\n","Epoch 00711: val_loss did not improve from 144.01224\n","Epoch 712/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2092 - val_loss: 214.8676\n","\n","Epoch 00712: val_loss did not improve from 144.01224\n","Epoch 713/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2197 - val_loss: 215.6296\n","\n","Epoch 00713: val_loss did not improve from 144.01224\n","Epoch 714/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2159 - val_loss: 216.8431\n","\n","Epoch 00714: val_loss did not improve from 144.01224\n","Epoch 715/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2118 - val_loss: 216.9653\n","\n","Epoch 00715: val_loss did not improve from 144.01224\n","Epoch 716/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2111 - val_loss: 218.3535\n","\n","Epoch 00716: val_loss did not improve from 144.01224\n","Epoch 717/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2067 - val_loss: 218.0154\n","\n","Epoch 00717: val_loss did not improve from 144.01224\n","Epoch 718/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1996 - val_loss: 216.5664\n","\n","Epoch 00718: val_loss did not improve from 144.01224\n","Epoch 719/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1883 - val_loss: 215.6913\n","\n","Epoch 00719: val_loss did not improve from 144.01224\n","Epoch 720/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1920 - val_loss: 216.9079\n","\n","Epoch 00720: val_loss did not improve from 144.01224\n","Epoch 721/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1947 - val_loss: 218.4931\n","\n","Epoch 00721: val_loss did not improve from 144.01224\n","Epoch 722/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1937 - val_loss: 219.6698\n","\n","Epoch 00722: val_loss did not improve from 144.01224\n","Epoch 723/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2031 - val_loss: 220.2812\n","\n","Epoch 00723: val_loss did not improve from 144.01224\n","Epoch 724/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2038 - val_loss: 220.1393\n","\n","Epoch 00724: val_loss did not improve from 144.01224\n","Epoch 725/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1779 - val_loss: 217.6530\n","\n","Epoch 00725: val_loss did not improve from 144.01224\n","Epoch 726/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1893 - val_loss: 218.6353\n","\n","Epoch 00726: val_loss did not improve from 144.01224\n","Epoch 727/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1894 - val_loss: 220.3452\n","\n","Epoch 00727: val_loss did not improve from 144.01224\n","Epoch 728/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1859 - val_loss: 220.8614\n","\n","Epoch 00728: val_loss did not improve from 144.01224\n","Epoch 729/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1853 - val_loss: 219.2997\n","\n","Epoch 00729: val_loss did not improve from 144.01224\n","Epoch 730/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1673 - val_loss: 219.2076\n","\n","Epoch 00730: val_loss did not improve from 144.01224\n","Epoch 731/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1788 - val_loss: 220.4528\n","\n","Epoch 00731: val_loss did not improve from 144.01224\n","Epoch 732/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1724 - val_loss: 221.1845\n","\n","Epoch 00732: val_loss did not improve from 144.01224\n","Epoch 733/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1768 - val_loss: 222.2633\n","\n","Epoch 00733: val_loss did not improve from 144.01224\n","Epoch 734/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1848 - val_loss: 223.1646\n","\n","Epoch 00734: val_loss did not improve from 144.01224\n","Epoch 735/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1834 - val_loss: 223.4070\n","\n","Epoch 00735: val_loss did not improve from 144.01224\n","Epoch 736/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1816 - val_loss: 222.1231\n","\n","Epoch 00736: val_loss did not improve from 144.01224\n","Epoch 737/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1697 - val_loss: 219.9013\n","\n","Epoch 00737: val_loss did not improve from 144.01224\n","Epoch 738/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1622 - val_loss: 220.3940\n","\n","Epoch 00738: val_loss did not improve from 144.01224\n","Epoch 739/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1627 - val_loss: 219.4474\n","\n","Epoch 00739: val_loss did not improve from 144.01224\n","Epoch 740/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1599 - val_loss: 218.6955\n","\n","Epoch 00740: val_loss did not improve from 144.01224\n","Epoch 741/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1777 - val_loss: 219.6454\n","\n","Epoch 00741: val_loss did not improve from 144.01224\n","Epoch 742/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1834 - val_loss: 221.6947\n","\n","Epoch 00742: val_loss did not improve from 144.01224\n","Epoch 743/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1753 - val_loss: 223.9293\n","\n","Epoch 00743: val_loss did not improve from 144.01224\n","Epoch 744/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1799 - val_loss: 223.8989\n","\n","Epoch 00744: val_loss did not improve from 144.01224\n","Epoch 745/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1556 - val_loss: 221.8153\n","\n","Epoch 00745: val_loss did not improve from 144.01224\n","Epoch 746/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1498 - val_loss: 221.3355\n","\n","Epoch 00746: val_loss did not improve from 144.01224\n","Epoch 747/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1563 - val_loss: 221.1577\n","\n","Epoch 00747: val_loss did not improve from 144.01224\n","Epoch 748/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1501 - val_loss: 221.6350\n","\n","Epoch 00748: val_loss did not improve from 144.01224\n","Epoch 749/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1650 - val_loss: 222.1994\n","\n","Epoch 00749: val_loss did not improve from 144.01224\n","Epoch 750/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1530 - val_loss: 222.3667\n","\n","Epoch 00750: val_loss did not improve from 144.01224\n","Epoch 751/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1638 - val_loss: 221.8935\n","\n","Epoch 00751: val_loss did not improve from 144.01224\n","Epoch 752/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1530 - val_loss: 222.1427\n","\n","Epoch 00752: val_loss did not improve from 144.01224\n","Epoch 753/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1657 - val_loss: 222.4926\n","\n","Epoch 00753: val_loss did not improve from 144.01224\n","Epoch 754/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1685 - val_loss: 224.2321\n","\n","Epoch 00754: val_loss did not improve from 144.01224\n","Epoch 755/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1543 - val_loss: 223.7174\n","\n","Epoch 00755: val_loss did not improve from 144.01224\n","Epoch 756/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1462 - val_loss: 223.3012\n","\n","Epoch 00756: val_loss did not improve from 144.01224\n","Epoch 757/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1543 - val_loss: 223.9427\n","\n","Epoch 00757: val_loss did not improve from 144.01224\n","Epoch 758/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1534 - val_loss: 225.6474\n","\n","Epoch 00758: val_loss did not improve from 144.01224\n","Epoch 759/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1409 - val_loss: 224.6561\n","\n","Epoch 00759: val_loss did not improve from 144.01224\n","Epoch 760/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1474 - val_loss: 222.3871\n","\n","Epoch 00760: val_loss did not improve from 144.01224\n","Epoch 761/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1600 - val_loss: 222.0471\n","\n","Epoch 00761: val_loss did not improve from 144.01224\n","Epoch 762/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1506 - val_loss: 222.7564\n","\n","Epoch 00762: val_loss did not improve from 144.01224\n","Epoch 763/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1526 - val_loss: 224.7975\n","\n","Epoch 00763: val_loss did not improve from 144.01224\n","Epoch 764/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1426 - val_loss: 224.9383\n","\n","Epoch 00764: val_loss did not improve from 144.01224\n","Epoch 765/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1581 - val_loss: 223.6180\n","\n","Epoch 00765: val_loss did not improve from 144.01224\n","Epoch 766/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1450 - val_loss: 223.7125\n","\n","Epoch 00766: val_loss did not improve from 144.01224\n","Epoch 767/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1639 - val_loss: 222.6765\n","\n","Epoch 00767: val_loss did not improve from 144.01224\n","Epoch 768/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1421 - val_loss: 223.4587\n","\n","Epoch 00768: val_loss did not improve from 144.01224\n","Epoch 769/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1536 - val_loss: 222.3658\n","\n","Epoch 00769: val_loss did not improve from 144.01224\n","Epoch 770/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1372 - val_loss: 222.2618\n","\n","Epoch 00770: val_loss did not improve from 144.01224\n","Epoch 771/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1444 - val_loss: 223.4636\n","\n","Epoch 00771: val_loss did not improve from 144.01224\n","Epoch 772/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1416 - val_loss: 224.9568\n","\n","Epoch 00772: val_loss did not improve from 144.01224\n","Epoch 773/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1609 - val_loss: 226.1656\n","\n","Epoch 00773: val_loss did not improve from 144.01224\n","Epoch 774/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1407 - val_loss: 224.8125\n","\n","Epoch 00774: val_loss did not improve from 144.01224\n","Epoch 775/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1492 - val_loss: 224.9195\n","\n","Epoch 00775: val_loss did not improve from 144.01224\n","Epoch 776/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1378 - val_loss: 224.9975\n","\n","Epoch 00776: val_loss did not improve from 144.01224\n","Epoch 777/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1361 - val_loss: 224.2245\n","\n","Epoch 00777: val_loss did not improve from 144.01224\n","Epoch 778/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1426 - val_loss: 225.0711\n","\n","Epoch 00778: val_loss did not improve from 144.01224\n","Epoch 779/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1231 - val_loss: 225.7946\n","\n","Epoch 00779: val_loss did not improve from 144.01224\n","Epoch 780/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1437 - val_loss: 225.6140\n","\n","Epoch 00780: val_loss did not improve from 144.01224\n","Epoch 781/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1666 - val_loss: 224.5802\n","\n","Epoch 00781: val_loss did not improve from 144.01224\n","Epoch 782/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1276 - val_loss: 224.9290\n","\n","Epoch 00782: val_loss did not improve from 144.01224\n","Epoch 783/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1264 - val_loss: 226.8006\n","\n","Epoch 00783: val_loss did not improve from 144.01224\n","Epoch 784/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1193 - val_loss: 226.4629\n","\n","Epoch 00784: val_loss did not improve from 144.01224\n","Epoch 785/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1308 - val_loss: 226.7071\n","\n","Epoch 00785: val_loss did not improve from 144.01224\n","Epoch 786/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1266 - val_loss: 227.2292\n","\n","Epoch 00786: val_loss did not improve from 144.01224\n","Epoch 787/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1244 - val_loss: 227.8184\n","\n","Epoch 00787: val_loss did not improve from 144.01224\n","Epoch 788/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1246 - val_loss: 228.7745\n","\n","Epoch 00788: val_loss did not improve from 144.01224\n","Epoch 789/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1275 - val_loss: 229.2743\n","\n","Epoch 00789: val_loss did not improve from 144.01224\n","Epoch 790/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1133 - val_loss: 228.2085\n","\n","Epoch 00790: val_loss did not improve from 144.01224\n","Epoch 791/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1311 - val_loss: 226.0145\n","\n","Epoch 00791: val_loss did not improve from 144.01224\n","Epoch 792/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1232 - val_loss: 226.4981\n","\n","Epoch 00792: val_loss did not improve from 144.01224\n","Epoch 793/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1273 - val_loss: 227.4460\n","\n","Epoch 00793: val_loss did not improve from 144.01224\n","Epoch 794/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1200 - val_loss: 227.3259\n","\n","Epoch 00794: val_loss did not improve from 144.01224\n","Epoch 795/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1265 - val_loss: 227.2658\n","\n","Epoch 00795: val_loss did not improve from 144.01224\n","Epoch 796/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1405 - val_loss: 227.4393\n","\n","Epoch 00796: val_loss did not improve from 144.01224\n","Epoch 797/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1314 - val_loss: 227.1345\n","\n","Epoch 00797: val_loss did not improve from 144.01224\n","Epoch 798/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1339 - val_loss: 225.2205\n","\n","Epoch 00798: val_loss did not improve from 144.01224\n","Epoch 799/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1319 - val_loss: 227.4948\n","\n","Epoch 00799: val_loss did not improve from 144.01224\n","Epoch 800/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1322 - val_loss: 229.9489\n","\n","Epoch 00800: val_loss did not improve from 144.01224\n","Epoch 801/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1274 - val_loss: 228.8531\n","\n","Epoch 00801: val_loss did not improve from 144.01224\n","Epoch 802/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1270 - val_loss: 228.3049\n","\n","Epoch 00802: val_loss did not improve from 144.01224\n","Epoch 803/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1278 - val_loss: 225.3362\n","\n","Epoch 00803: val_loss did not improve from 144.01224\n","Epoch 804/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1264 - val_loss: 224.1237\n","\n","Epoch 00804: val_loss did not improve from 144.01224\n","Epoch 805/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1390 - val_loss: 225.3014\n","\n","Epoch 00805: val_loss did not improve from 144.01224\n","Epoch 806/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1221 - val_loss: 225.6195\n","\n","Epoch 00806: val_loss did not improve from 144.01224\n","Epoch 807/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1350 - val_loss: 226.5157\n","\n","Epoch 00807: val_loss did not improve from 144.01224\n","Epoch 808/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1310 - val_loss: 223.8681\n","\n","Epoch 00808: val_loss did not improve from 144.01224\n","Epoch 809/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1338 - val_loss: 223.8201\n","\n","Epoch 00809: val_loss did not improve from 144.01224\n","Epoch 810/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1350 - val_loss: 224.8175\n","\n","Epoch 00810: val_loss did not improve from 144.01224\n","Epoch 811/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1492 - val_loss: 224.8431\n","\n","Epoch 00811: val_loss did not improve from 144.01224\n","Epoch 812/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1432 - val_loss: 226.4555\n","\n","Epoch 00812: val_loss did not improve from 144.01224\n","Epoch 813/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1333 - val_loss: 224.8332\n","\n","Epoch 00813: val_loss did not improve from 144.01224\n","Epoch 814/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1101 - val_loss: 225.9578\n","\n","Epoch 00814: val_loss did not improve from 144.01224\n","Epoch 815/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1133 - val_loss: 227.8003\n","\n","Epoch 00815: val_loss did not improve from 144.01224\n","Epoch 816/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1038 - val_loss: 229.2238\n","\n","Epoch 00816: val_loss did not improve from 144.01224\n","Epoch 817/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1232 - val_loss: 229.4924\n","\n","Epoch 00817: val_loss did not improve from 144.01224\n","Epoch 818/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1133 - val_loss: 228.6302\n","\n","Epoch 00818: val_loss did not improve from 144.01224\n","Epoch 819/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1098 - val_loss: 228.2163\n","\n","Epoch 00819: val_loss did not improve from 144.01224\n","Epoch 820/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1168 - val_loss: 227.4126\n","\n","Epoch 00820: val_loss did not improve from 144.01224\n","Epoch 821/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1089 - val_loss: 228.9443\n","\n","Epoch 00821: val_loss did not improve from 144.01224\n","Epoch 822/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1023 - val_loss: 230.1734\n","\n","Epoch 00822: val_loss did not improve from 144.01224\n","Epoch 823/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1320 - val_loss: 230.8111\n","\n","Epoch 00823: val_loss did not improve from 144.01224\n","Epoch 824/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1173 - val_loss: 230.1981\n","\n","Epoch 00824: val_loss did not improve from 144.01224\n","Epoch 825/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1379 - val_loss: 230.6040\n","\n","Epoch 00825: val_loss did not improve from 144.01224\n","Epoch 826/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1280 - val_loss: 232.0035\n","\n","Epoch 00826: val_loss did not improve from 144.01224\n","Epoch 827/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1256 - val_loss: 229.6144\n","\n","Epoch 00827: val_loss did not improve from 144.01224\n","Epoch 828/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1302 - val_loss: 227.5543\n","\n","Epoch 00828: val_loss did not improve from 144.01224\n","Epoch 829/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1168 - val_loss: 228.7639\n","\n","Epoch 00829: val_loss did not improve from 144.01224\n","Epoch 830/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1062 - val_loss: 231.5451\n","\n","Epoch 00830: val_loss did not improve from 144.01224\n","Epoch 831/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1278 - val_loss: 230.4040\n","\n","Epoch 00831: val_loss did not improve from 144.01224\n","Epoch 832/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1235 - val_loss: 231.2939\n","\n","Epoch 00832: val_loss did not improve from 144.01224\n","Epoch 833/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1172 - val_loss: 232.4627\n","\n","Epoch 00833: val_loss did not improve from 144.01224\n","Epoch 834/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1170 - val_loss: 233.7915\n","\n","Epoch 00834: val_loss did not improve from 144.01224\n","Epoch 835/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1388 - val_loss: 232.2198\n","\n","Epoch 00835: val_loss did not improve from 144.01224\n","Epoch 836/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1191 - val_loss: 229.4696\n","\n","Epoch 00836: val_loss did not improve from 144.01224\n","Epoch 837/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1127 - val_loss: 230.8240\n","\n","Epoch 00837: val_loss did not improve from 144.01224\n","Epoch 838/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1123 - val_loss: 230.9460\n","\n","Epoch 00838: val_loss did not improve from 144.01224\n","Epoch 839/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1248 - val_loss: 227.6875\n","\n","Epoch 00839: val_loss did not improve from 144.01224\n","Epoch 840/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1154 - val_loss: 228.5069\n","\n","Epoch 00840: val_loss did not improve from 144.01224\n","Epoch 841/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1124 - val_loss: 229.1681\n","\n","Epoch 00841: val_loss did not improve from 144.01224\n","Epoch 842/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1148 - val_loss: 230.1744\n","\n","Epoch 00842: val_loss did not improve from 144.01224\n","Epoch 843/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1215 - val_loss: 230.7482\n","\n","Epoch 00843: val_loss did not improve from 144.01224\n","Epoch 844/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1009 - val_loss: 229.7960\n","\n","Epoch 00844: val_loss did not improve from 144.01224\n","Epoch 845/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1236 - val_loss: 230.2760\n","\n","Epoch 00845: val_loss did not improve from 144.01224\n","Epoch 846/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1044 - val_loss: 231.9562\n","\n","Epoch 00846: val_loss did not improve from 144.01224\n","Epoch 847/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1131 - val_loss: 232.2728\n","\n","Epoch 00847: val_loss did not improve from 144.01224\n","Epoch 848/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1328 - val_loss: 231.7105\n","\n","Epoch 00848: val_loss did not improve from 144.01224\n","Epoch 849/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1104 - val_loss: 233.6808\n","\n","Epoch 00849: val_loss did not improve from 144.01224\n","Epoch 850/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1149 - val_loss: 231.7539\n","\n","Epoch 00850: val_loss did not improve from 144.01224\n","Epoch 851/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1106 - val_loss: 228.5983\n","\n","Epoch 00851: val_loss did not improve from 144.01224\n","Epoch 852/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1354 - val_loss: 231.4330\n","\n","Epoch 00852: val_loss did not improve from 144.01224\n","Epoch 853/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1458 - val_loss: 234.4762\n","\n","Epoch 00853: val_loss did not improve from 144.01224\n","Epoch 854/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1134 - val_loss: 228.1248\n","\n","Epoch 00854: val_loss did not improve from 144.01224\n","Epoch 855/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1550 - val_loss: 234.8701\n","\n","Epoch 00855: val_loss did not improve from 144.01224\n","Epoch 856/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.3316 - val_loss: 226.7080\n","\n","Epoch 00856: val_loss did not improve from 144.01224\n","Epoch 857/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.4178 - val_loss: 211.4290\n","\n","Epoch 00857: val_loss did not improve from 144.01224\n","Epoch 858/1000\n","12/12 [==============================] - 25s 2s/step - loss: 12.8447 - val_loss: 270.3267\n","\n","Epoch 00858: val_loss did not improve from 144.01224\n","Epoch 859/1000\n","12/12 [==============================] - 25s 2s/step - loss: 44.0867 - val_loss: 199.9230\n","\n","Epoch 00859: val_loss did not improve from 144.01224\n","Epoch 860/1000\n","12/12 [==============================] - 25s 2s/step - loss: 25.3871 - val_loss: 199.0092\n","\n","Epoch 00860: val_loss did not improve from 144.01224\n","Epoch 861/1000\n","12/12 [==============================] - 25s 2s/step - loss: 15.1789 - val_loss: 181.6817\n","\n","Epoch 00861: val_loss did not improve from 144.01224\n","Epoch 862/1000\n","12/12 [==============================] - 25s 2s/step - loss: 7.4171 - val_loss: 214.7263\n","\n","Epoch 00862: val_loss did not improve from 144.01224\n","Epoch 863/1000\n","12/12 [==============================] - 25s 2s/step - loss: 6.1641 - val_loss: 189.6028\n","\n","Epoch 00863: val_loss did not improve from 144.01224\n","Epoch 864/1000\n","12/12 [==============================] - 25s 2s/step - loss: 3.4824 - val_loss: 193.5190\n","\n","Epoch 00864: val_loss did not improve from 144.01224\n","Epoch 865/1000\n","12/12 [==============================] - 25s 2s/step - loss: 2.1623 - val_loss: 195.3965\n","\n","Epoch 00865: val_loss did not improve from 144.01224\n","Epoch 866/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.4452 - val_loss: 198.9191\n","\n","Epoch 00866: val_loss did not improve from 144.01224\n","Epoch 867/1000\n","12/12 [==============================] - 25s 2s/step - loss: 1.0752 - val_loss: 199.0963\n","\n","Epoch 00867: val_loss did not improve from 144.01224\n","Epoch 868/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.8984 - val_loss: 199.5276\n","\n","Epoch 00868: val_loss did not improve from 144.01224\n","Epoch 869/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.7648 - val_loss: 200.5757\n","\n","Epoch 00869: val_loss did not improve from 144.01224\n","Epoch 870/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.6586 - val_loss: 201.3172\n","\n","Epoch 00870: val_loss did not improve from 144.01224\n","Epoch 871/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.6011 - val_loss: 201.8053\n","\n","Epoch 00871: val_loss did not improve from 144.01224\n","Epoch 872/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.5199 - val_loss: 201.8161\n","\n","Epoch 00872: val_loss did not improve from 144.01224\n","Epoch 873/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.4763 - val_loss: 202.2459\n","\n","Epoch 00873: val_loss did not improve from 144.01224\n","Epoch 874/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.4490 - val_loss: 202.7325\n","\n","Epoch 00874: val_loss did not improve from 144.01224\n","Epoch 875/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.4069 - val_loss: 203.4224\n","\n","Epoch 00875: val_loss did not improve from 144.01224\n","Epoch 876/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.4123 - val_loss: 203.3047\n","\n","Epoch 00876: val_loss did not improve from 144.01224\n","Epoch 877/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.3797 - val_loss: 202.6762\n","\n","Epoch 00877: val_loss did not improve from 144.01224\n","Epoch 878/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.3687 - val_loss: 203.6627\n","\n","Epoch 00878: val_loss did not improve from 144.01224\n","Epoch 879/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.3465 - val_loss: 205.0966\n","\n","Epoch 00879: val_loss did not improve from 144.01224\n","Epoch 880/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.3551 - val_loss: 204.7568\n","\n","Epoch 00880: val_loss did not improve from 144.01224\n","Epoch 881/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.3160 - val_loss: 205.9698\n","\n","Epoch 00881: val_loss did not improve from 144.01224\n","Epoch 882/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2863 - val_loss: 207.2578\n","\n","Epoch 00882: val_loss did not improve from 144.01224\n","Epoch 883/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2950 - val_loss: 206.9449\n","\n","Epoch 00883: val_loss did not improve from 144.01224\n","Epoch 884/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.3022 - val_loss: 206.5902\n","\n","Epoch 00884: val_loss did not improve from 144.01224\n","Epoch 885/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2685 - val_loss: 206.9204\n","\n","Epoch 00885: val_loss did not improve from 144.01224\n","Epoch 886/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2621 - val_loss: 207.5795\n","\n","Epoch 00886: val_loss did not improve from 144.01224\n","Epoch 887/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2705 - val_loss: 207.8854\n","\n","Epoch 00887: val_loss did not improve from 144.01224\n","Epoch 888/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2468 - val_loss: 207.4047\n","\n","Epoch 00888: val_loss did not improve from 144.01224\n","Epoch 889/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2385 - val_loss: 208.3055\n","\n","Epoch 00889: val_loss did not improve from 144.01224\n","Epoch 890/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2422 - val_loss: 208.6963\n","\n","Epoch 00890: val_loss did not improve from 144.01224\n","Epoch 891/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2422 - val_loss: 209.2064\n","\n","Epoch 00891: val_loss did not improve from 144.01224\n","Epoch 892/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2404 - val_loss: 209.7455\n","\n","Epoch 00892: val_loss did not improve from 144.01224\n","Epoch 893/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2262 - val_loss: 210.5136\n","\n","Epoch 00893: val_loss did not improve from 144.01224\n","Epoch 894/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2208 - val_loss: 210.4521\n","\n","Epoch 00894: val_loss did not improve from 144.01224\n","Epoch 895/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2319 - val_loss: 210.2450\n","\n","Epoch 00895: val_loss did not improve from 144.01224\n","Epoch 896/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2209 - val_loss: 210.0872\n","\n","Epoch 00896: val_loss did not improve from 144.01224\n","Epoch 897/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2095 - val_loss: 210.2777\n","\n","Epoch 00897: val_loss did not improve from 144.01224\n","Epoch 898/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2183 - val_loss: 210.8453\n","\n","Epoch 00898: val_loss did not improve from 144.01224\n","Epoch 899/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2106 - val_loss: 211.4156\n","\n","Epoch 00899: val_loss did not improve from 144.01224\n","Epoch 900/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1936 - val_loss: 211.4905\n","\n","Epoch 00900: val_loss did not improve from 144.01224\n","Epoch 901/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2022 - val_loss: 211.6358\n","\n","Epoch 00901: val_loss did not improve from 144.01224\n","Epoch 902/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2315 - val_loss: 211.6230\n","\n","Epoch 00902: val_loss did not improve from 144.01224\n","Epoch 903/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1993 - val_loss: 211.9064\n","\n","Epoch 00903: val_loss did not improve from 144.01224\n","Epoch 904/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.2169 - val_loss: 212.1353\n","\n","Epoch 00904: val_loss did not improve from 144.01224\n","Epoch 905/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1814 - val_loss: 213.2546\n","\n","Epoch 00905: val_loss did not improve from 144.01224\n","Epoch 906/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1894 - val_loss: 213.3973\n","\n","Epoch 00906: val_loss did not improve from 144.01224\n","Epoch 907/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1914 - val_loss: 213.4510\n","\n","Epoch 00907: val_loss did not improve from 144.01224\n","Epoch 908/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1797 - val_loss: 213.4019\n","\n","Epoch 00908: val_loss did not improve from 144.01224\n","Epoch 909/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1750 - val_loss: 213.2290\n","\n","Epoch 00909: val_loss did not improve from 144.01224\n","Epoch 910/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1904 - val_loss: 213.3383\n","\n","Epoch 00910: val_loss did not improve from 144.01224\n","Epoch 911/1000\n","12/12 [==============================] - 25s 2s/step - loss: 0.1906 - val_loss: 213.2731\n","\n","Epoch 00911: val_loss did not improve from 144.01224\n","Epoch 912/1000\n"," 1/12 [=>............................] - ETA: 25s - loss: 0.1428"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xgPlBkSrGAFi","executionInfo":{"status":"ok","timestamp":1615807571311,"user_tz":-360,"elapsed":25477,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"2a906697-85e4-4b2c-f749-86f281662f1f"},"source":["!mkdir -p saved_model\r\n","model.save('saved_model/vgg19')"],"execution_count":36,"outputs":[{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as lstm_cell_52_layer_call_and_return_conditional_losses, lstm_cell_52_layer_call_fn, lstm_cell_53_layer_call_and_return_conditional_losses, lstm_cell_53_layer_call_fn, lstm_cell_55_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n","WARNING:absl:Found untraced functions such as lstm_cell_52_layer_call_and_return_conditional_losses, lstm_cell_52_layer_call_fn, lstm_cell_53_layer_call_and_return_conditional_losses, lstm_cell_53_layer_call_fn, lstm_cell_55_layer_call_and_return_conditional_losses while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: saved_model/vgg19/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: saved_model/vgg19/assets\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"KlHsiifdy5uv","colab":{"base_uri":"https://localhost:8080/","height":598},"executionInfo":{"status":"ok","timestamp":1614692805767,"user_tz":-360,"elapsed":3039,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"960bf945-6dcc-4ea6-fc93-eb42d62e09d5"},"source":["# Get the prediction model by extracting layers till the output layer\r\n","prediction_model = keras.models.Model(\r\n","    model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output\r\n",")\r\n","prediction_model.summary()\r\n","\r\n","# A utility function to decode the output of the network\r\n","def decode_batch_predictions(pred):\r\n","    input_len = np.ones(pred.shape[0]) * pred.shape[1]\r\n","    # Use greedy search. For complex tasks, you can use beam search\r\n","    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=False, beam_width=100)[0][0][\r\n","        :, :max_length\r\n","    ]\r\n","    # Iterate over the results and get back the text\r\n","    output_text = []\r\n","    for res in results:\r\n","        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\r\n","        output_text.append(res)\r\n","    return output_text\r\n","\r\n","for batch in validation_dataset.take(1):\r\n","    batch_images = batch[\"image\"]\r\n","    batch_labels = batch[\"label\"]\r\n","\r\n","    preds = prediction_model.predict(batch_images)\r\n","    pred_texts = decode_batch_predictions(preds)\r\n","    print(pred_texts)\r\n","    for i in range(1):\r\n","        img = (batch_images[i] * 255).numpy().astype(\"uint8\")\r\n","        # print(img.shape) # (200, 50, 1)\r\n","        plt.imshow(img[:, :, 0].T, cmap=\"gray\")\r\n","        plt.axis(\"off\")\r\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","image (InputLayer)           [(None, 2452, 144, 1)]    0         \n","_________________________________________________________________\n","Conv1 (Conv2D)               (None, 2452, 144, 32)     320       \n","_________________________________________________________________\n","pool1 (MaxPooling2D)         (None, 1226, 72, 32)      0         \n","_________________________________________________________________\n","Conv2 (Conv2D)               (None, 1226, 72, 64)      18496     \n","_________________________________________________________________\n","pool2 (MaxPooling2D)         (None, 613, 36, 64)       0         \n","_________________________________________________________________\n","reshape (Reshape)            (None, 613, 2304)         0         \n","_________________________________________________________________\n","dense1 (Dense)               (None, 613, 512)          1180160   \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 613, 512)          0         \n","_________________________________________________________________\n","bidirectional_12 (Bidirectio (None, 613, 1024)         4198400   \n","_________________________________________________________________\n","bidirectional_13 (Bidirectio (None, 613, 1024)         6295552   \n","_________________________________________________________________\n","dense2 (Dense)               (None, 613, 99)           101475    \n","=================================================================\n","Total params: 11,794,403\n","Trainable params: 11,794,403\n","Non-trainable params: 0\n","_________________________________________________________________\n","[' [UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]']\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"-gRufb3jy5uv","colab":{"base_uri":"https://localhost:8080/","height":598},"executionInfo":{"status":"ok","timestamp":1615458751432,"user_tz":-360,"elapsed":3530,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"a02f9a4a-f881-4993-b9ce-95901e837dc9"},"source":["test_datafile = open('./Data/ICBOCR-D4/custom-images/Groundtruth.txt', encoding='utf8')\r\n","test_lines = [line.rstrip() for line in test_datafile]\r\n","\r\n","test_image_dir = \"./Data/ICBOCR-D4/custom-images/Line_images/\"\r\n","test_image_paths = [os.path.join(test_image_dir, line.split('@')[0]) for line in test_lines]\r\n","test_captions = [line.split('@')[1].lstrip() for line in test_lines]\r\n","\r\n","\r\n","x_test, _, y_test, _ = split_data(np.array(test_image_paths), np.array(test_captions), train_size=1.0, shuffle=True)\r\n","\r\n","batch_size = 1\r\n","test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\r\n","test_dataset = test_dataset.map(\r\n","        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\r\n","    ).batch(batch_size)#.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n","\r\n","# Get the prediction model by extracting layers till the output layer\r\n","prediction_model = keras.models.Model(\r\n","    model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output\r\n",")\r\n","prediction_model.summary()\r\n","\r\n","# A utility function to decode the output of the network\r\n","def decode_batch_predictions(pred):\r\n","    input_len = np.ones(pred.shape[0]) * pred.shape[1]\r\n","    # Use greedy search. For complex tasks, you can use beam search\r\n","    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=False, beam_width=100)[0][0][\r\n","        :, :max_length\r\n","    ]\r\n","    # Iterate over the results and get back the text\r\n","    output_text = []\r\n","    for res in results:\r\n","        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\r\n","        output_text.append(res)\r\n","    return output_text\r\n","\r\n","for batch in test_dataset.take(1):\r\n","    batch_images = batch[\"image\"]\r\n","    batch_labels = batch[\"label\"]\r\n","\r\n","    preds = prediction_model.predict(batch_images)\r\n","    pred_texts = decode_batch_predictions(preds)\r\n","    print(pred_texts)\r\n","    for i in range(1):\r\n","        img = (batch_images[i] * 255).numpy().astype(\"uint8\")\r\n","        # print(img.shape) # (200, 50, 1)\r\n","        plt.imshow(img[:, :, 0].T, cmap=\"gray\")\r\n","        plt.axis(\"off\")\r\n","plt.show()\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","image (InputLayer)           [(None, 2452, 144, 1)]    0         \n","_________________________________________________________________\n","Conv1 (Conv2D)               (None, 2452, 144, 32)     320       \n","_________________________________________________________________\n","pool1 (MaxPooling2D)         (None, 1226, 72, 32)      0         \n","_________________________________________________________________\n","Conv2 (Conv2D)               (None, 1226, 72, 64)      18496     \n","_________________________________________________________________\n","pool2 (MaxPooling2D)         (None, 613, 36, 64)       0         \n","_________________________________________________________________\n","reshape (Reshape)            (None, 613, 2304)         0         \n","_________________________________________________________________\n","dense1 (Dense)               (None, 613, 512)          1180160   \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 613, 512)          0         \n","_________________________________________________________________\n","bidirectional_2 (Bidirection (None, 613, 1024)         4198400   \n","_________________________________________________________________\n","bidirectional_3 (Bidirection (None, 613, 1024)         6295552   \n","_________________________________________________________________\n","dense2 (Dense)               (None, 613, 103)          105575    \n","=================================================================\n","Total params: 11,798,503\n","Trainable params: 11,798,503\n","Non-trainable params: 0\n","_________________________________________________________________\n","['খন\"[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]']\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"Pte4vaRCy5uv"},"source":[""],"execution_count":null,"outputs":[]}]}