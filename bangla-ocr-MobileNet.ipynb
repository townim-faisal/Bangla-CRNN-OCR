{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"bangla-ocr-MobileNet.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e1KzxL_BzASM","executionInfo":{"status":"ok","timestamp":1615810884966,"user_tz":-360,"elapsed":17775,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"a33d58f4-adf3-454e-a6ea-86b64332d514"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YcsywByoy65s","executionInfo":{"status":"ok","timestamp":1615810884967,"user_tz":-360,"elapsed":3225,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"2e1fd667-1a7a-46fc-e7d7-a5cfe6104b37"},"source":["%cd '/content/drive/MyDrive/Colab Notebooks/bilstm-ctc'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/bilstm-ctc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l6CwQQr1y5uf","executionInfo":{"status":"ok","timestamp":1615810886719,"user_tz":-360,"elapsed":4165,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}}},"source":["import os, sys\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from pathlib import Path\n","from collections import Counter\n","\n","import tensorflow as tf\n","# tf.config.run_functions_eagerly(True)\n","from tensorflow import keras\n","from tensorflow.keras import layers\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oWSoH-Dzy5uk","executionInfo":{"status":"ok","timestamp":1614692347228,"user_tz":-360,"elapsed":1087,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"abeca241-1c8d-4bb8-bdf0-3f539cdb0f83"},"source":["datafile = open('./Data/ICBOCR-D4/Train/Groundtruth.txt', encoding='utf8')\n","line = datafile.readline().split('@')[1].rstrip()\n","print(line, len(\"পিঞ্জরের\"))\n","line = line.encode(\"unicode-escape\").decode()\n","print(line, len(line))\n","print('প'+'ি'+'ঞ'+'্'+'জ'+'র'+'ে'+'র')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["বনের পাখির চেয়ে পিঞ্জরের পাখিটাই বেশি ছটফট করে ! সুরেন্দ্র 8\n","\\u09ac\\u09a8\\u09c7\\u09b0 \\u09aa\\u09be\\u0996\\u09bf\\u09b0 \\u099a\\u09c7\\u09df\\u09c7 \\u09aa\\u09bf\\u099e\\u09cd\\u099c\\u09b0\\u09c7\\u09b0 \\u09aa\\u09be\\u0996\\u09bf\\u099f\\u09be\\u0987 \\u09ac\\u09c7\\u09b6\\u09bf \\u099b\\u099f\\u09ab\\u099f \\u0995\\u09b0\\u09c7 ! \\u09b8\\u09c1\\u09b0\\u09c7\\u09a8\\u09cd\\u09a6\\u09cd\\u09b0 298\n","পিঞ্জরের\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3UuFx8q0y5um","executionInfo":{"status":"ok","timestamp":1615810887624,"user_tz":-360,"elapsed":3305,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"62b0fc48-c5a3-4164-92ef-9a3c95695439"},"source":["train_datafile = open('./Data/ICBOCR-D4/Train/Groundtruth.txt', encoding='utf8')\n","test_datafile = open('./Data/ICBOCR-D4/Test/Groundtruth.txt', encoding='utf8')\n","train_lines = [line.rstrip() for line in train_datafile]\n","test_lines = [line.rstrip() for line in test_datafile]\n","\n","train_image_dir = \"./Data/ICBOCR-D4/Train/Line_Images/\"\n","train_image_paths = [os.path.join(train_image_dir, line.split('@')[0]) for line in train_lines]\n","train_captions = [line.split('@')[1].lstrip() for line in train_lines]\n","\n","test_image_dir = \"./Data/ICBOCR-D4/Test/Line_Images/\"\n","for i in range(len(test_lines)):\n","  try:\n","    train_captions.append(test_lines[i].split('@')[1].lstrip())\n","    train_image_paths.append(os.path.join(test_image_dir, test_lines[i].split('@')[0]))\n","  except:\n","    print(i+1, test_lines[i])\n","\n","# for caption in train_captions:\n","#   print(caption)\n","print(len(train_image_paths), len(train_captions))\n","characters = set(char for label in train_captions for char in label)\n","print(len(characters), characters)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["200 200\n","102 {'৭', 'ণ', 'h', 'ৎ', 'l', 'ম', 'উ', '।', 'f', '০', '’', 'n', 'ই', '!', 'ে', 'া', '২', 'g', '?', 'দ', 'অ', '.', '—', 'ঝ', 'ও', ' ', '-', 'গ', 'ু', 'ঠ', 'ঢ', 'ক', '\\u200c', ';', '়', 'খ', '”', 'ৌ', '৩', 'ঙ', '৮', 'ো', 'ব', 'ভ', 'I', 'স', '\\t', '–', 'ঞ', 'ট', '“', 'm', 'হ', 'i', 'L', 'ধ', 'ি', 'ড়', 'ং', 'চ', 'ঁ', 'ৃ', 'ত', 'r', 'প', '্', 'ঘ', 'য', 'ৰ', \"'\", 'e', '৫', 'ছ', 'শ', 'ল', 'y', 'এ', 'ঔ', 'o', 's', 'w', 'ঃ', 'জ', 'থ', ',', '৯', '১', '4', 'ূ', 'ড', 'ন', 'ফ', 'u', 'য়', 't', 'ী', 'v', 'ৈ', 'ষ', 'র', '\"', 'আ'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kIxkTTg7y5um","executionInfo":{"status":"ok","timestamp":1614678103092,"user_tz":-360,"elapsed":937,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"cde74b55-e41d-4946-e98f-e808a68bb70a"},"source":["f = open('./Dict/AllCharcaters.txt', encoding='utf-8')\n","lines = [line.rstrip().split(',')[1] for line in f]\n","lines = set(lines)\n","print(len(lines))\n","print(lines - characters)\n","print(characters-lines)\n","characters = lines.union(characters)\n","print(len(characters), characters)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["84\n","{'ৎ', '—', 'ঙ', '৬', '•', 'ঋ', '০', 'ড', 'ৌ', '\\u200d', 'ৗ', 'ঁ', 'ঐ', '॥', '\\u200c', 'ঈ', 'ঃ', 'ং', 'ৰ', 'ঊ', '৮', '়', '১', '৪', '‘', '৭', '৯', 'ঔ', '২', 'ঢ', '৩', '৫', '\\ufeff', 'ৱ', 'ৈ', 'ঢ়'}\n","{' ', '?', '!', ',', '-'}\n","89 {'?', 'া', '।', 'ঙ', 'চ', 'ু', 'আ', 'ঋ', '•', '০', 'ঝ', 'ূ', 'ঘ', '্', 'ো', 'ধ', 'য', 'ড', 'গ', 'ভ', 'অ', 'উ', 'ঐ', 'ঈ', 'ং', 'ৰ', '৮', 'প', 'স', 'খ', 'ট', '৪', '‘', 'ছ', 'ঠ', 'ষ', 'ঔ', '২', 'ঢ', 'ত', '\\ufeff', 'ৱ', 'ৈ', 'ম', 'ৎ', ' ', 'ি', '—', '৬', 'ও', 'হ', 'ক', 'ে', 'শ', 'ৌ', 'য়', 'দ', '\\u200d', 'ৗ', 'ল', 'জ', 'ঁ', 'থ', '!', 'ই', '॥', '\\u200c', '-', 'ঃ', 'ী', 'ঊ', 'ড়', 'এ', '–', '়', 'ঞ', '১', '৭', '৯', 'র', 'ৃ', '৩', 'ব', '৫', 'ন', ',', 'ঢ়', 'ণ', 'ফ'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0EpMcw4My5un","executionInfo":{"status":"ok","timestamp":1615810889508,"user_tz":-360,"elapsed":1292,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"0b2d41c8-0742-4446-b3e4-21dd7a1db28a"},"source":["# characters= lines\n","print(\"Number of images found: \", len(train_image_paths))\n","print(\"Number of labels found: \", len(train_captions))\n","print(\"Number of unique characters: \", len(characters))\n","print(\"Characters present: \", characters)\n","\n","# Batch size for training and validation\n","batch_size = 16\n","\n","# Desired image dimensions\n","img_width = 2452#224#2452\n","img_height = 144#224#144\n","\n","# Factor by which the image is going to be downsampled\n","# by the convolutional blocks. We will be using two\n","# convolution blocks and each block will have\n","# a pooling layer which downsample the features by a factor of 2.\n","# Hence total downsampling factor would be 4.\n","downsample_factor = 4\n","\n","# Maximum length of any captcha in the dataset\n","max_length = max([len(label) for label in train_captions])\n","print(\"Maximum length: \",max_length)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Number of images found:  200\n","Number of labels found:  200\n","Number of unique characters:  102\n","Characters present:  {'৭', 'ণ', 'h', 'ৎ', 'l', 'ম', 'উ', '।', 'f', '০', '’', 'n', 'ই', '!', 'ে', 'া', '২', 'g', '?', 'দ', 'অ', '.', '—', 'ঝ', 'ও', ' ', '-', 'গ', 'ু', 'ঠ', 'ঢ', 'ক', '\\u200c', ';', '়', 'খ', '”', 'ৌ', '৩', 'ঙ', '৮', 'ো', 'ব', 'ভ', 'I', 'স', '\\t', '–', 'ঞ', 'ট', '“', 'm', 'হ', 'i', 'L', 'ধ', 'ি', 'ড়', 'ং', 'চ', 'ঁ', 'ৃ', 'ত', 'r', 'প', '্', 'ঘ', 'য', 'ৰ', \"'\", 'e', '৫', 'ছ', 'শ', 'ল', 'y', 'এ', 'ঔ', 'o', 's', 'w', 'ঃ', 'জ', 'থ', ',', '৯', '১', '4', 'ূ', 'ড', 'ন', 'ফ', 'u', 'য়', 't', 'ী', 'v', 'ৈ', 'ষ', 'র', '\"', 'আ'}\n","Maximum length:  75\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9n4F5kceseeM","executionInfo":{"status":"ok","timestamp":1615810891489,"user_tz":-360,"elapsed":912,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}}},"source":["def new_py_function(func, inp, Tout, name=None):\r\n","  def wrapped_func(*flat_inp):\r\n","    reconstructed_inp = tf.nest.pack_sequence_as(inp, flat_inp,expand_composites=True)\r\n","    out = func(*reconstructed_inp)\r\n","    return tf.nest.flatten(out, expand_composites=True)\r\n","  flat_Tout = tf.nest.flatten(Tout, expand_composites=True)\r\n","  flat_out = tf.py_function(\r\n","      func=wrapped_func, \r\n","      inp=tf.nest.flatten(inp, expand_composites=True),\r\n","      Tout=[_tensor_spec_to_dtype(v) for v in flat_Tout],\r\n","      name=name)\r\n","  spec_out = tf.nest.map_structure(_dtype_to_tensor_spec, Tout, \r\n","                                   expand_composites=True)\r\n","  out = tf.nest.pack_sequence_as(spec_out, flat_out, expand_composites=True)\r\n","  return out\r\n","\r\n","def _dtype_to_tensor_spec(v):\r\n","  return tf.TensorSpec(None, v) if isinstance(v, tf.dtypes.DType) else v\r\n","\r\n","def _tensor_spec_to_dtype(v):\r\n","  return v.dtype if isinstance(v, tf.TensorSpec) else v"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"6yNv88sMhBxN","executionInfo":{"status":"ok","timestamp":1615810898569,"user_tz":-360,"elapsed":6100,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}}},"source":["# Mapping characters to integers\n","char_to_num = layers.experimental.preprocessing.StringLookup(\n","    vocabulary=list(characters), num_oov_indices=0, mask_token=''\n",")\n","\n","# Mapping integers back to original characters\n","num_to_char = layers.experimental.preprocessing.StringLookup(\n","    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n",")\n","\n","def label_to_tensor(st):\n","    f_list = list(characters)\n","    f_list.sort()\n","    st_list = []\n","    for s in st:\n","        s_index = f_list.index(s)\n","        st_list.append(s_index)\n","    return tf.convert_to_tensor(st_list, dtype=tf.float32)\n","\n","def split_data(images, labels, train_size=0.9, shuffle=True):\n","    # 1. Get the total size of the dataset\n","    size = len(images)\n","    # 2. Make an indices array and shuffle it, if required\n","    indices = np.arange(size)\n","    if shuffle:\n","        np.random.shuffle(indices)\n","    # 3. Get the size of training samples\n","    train_samples = int(size * train_size)\n","    # 4. Split data into training and validation sets\n","    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n","    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\n","    return x_train, x_valid, y_train, y_valid\n","\n","\n","# Splitting data into training and validation sets\n","x_train, x_valid, y_train, y_valid = split_data(np.array(train_image_paths), np.array(train_captions))\n","\n","\n","def encode_single_sample(img_path, label):\n","    # 1. Read image\n","    img = tf.io.read_file(img_path)\n","    # 2. Decode and convert to grayscale\n","    img = tf.io.decode_jpeg(img, channels=3)\n","    # 3. Convert to float32 in [0, 1] range\n","    img = tf.image.convert_image_dtype(img, tf.float32)\n","    # 4. Resize to the desired size\n","    img = tf.image.resize(img, [img_height, img_width])\n","    img = tf.image.per_image_standardization(img) # normalize data\n","    # 5. Transpose the image because we want the time dimension to correspond to the width of the image.\n","    img = tf.transpose(img, perm=[1, 0, 2])\n","    # 6. Map the characters in label to numbers\n","    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n","    # label = label.numpy()\n","    label = tf.keras.preprocessing.sequence.pad_sequences([label.numpy()], maxlen=max_length, padding='post').squeeze()\n","    # 7. Return a dict as our model is expecting two inputs\n","    return {\"image\": img, \"label\": label}\n","\n","def pad_map_fn(img_path, label):\n","    return new_py_function(encode_single_sample, inp=(img_path, label), Tout=({\"image\": tf.float32, \"label\": tf.int32}))"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"nLE-Vp-7AeAe"},"source":["sample_train_x, sample_train_y = [], []\r\n","for i in range(len(x_train)):\r\n","  sample = encode_single_sample(x_train[i], y_train[i])\r\n","  sample_train_x.append(np.asarray(sample['image']))\r\n","  label = np.asarray(sample['label'])\r\n","  label = np.expand_dims(label, -1)\r\n","  sample_train_y.append(label)\r\n","sample_train_x = np.asarray(sample_train_x)/255.0\r\n","sample_train_y = np.asarray(sample_train_y)\r\n","sample_train_y = np.expand_dims(sample_train_y, -1)\r\n","\r\n","print(sample_train_x.shape)\r\n","print(sample_train_y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lDFgzzLzhRue"},"source":["y_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nlKmv6cdy5uo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615810904496,"user_tz":-360,"elapsed":5899,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"af59299f-a893-4362-fdd2-9bf20ab16756"},"source":["batch_size = 16\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","train_dataset = train_dataset.map(\n","        pad_map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n","    ).batch(batch_size)#.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","\n","validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n","validation_dataset = validation_dataset.map(\n","        pad_map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE\n","    ).batch(batch_size)\n","\n","dataset = train_dataset.take(1)\n","# list(dataset.as_numpy_iterator())\n","a = list(dataset.as_numpy_iterator())\n","for a in list(dataset.as_numpy_iterator()):\n","  print(a['image'].shape, a['label'].shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["(16, 2452, 144, 3) (16, 75)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FtVyh4ZOy5uo","executionInfo":{"status":"ok","timestamp":1615455887011,"user_tz":-360,"elapsed":861,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"f30d06c7-6d8c-454a-a189-238fc7dac703"},"source":["# dataset = train_dataset.take(1)\r\n","# # list(dataset.as_numpy_iterator())\r\n","# a = list(dataset.as_numpy_iterator())\r\n","# a[0]['image'].shape, a[0]['label'].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1, 2452, 144, 1), (1, 51))"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"UGTSQr_utfRp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615541261369,"user_tz":-360,"elapsed":857,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"321dfbb4-cabe-41c1-b390-eed1b74fdc77"},"source":["# ragged tensor\r\n","batch_size = 16\r\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n","train_dataset = train_dataset.map(\r\n","        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\r\n","    ).apply(tf.data.experimental.dense_to_ragged_batch(batch_size=batch_size))\r\n","\r\n","\r\n","validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\r\n","validation_dataset = validation_dataset.map(\r\n","        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\r\n","    ).apply(tf.data.experimental.dense_to_ragged_batch(batch_size=batch_size))\r\n","\r\n","dataset = train_dataset.take(1)\r\n","for a in list(dataset.as_numpy_iterator()):\r\n","  print(a['image'].shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(16, 2452, 144, 1)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ragged/ragged_tensor.py:2012: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return np.array(rows)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"kHKp3EwCy5up"},"source":["plt.rc('font')\n","for batch in train_dataset.take(1):\n","    images = batch[\"image\"]\n","    labels = batch[\"label\"]\n","    print(labels)\n","    for i in range(1):\n","        img = (images[i] * 255).numpy().astype(\"uint8\")\n","        print(img.shape) # (200, 50, 1)\n","        label = tf.strings.reduce_join(num_to_char(labels[i])).numpy().decode()\n","        plt.imshow(img[:, :, 0].T, cmap=\"gray\")\n","        plt.title(label)\n","        plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ocAzBnSzzRKD","executionInfo":{"status":"ok","timestamp":1614679263461,"user_tz":-360,"elapsed":983,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"a1fd7471-d8b1-42fe-c42c-783e789acbc9"},"source":["###### TEST EXAMPLE\r\n","paragraph1 = np.random.random((20, 10, 50)).astype(np.float32)\r\n","paragraph2 = np.random.random((20, 10, 50)).astype(np.float32)\r\n","paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)\r\n","\r\n","lstm_layer = layers.Bidirectional(layers.LSTM(512, return_sequences=True, dropout=0.25))\r\n","linear_layer = layers.Dense(512, activation=\"relu\", name=\"dense1\")\r\n","output = lstm_layer(paragraph1)\r\n","output = lstm_layer(paragraph2)\r\n","output = lstm_layer(paragraph3)\r\n","print(paragraph1.shape, output.shape, linear_layer(paragraph1).shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(20, 10, 50) (20, 10, 1024) (20, 10, 512)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sQNyWJVay5up","executionInfo":{"status":"ok","timestamp":1615810908324,"user_tz":-360,"elapsed":3811,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"377c8a14-e89e-417b-8e17-1158258b9757"},"source":["class CTCLayer(layers.Layer):\n","    def __init__(self, name=None):\n","        super().__init__(name=name)\n","        self.loss_fn = keras.backend.ctc_batch_cost\n","\n","    def call(self, y_true, y_pred):\n","        # Compute the training-time loss value and add it to the layer using `self.add_loss()`.\n","        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n","        input_length =  tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n","        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n","        # print(y_true.shape, y_pred.shape)\n","        # batch_len =  nrows(y_true, 'int64')\n","        # input_length = y_pred.uniform_row_length\n","        # label_length = y_true.uniform_row_length\n","\n","        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n","        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n","\n","        loss = self.loss_fn(y_true, y_pred, input_length, label_length) + 0.01\n","        self.add_loss(loss)\n","\n","        # At test time, just return the computed predictions\n","        return y_pred\n","\n","    # def loss(self, y_true, y_pred):\n","    #     \"\"\"The actual loss\"\"\"\n","\n","    #     batch_labels = y_true[:, :, 0]\n","    #     label_length = y_true[:, 0, 1]\n","    #     input_length = y_true[:, 0, 2]\n","\n","    #     #reshape for the loss, add that extra dimension\n","    #     label_length = tf.expand_dims(label_length, -1)\n","    #     input_length = tf.expand_dims(input_length, -1)\n","\n","    #     # use keras backend function for the loss\n","    #     return keras.backend.ctc_batch_cost(batch_labels, y_pred, input_length, label_length)\n","\n","\n","def build_model():\n","    # Inputs to the model\n","    input_img = layers.Input(shape=(img_width, img_height, 3), name=\"image\", dtype=\"float32\")\n","    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n","\n","    x = tf.keras.applications.MobileNetV2(include_top=False,weights=\"imagenet\",input_tensor=input_img)(input_img)\n","    \n","    new_shape = (77*5, 1280)\n","\n","    # We have used two max pool with pool size and strides 2.\n","    # Hence, downsampled feature maps are 4x smaller. The number of\n","    # filters in the last layer is 64. Reshape accordingly before\n","    # passing the output to the RNN part of the model\n","    # new_shape = ((img_width // 4), (img_height // 4) * 64)\n","    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n","    # x = layers.Dense(1024, activation=\"relu\", name=\"dense1\")(x)\n","    # x = layers.Dropout(0.2)(x)\n","    x = layers.Dense(512, activation=\"relu\", name=\"dense5\")(x)\n","    x = layers.Dropout(0.2)(x)\n","    # x = layers.Dense(64, activation=\"relu\", name=\"dense2\")(x)\n","    # x = layers.Dropout(0.2)(x)\n","\n","    # RNNs\n","    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True, dropout=0.25))(x)\n","    # x = layers.Bidirectional(layers.LSTM(512, return_sequences=True, dropout=0.25))(x)\n","    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n","\n","    # Output layer\n","    x = layers.Dense(len(characters) + 2, activation=\"softmax\", name=\"dense3\")(x)\n","\n","    # Add CTC layer for calculating CTC loss at each step\n","    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n","\n","    # Define the model\n","    model = keras.models.Model(\n","        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n","    )\n","    # Optimizer\n","    opt = keras.optimizers.Adam(learning_rate=1e-4, decay=1e-6)\n","    # Compile the model and return\n","    model.compile(optimizer=opt)\n","    return model\n","\n","\n","# Get the model\n","model = build_model()\n","model.summary()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n","9412608/9406464 [==============================] - 0s 0us/step\n","Model: \"ocr_model_v1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","image (InputLayer)              [(None, 2452, 144, 3 0                                            \n","__________________________________________________________________________________________________\n","mobilenetv2_1.00_224 (Functiona (None, 77, 5, 1280)  2257984     image[0][0]                      \n","__________________________________________________________________________________________________\n","reshape (Reshape)               (None, 385, 1280)    0           mobilenetv2_1.00_224[0][0]       \n","__________________________________________________________________________________________________\n","dense5 (Dense)                  (None, 385, 512)     655872      reshape[0][0]                    \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 385, 512)     0           dense5[0][0]                     \n","__________________________________________________________________________________________________\n","bidirectional (Bidirectional)   (None, 385, 512)     1574912     dropout[0][0]                    \n","__________________________________________________________________________________________________\n","bidirectional_1 (Bidirectional) (None, 385, 256)     656384      bidirectional[0][0]              \n","__________________________________________________________________________________________________\n","label (InputLayer)              [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","dense3 (Dense)                  (None, 385, 104)     26728       bidirectional_1[0][0]            \n","__________________________________________________________________________________________________\n","ctc_loss (CTCLayer)             (None, 385, 104)     0           label[0][0]                      \n","                                                                 dense3[0][0]                     \n","==================================================================================================\n","Total params: 5,171,880\n","Trainable params: 5,137,768\n","Non-trainable params: 34,112\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ooJF5EcYhBxU","executionInfo":{"status":"ok","timestamp":1615817489120,"user_tz":-360,"elapsed":6551126,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"f3120f4e-fe9e-4371-98a3-36a9e9407695"},"source":["epochs = 500\n","early_stopping_patience = 10\n","# Add early stopping\n","early_stopping = keras.callbacks.EarlyStopping(\n","    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",")\n","\n","# Train the model\n","# history = model.fit(\n","#     x=[sample_train_x, sample_train_y], \n","#     y=sample_train_y, \n","#     batch_size=batch_size,\n","#     validation_split=0.1,\n","#     epochs=epochs\n","# )\n","\n","checkpoint_filepath = 'checkpoint/mobilenet/cp.ckpt'\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_filepath,\n","    save_weights_only=True,\n","    monitor='val_loss',\n","    mode='min', verbose=1,\n","    save_best_only=True)\n","\n","# model.load_weights(checkpoint_filepath)\n","history = model.fit(\n","    train_dataset,\n","    validation_data=validation_dataset,\n","    epochs=epochs,\n","    callbacks=[model_checkpoint_callback]\n",")"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Epoch 1/500\n","12/12 [==============================] - 75s 3s/step - loss: 1174.4093 - val_loss: 347.9620\n","\n","Epoch 00001: val_loss improved from inf to 347.96198, saving model to checkpoint/mobilenet/cp.ckpt\n","Epoch 2/500\n","12/12 [==============================] - 13s 1s/step - loss: 340.2082 - val_loss: 324.8961\n","\n","Epoch 00002: val_loss improved from 347.96198 to 324.89606, saving model to checkpoint/mobilenet/cp.ckpt\n","Epoch 3/500\n","12/12 [==============================] - 13s 1s/step - loss: 299.6862 - val_loss: 269.7519\n","\n","Epoch 00003: val_loss improved from 324.89606 to 269.75189, saving model to checkpoint/mobilenet/cp.ckpt\n","Epoch 4/500\n","12/12 [==============================] - 13s 1s/step - loss: 262.6338 - val_loss: 255.5472\n","\n","Epoch 00004: val_loss improved from 269.75189 to 255.54721, saving model to checkpoint/mobilenet/cp.ckpt\n","Epoch 5/500\n","12/12 [==============================] - 13s 1s/step - loss: 247.6657 - val_loss: 247.6996\n","\n","Epoch 00005: val_loss improved from 255.54721 to 247.69962, saving model to checkpoint/mobilenet/cp.ckpt\n","Epoch 6/500\n","12/12 [==============================] - 13s 1s/step - loss: 238.0358 - val_loss: 238.8528\n","\n","Epoch 00006: val_loss improved from 247.69962 to 238.85278, saving model to checkpoint/mobilenet/cp.ckpt\n","Epoch 7/500\n","12/12 [==============================] - 13s 1s/step - loss: 229.9512 - val_loss: 233.3952\n","\n","Epoch 00007: val_loss improved from 238.85278 to 233.39523, saving model to checkpoint/mobilenet/cp.ckpt\n","Epoch 8/500\n","12/12 [==============================] - 13s 1s/step - loss: 221.8298 - val_loss: 229.0046\n","\n","Epoch 00008: val_loss improved from 233.39523 to 229.00464, saving model to checkpoint/mobilenet/cp.ckpt\n","Epoch 9/500\n","12/12 [==============================] - 13s 1s/step - loss: 213.9168 - val_loss: 230.5549\n","\n","Epoch 00009: val_loss did not improve from 229.00464\n","Epoch 10/500\n","12/12 [==============================] - 13s 1s/step - loss: 204.5610 - val_loss: 243.1653\n","\n","Epoch 00010: val_loss did not improve from 229.00464\n","Epoch 11/500\n","12/12 [==============================] - 13s 1s/step - loss: 195.0363 - val_loss: 253.3066\n","\n","Epoch 00011: val_loss did not improve from 229.00464\n","Epoch 12/500\n","12/12 [==============================] - 13s 1s/step - loss: 188.1189 - val_loss: 264.0388\n","\n","Epoch 00012: val_loss did not improve from 229.00464\n","Epoch 13/500\n","12/12 [==============================] - 13s 1s/step - loss: 183.9297 - val_loss: 272.0237\n","\n","Epoch 00013: val_loss did not improve from 229.00464\n","Epoch 14/500\n","12/12 [==============================] - 13s 1s/step - loss: 181.2365 - val_loss: 276.7306\n","\n","Epoch 00014: val_loss did not improve from 229.00464\n","Epoch 15/500\n","12/12 [==============================] - 13s 1s/step - loss: 179.2387 - val_loss: 279.4093\n","\n","Epoch 00015: val_loss did not improve from 229.00464\n","Epoch 16/500\n","12/12 [==============================] - 13s 1s/step - loss: 177.8789 - val_loss: 282.9060\n","\n","Epoch 00016: val_loss did not improve from 229.00464\n","Epoch 17/500\n","12/12 [==============================] - 13s 1s/step - loss: 176.5942 - val_loss: 286.2620\n","\n","Epoch 00017: val_loss did not improve from 229.00464\n","Epoch 18/500\n","12/12 [==============================] - 13s 1s/step - loss: 175.6155 - val_loss: 286.1004\n","\n","Epoch 00018: val_loss did not improve from 229.00464\n","Epoch 19/500\n","12/12 [==============================] - 13s 1s/step - loss: 174.5229 - val_loss: 284.6728\n","\n","Epoch 00019: val_loss did not improve from 229.00464\n","Epoch 20/500\n","12/12 [==============================] - 13s 1s/step - loss: 173.4136 - val_loss: 286.3393\n","\n","Epoch 00020: val_loss did not improve from 229.00464\n","Epoch 21/500\n","12/12 [==============================] - 13s 1s/step - loss: 172.2847 - val_loss: 286.1272\n","\n","Epoch 00021: val_loss did not improve from 229.00464\n","Epoch 22/500\n","12/12 [==============================] - 13s 1s/step - loss: 172.2277 - val_loss: 273.1705\n","\n","Epoch 00022: val_loss did not improve from 229.00464\n","Epoch 23/500\n","12/12 [==============================] - 13s 1s/step - loss: 171.2125 - val_loss: 271.8702\n","\n","Epoch 00023: val_loss did not improve from 229.00464\n","Epoch 24/500\n","12/12 [==============================] - 13s 1s/step - loss: 170.6355 - val_loss: 291.4865\n","\n","Epoch 00024: val_loss did not improve from 229.00464\n","Epoch 25/500\n","12/12 [==============================] - 13s 1s/step - loss: 170.8461 - val_loss: 286.4512\n","\n","Epoch 00025: val_loss did not improve from 229.00464\n","Epoch 26/500\n","12/12 [==============================] - 13s 1s/step - loss: 169.7246 - val_loss: 286.7811\n","\n","Epoch 00026: val_loss did not improve from 229.00464\n","Epoch 27/500\n","12/12 [==============================] - 13s 1s/step - loss: 166.9244 - val_loss: 286.6676\n","\n","Epoch 00027: val_loss did not improve from 229.00464\n","Epoch 28/500\n","12/12 [==============================] - 13s 1s/step - loss: 164.3528 - val_loss: 286.5700\n","\n","Epoch 00028: val_loss did not improve from 229.00464\n","Epoch 29/500\n","12/12 [==============================] - 13s 1s/step - loss: 162.3169 - val_loss: 290.3760\n","\n","Epoch 00029: val_loss did not improve from 229.00464\n","Epoch 30/500\n","12/12 [==============================] - 13s 1s/step - loss: 161.2269 - val_loss: 289.2974\n","\n","Epoch 00030: val_loss did not improve from 229.00464\n","Epoch 31/500\n","12/12 [==============================] - 13s 1s/step - loss: 160.1083 - val_loss: 291.0630\n","\n","Epoch 00031: val_loss did not improve from 229.00464\n","Epoch 32/500\n","12/12 [==============================] - 13s 1s/step - loss: 159.0356 - val_loss: 300.9832\n","\n","Epoch 00032: val_loss did not improve from 229.00464\n","Epoch 33/500\n","12/12 [==============================] - 13s 1s/step - loss: 156.8622 - val_loss: 302.9315\n","\n","Epoch 00033: val_loss did not improve from 229.00464\n","Epoch 34/500\n","12/12 [==============================] - 13s 1s/step - loss: 155.0267 - val_loss: 300.9927\n","\n","Epoch 00034: val_loss did not improve from 229.00464\n","Epoch 35/500\n","12/12 [==============================] - 13s 1s/step - loss: 153.7178 - val_loss: 307.8699\n","\n","Epoch 00035: val_loss did not improve from 229.00464\n","Epoch 36/500\n","12/12 [==============================] - 13s 1s/step - loss: 153.3401 - val_loss: 299.1087\n","\n","Epoch 00036: val_loss did not improve from 229.00464\n","Epoch 37/500\n","12/12 [==============================] - 13s 1s/step - loss: 151.1717 - val_loss: 294.8243\n","\n","Epoch 00037: val_loss did not improve from 229.00464\n","Epoch 38/500\n","12/12 [==============================] - 13s 1s/step - loss: 148.6912 - val_loss: 302.3192\n","\n","Epoch 00038: val_loss did not improve from 229.00464\n","Epoch 39/500\n","12/12 [==============================] - 13s 1s/step - loss: 145.7993 - val_loss: 306.9059\n","\n","Epoch 00039: val_loss did not improve from 229.00464\n","Epoch 40/500\n","12/12 [==============================] - 13s 1s/step - loss: 142.8220 - val_loss: 306.9542\n","\n","Epoch 00040: val_loss did not improve from 229.00464\n","Epoch 41/500\n","12/12 [==============================] - 13s 1s/step - loss: 140.7834 - val_loss: 303.6172\n","\n","Epoch 00041: val_loss did not improve from 229.00464\n","Epoch 42/500\n","12/12 [==============================] - 13s 1s/step - loss: 138.9682 - val_loss: 303.1285\n","\n","Epoch 00042: val_loss did not improve from 229.00464\n","Epoch 43/500\n","12/12 [==============================] - 13s 1s/step - loss: 138.1447 - val_loss: 325.1313\n","\n","Epoch 00043: val_loss did not improve from 229.00464\n","Epoch 44/500\n","12/12 [==============================] - 13s 1s/step - loss: 136.5718 - val_loss: 311.7343\n","\n","Epoch 00044: val_loss did not improve from 229.00464\n","Epoch 45/500\n","12/12 [==============================] - 13s 1s/step - loss: 135.4855 - val_loss: 294.8058\n","\n","Epoch 00045: val_loss did not improve from 229.00464\n","Epoch 46/500\n","12/12 [==============================] - 13s 1s/step - loss: 133.6343 - val_loss: 318.5733\n","\n","Epoch 00046: val_loss did not improve from 229.00464\n","Epoch 47/500\n","12/12 [==============================] - 13s 1s/step - loss: 132.4270 - val_loss: 303.0914\n","\n","Epoch 00047: val_loss did not improve from 229.00464\n","Epoch 48/500\n","12/12 [==============================] - 13s 1s/step - loss: 130.1852 - val_loss: 302.9508\n","\n","Epoch 00048: val_loss did not improve from 229.00464\n","Epoch 49/500\n","12/12 [==============================] - 13s 1s/step - loss: 129.2409 - val_loss: 305.4768\n","\n","Epoch 00049: val_loss did not improve from 229.00464\n","Epoch 50/500\n","12/12 [==============================] - 13s 1s/step - loss: 127.0615 - val_loss: 291.1434\n","\n","Epoch 00050: val_loss did not improve from 229.00464\n","Epoch 51/500\n","12/12 [==============================] - 13s 1s/step - loss: 123.4676 - val_loss: 299.5306\n","\n","Epoch 00051: val_loss did not improve from 229.00464\n","Epoch 52/500\n","12/12 [==============================] - 13s 1s/step - loss: 122.0850 - val_loss: 315.0777\n","\n","Epoch 00052: val_loss did not improve from 229.00464\n","Epoch 53/500\n","12/12 [==============================] - 13s 1s/step - loss: 119.9727 - val_loss: 314.3802\n","\n","Epoch 00053: val_loss did not improve from 229.00464\n","Epoch 54/500\n","12/12 [==============================] - 13s 1s/step - loss: 118.5618 - val_loss: 317.8331\n","\n","Epoch 00054: val_loss did not improve from 229.00464\n","Epoch 55/500\n","12/12 [==============================] - 13s 1s/step - loss: 118.1214 - val_loss: 290.2346\n","\n","Epoch 00055: val_loss did not improve from 229.00464\n","Epoch 56/500\n","12/12 [==============================] - 13s 1s/step - loss: 115.8117 - val_loss: 289.8240\n","\n","Epoch 00056: val_loss did not improve from 229.00464\n","Epoch 57/500\n","12/12 [==============================] - 13s 1s/step - loss: 113.7933 - val_loss: 302.4231\n","\n","Epoch 00057: val_loss did not improve from 229.00464\n","Epoch 58/500\n","12/12 [==============================] - 13s 1s/step - loss: 111.6495 - val_loss: 304.2007\n","\n","Epoch 00058: val_loss did not improve from 229.00464\n","Epoch 59/500\n","12/12 [==============================] - 13s 1s/step - loss: 108.6265 - val_loss: 294.2511\n","\n","Epoch 00059: val_loss did not improve from 229.00464\n","Epoch 60/500\n","12/12 [==============================] - 13s 1s/step - loss: 106.7432 - val_loss: 299.6472\n","\n","Epoch 00060: val_loss did not improve from 229.00464\n","Epoch 61/500\n","12/12 [==============================] - 13s 1s/step - loss: 106.0716 - val_loss: 296.0765\n","\n","Epoch 00061: val_loss did not improve from 229.00464\n","Epoch 62/500\n","12/12 [==============================] - 13s 1s/step - loss: 104.9198 - val_loss: 285.5698\n","\n","Epoch 00062: val_loss did not improve from 229.00464\n","Epoch 63/500\n","12/12 [==============================] - 13s 1s/step - loss: 103.2020 - val_loss: 291.3434\n","\n","Epoch 00063: val_loss did not improve from 229.00464\n","Epoch 64/500\n","12/12 [==============================] - 13s 1s/step - loss: 101.5830 - val_loss: 295.3272\n","\n","Epoch 00064: val_loss did not improve from 229.00464\n","Epoch 65/500\n","12/12 [==============================] - 13s 1s/step - loss: 100.3406 - val_loss: 291.3520\n","\n","Epoch 00065: val_loss did not improve from 229.00464\n","Epoch 66/500\n","12/12 [==============================] - 13s 1s/step - loss: 100.0437 - val_loss: 300.7422\n","\n","Epoch 00066: val_loss did not improve from 229.00464\n","Epoch 67/500\n","12/12 [==============================] - 13s 1s/step - loss: 98.9017 - val_loss: 298.0097\n","\n","Epoch 00067: val_loss did not improve from 229.00464\n","Epoch 68/500\n","12/12 [==============================] - 13s 1s/step - loss: 97.7738 - val_loss: 292.4301\n","\n","Epoch 00068: val_loss did not improve from 229.00464\n","Epoch 69/500\n","12/12 [==============================] - 13s 1s/step - loss: 96.3383 - val_loss: 277.1050\n","\n","Epoch 00069: val_loss did not improve from 229.00464\n","Epoch 70/500\n","12/12 [==============================] - 13s 1s/step - loss: 94.9004 - val_loss: 273.8967\n","\n","Epoch 00070: val_loss did not improve from 229.00464\n","Epoch 71/500\n","12/12 [==============================] - 13s 1s/step - loss: 93.2323 - val_loss: 279.5563\n","\n","Epoch 00071: val_loss did not improve from 229.00464\n","Epoch 72/500\n","12/12 [==============================] - 13s 1s/step - loss: 90.5438 - val_loss: 288.4332\n","\n","Epoch 00072: val_loss did not improve from 229.00464\n","Epoch 73/500\n","12/12 [==============================] - 13s 1s/step - loss: 88.8096 - val_loss: 290.6404\n","\n","Epoch 00073: val_loss did not improve from 229.00464\n","Epoch 74/500\n","12/12 [==============================] - 13s 1s/step - loss: 87.7904 - val_loss: 291.5700\n","\n","Epoch 00074: val_loss did not improve from 229.00464\n","Epoch 75/500\n","12/12 [==============================] - 13s 1s/step - loss: 85.5363 - val_loss: 292.7077\n","\n","Epoch 00075: val_loss did not improve from 229.00464\n","Epoch 76/500\n","12/12 [==============================] - 13s 1s/step - loss: 84.6414 - val_loss: 291.7212\n","\n","Epoch 00076: val_loss did not improve from 229.00464\n","Epoch 77/500\n","12/12 [==============================] - 13s 1s/step - loss: 83.7123 - val_loss: 294.0072\n","\n","Epoch 00077: val_loss did not improve from 229.00464\n","Epoch 78/500\n","12/12 [==============================] - 13s 1s/step - loss: 83.8038 - val_loss: 289.0975\n","\n","Epoch 00078: val_loss did not improve from 229.00464\n","Epoch 79/500\n","12/12 [==============================] - 13s 1s/step - loss: 84.3498 - val_loss: 282.9920\n","\n","Epoch 00079: val_loss did not improve from 229.00464\n","Epoch 80/500\n","12/12 [==============================] - 13s 1s/step - loss: 83.4362 - val_loss: 295.3244\n","\n","Epoch 00080: val_loss did not improve from 229.00464\n","Epoch 81/500\n","12/12 [==============================] - 13s 1s/step - loss: 81.9232 - val_loss: 289.4767\n","\n","Epoch 00081: val_loss did not improve from 229.00464\n","Epoch 82/500\n","12/12 [==============================] - 13s 1s/step - loss: 81.0072 - val_loss: 285.2364\n","\n","Epoch 00082: val_loss did not improve from 229.00464\n","Epoch 83/500\n","12/12 [==============================] - 13s 1s/step - loss: 78.4207 - val_loss: 286.8607\n","\n","Epoch 00083: val_loss did not improve from 229.00464\n","Epoch 84/500\n","12/12 [==============================] - 13s 1s/step - loss: 76.4567 - val_loss: 285.0281\n","\n","Epoch 00084: val_loss did not improve from 229.00464\n","Epoch 85/500\n","12/12 [==============================] - 13s 1s/step - loss: 75.5009 - val_loss: 281.5615\n","\n","Epoch 00085: val_loss did not improve from 229.00464\n","Epoch 86/500\n","12/12 [==============================] - 13s 1s/step - loss: 74.0861 - val_loss: 281.6280\n","\n","Epoch 00086: val_loss did not improve from 229.00464\n","Epoch 87/500\n","12/12 [==============================] - 13s 1s/step - loss: 73.6535 - val_loss: 288.0285\n","\n","Epoch 00087: val_loss did not improve from 229.00464\n","Epoch 88/500\n","12/12 [==============================] - 13s 1s/step - loss: 72.3995 - val_loss: 294.4341\n","\n","Epoch 00088: val_loss did not improve from 229.00464\n","Epoch 89/500\n","12/12 [==============================] - 13s 1s/step - loss: 71.6644 - val_loss: 295.4600\n","\n","Epoch 00089: val_loss did not improve from 229.00464\n","Epoch 90/500\n","12/12 [==============================] - 13s 1s/step - loss: 70.9849 - val_loss: 298.2595\n","\n","Epoch 00090: val_loss did not improve from 229.00464\n","Epoch 91/500\n","12/12 [==============================] - 13s 1s/step - loss: 70.5144 - val_loss: 292.2924\n","\n","Epoch 00091: val_loss did not improve from 229.00464\n","Epoch 92/500\n","12/12 [==============================] - 13s 1s/step - loss: 69.9651 - val_loss: 275.9505\n","\n","Epoch 00092: val_loss did not improve from 229.00464\n","Epoch 93/500\n","12/12 [==============================] - 13s 1s/step - loss: 70.2923 - val_loss: 269.9157\n","\n","Epoch 00093: val_loss did not improve from 229.00464\n","Epoch 94/500\n","12/12 [==============================] - 13s 1s/step - loss: 70.5007 - val_loss: 287.1810\n","\n","Epoch 00094: val_loss did not improve from 229.00464\n","Epoch 95/500\n","12/12 [==============================] - 13s 1s/step - loss: 69.9883 - val_loss: 298.3213\n","\n","Epoch 00095: val_loss did not improve from 229.00464\n","Epoch 96/500\n","12/12 [==============================] - 13s 1s/step - loss: 69.5390 - val_loss: 303.0890\n","\n","Epoch 00096: val_loss did not improve from 229.00464\n","Epoch 97/500\n","12/12 [==============================] - 13s 1s/step - loss: 69.5790 - val_loss: 291.4153\n","\n","Epoch 00097: val_loss did not improve from 229.00464\n","Epoch 98/500\n","12/12 [==============================] - 13s 1s/step - loss: 68.0058 - val_loss: 280.1562\n","\n","Epoch 00098: val_loss did not improve from 229.00464\n","Epoch 99/500\n","12/12 [==============================] - 13s 1s/step - loss: 67.2655 - val_loss: 285.9785\n","\n","Epoch 00099: val_loss did not improve from 229.00464\n","Epoch 100/500\n","12/12 [==============================] - 13s 1s/step - loss: 65.2660 - val_loss: 290.9592\n","\n","Epoch 00100: val_loss did not improve from 229.00464\n","Epoch 101/500\n","12/12 [==============================] - 13s 1s/step - loss: 65.9122 - val_loss: 292.0582\n","\n","Epoch 00101: val_loss did not improve from 229.00464\n","Epoch 102/500\n","12/12 [==============================] - 13s 1s/step - loss: 64.6053 - val_loss: 292.9825\n","\n","Epoch 00102: val_loss did not improve from 229.00464\n","Epoch 103/500\n","12/12 [==============================] - 13s 1s/step - loss: 63.7570 - val_loss: 308.1580\n","\n","Epoch 00103: val_loss did not improve from 229.00464\n","Epoch 104/500\n","12/12 [==============================] - 13s 1s/step - loss: 62.9666 - val_loss: 304.9489\n","\n","Epoch 00104: val_loss did not improve from 229.00464\n","Epoch 105/500\n","12/12 [==============================] - 13s 1s/step - loss: 61.5369 - val_loss: 298.4578\n","\n","Epoch 00105: val_loss did not improve from 229.00464\n","Epoch 106/500\n","12/12 [==============================] - 13s 1s/step - loss: 60.7860 - val_loss: 289.9637\n","\n","Epoch 00106: val_loss did not improve from 229.00464\n","Epoch 107/500\n","12/12 [==============================] - 13s 1s/step - loss: 60.4820 - val_loss: 282.9481\n","\n","Epoch 00107: val_loss did not improve from 229.00464\n","Epoch 108/500\n","12/12 [==============================] - 13s 1s/step - loss: 60.0637 - val_loss: 291.7533\n","\n","Epoch 00108: val_loss did not improve from 229.00464\n","Epoch 109/500\n","12/12 [==============================] - 13s 1s/step - loss: 59.4561 - val_loss: 316.5009\n","\n","Epoch 00109: val_loss did not improve from 229.00464\n","Epoch 110/500\n","12/12 [==============================] - 13s 1s/step - loss: 59.4005 - val_loss: 317.2720\n","\n","Epoch 00110: val_loss did not improve from 229.00464\n","Epoch 111/500\n","12/12 [==============================] - 13s 1s/step - loss: 59.3386 - val_loss: 300.1931\n","\n","Epoch 00111: val_loss did not improve from 229.00464\n","Epoch 112/500\n","12/12 [==============================] - 13s 1s/step - loss: 58.5667 - val_loss: 287.6804\n","\n","Epoch 00112: val_loss did not improve from 229.00464\n","Epoch 113/500\n","12/12 [==============================] - 13s 1s/step - loss: 57.4281 - val_loss: 296.6292\n","\n","Epoch 00113: val_loss did not improve from 229.00464\n","Epoch 114/500\n","12/12 [==============================] - 13s 1s/step - loss: 57.3753 - val_loss: 295.1586\n","\n","Epoch 00114: val_loss did not improve from 229.00464\n","Epoch 115/500\n","12/12 [==============================] - 13s 1s/step - loss: 56.0756 - val_loss: 304.6804\n","\n","Epoch 00115: val_loss did not improve from 229.00464\n","Epoch 116/500\n","12/12 [==============================] - 13s 1s/step - loss: 55.7460 - val_loss: 307.9320\n","\n","Epoch 00116: val_loss did not improve from 229.00464\n","Epoch 117/500\n","12/12 [==============================] - 13s 1s/step - loss: 55.0799 - val_loss: 302.7001\n","\n","Epoch 00117: val_loss did not improve from 229.00464\n","Epoch 118/500\n","12/12 [==============================] - 13s 1s/step - loss: 54.2582 - val_loss: 301.4395\n","\n","Epoch 00118: val_loss did not improve from 229.00464\n","Epoch 119/500\n","12/12 [==============================] - 13s 1s/step - loss: 54.2737 - val_loss: 297.6686\n","\n","Epoch 00119: val_loss did not improve from 229.00464\n","Epoch 120/500\n","12/12 [==============================] - 13s 1s/step - loss: 53.3167 - val_loss: 304.1225\n","\n","Epoch 00120: val_loss did not improve from 229.00464\n","Epoch 121/500\n","12/12 [==============================] - 13s 1s/step - loss: 53.1112 - val_loss: 312.3074\n","\n","Epoch 00121: val_loss did not improve from 229.00464\n","Epoch 122/500\n","12/12 [==============================] - 13s 1s/step - loss: 52.6330 - val_loss: 313.0189\n","\n","Epoch 00122: val_loss did not improve from 229.00464\n","Epoch 123/500\n","12/12 [==============================] - 13s 1s/step - loss: 52.4746 - val_loss: 298.1000\n","\n","Epoch 00123: val_loss did not improve from 229.00464\n","Epoch 124/500\n","12/12 [==============================] - 13s 1s/step - loss: 51.4823 - val_loss: 300.3302\n","\n","Epoch 00124: val_loss did not improve from 229.00464\n","Epoch 125/500\n","12/12 [==============================] - 13s 1s/step - loss: 51.5312 - val_loss: 306.1932\n","\n","Epoch 00125: val_loss did not improve from 229.00464\n","Epoch 126/500\n","12/12 [==============================] - 13s 1s/step - loss: 50.7267 - val_loss: 310.9305\n","\n","Epoch 00126: val_loss did not improve from 229.00464\n","Epoch 127/500\n","12/12 [==============================] - 13s 1s/step - loss: 50.0571 - val_loss: 320.1584\n","\n","Epoch 00127: val_loss did not improve from 229.00464\n","Epoch 128/500\n","12/12 [==============================] - 13s 1s/step - loss: 49.9246 - val_loss: 309.1970\n","\n","Epoch 00128: val_loss did not improve from 229.00464\n","Epoch 129/500\n","12/12 [==============================] - 13s 1s/step - loss: 49.1788 - val_loss: 300.6670\n","\n","Epoch 00129: val_loss did not improve from 229.00464\n","Epoch 130/500\n","12/12 [==============================] - 13s 1s/step - loss: 49.0206 - val_loss: 297.2180\n","\n","Epoch 00130: val_loss did not improve from 229.00464\n","Epoch 131/500\n","12/12 [==============================] - 13s 1s/step - loss: 48.1545 - val_loss: 303.8954\n","\n","Epoch 00131: val_loss did not improve from 229.00464\n","Epoch 132/500\n","12/12 [==============================] - 13s 1s/step - loss: 47.6745 - val_loss: 314.7021\n","\n","Epoch 00132: val_loss did not improve from 229.00464\n","Epoch 133/500\n","12/12 [==============================] - 13s 1s/step - loss: 47.4027 - val_loss: 313.4890\n","\n","Epoch 00133: val_loss did not improve from 229.00464\n","Epoch 134/500\n","12/12 [==============================] - 13s 1s/step - loss: 47.1009 - val_loss: 306.0795\n","\n","Epoch 00134: val_loss did not improve from 229.00464\n","Epoch 135/500\n","12/12 [==============================] - 13s 1s/step - loss: 46.3257 - val_loss: 296.7983\n","\n","Epoch 00135: val_loss did not improve from 229.00464\n","Epoch 136/500\n","12/12 [==============================] - 13s 1s/step - loss: 46.5119 - val_loss: 299.1265\n","\n","Epoch 00136: val_loss did not improve from 229.00464\n","Epoch 137/500\n","12/12 [==============================] - 13s 1s/step - loss: 45.6501 - val_loss: 313.8900\n","\n","Epoch 00137: val_loss did not improve from 229.00464\n","Epoch 138/500\n","12/12 [==============================] - 13s 1s/step - loss: 45.2906 - val_loss: 320.9500\n","\n","Epoch 00138: val_loss did not improve from 229.00464\n","Epoch 139/500\n","12/12 [==============================] - 13s 1s/step - loss: 45.0419 - val_loss: 309.9932\n","\n","Epoch 00139: val_loss did not improve from 229.00464\n","Epoch 140/500\n","12/12 [==============================] - 13s 1s/step - loss: 44.1084 - val_loss: 296.7553\n","\n","Epoch 00140: val_loss did not improve from 229.00464\n","Epoch 141/500\n","12/12 [==============================] - 13s 1s/step - loss: 44.1053 - val_loss: 295.0558\n","\n","Epoch 00141: val_loss did not improve from 229.00464\n","Epoch 142/500\n","12/12 [==============================] - 13s 1s/step - loss: 43.6645 - val_loss: 310.0542\n","\n","Epoch 00142: val_loss did not improve from 229.00464\n","Epoch 143/500\n","12/12 [==============================] - 13s 1s/step - loss: 43.1799 - val_loss: 313.5578\n","\n","Epoch 00143: val_loss did not improve from 229.00464\n","Epoch 144/500\n","12/12 [==============================] - 13s 1s/step - loss: 42.4660 - val_loss: 309.5635\n","\n","Epoch 00144: val_loss did not improve from 229.00464\n","Epoch 145/500\n","12/12 [==============================] - 13s 1s/step - loss: 41.9640 - val_loss: 302.1017\n","\n","Epoch 00145: val_loss did not improve from 229.00464\n","Epoch 146/500\n","12/12 [==============================] - 13s 1s/step - loss: 40.9122 - val_loss: 308.5593\n","\n","Epoch 00146: val_loss did not improve from 229.00464\n","Epoch 147/500\n","12/12 [==============================] - 13s 1s/step - loss: 40.2703 - val_loss: 325.7397\n","\n","Epoch 00147: val_loss did not improve from 229.00464\n","Epoch 148/500\n","12/12 [==============================] - 13s 1s/step - loss: 39.4409 - val_loss: 317.3312\n","\n","Epoch 00148: val_loss did not improve from 229.00464\n","Epoch 149/500\n","12/12 [==============================] - 13s 1s/step - loss: 38.8141 - val_loss: 301.3121\n","\n","Epoch 00149: val_loss did not improve from 229.00464\n","Epoch 150/500\n","12/12 [==============================] - 13s 1s/step - loss: 38.1478 - val_loss: 299.2289\n","\n","Epoch 00150: val_loss did not improve from 229.00464\n","Epoch 151/500\n","12/12 [==============================] - 13s 1s/step - loss: 37.4582 - val_loss: 312.0836\n","\n","Epoch 00151: val_loss did not improve from 229.00464\n","Epoch 152/500\n","12/12 [==============================] - 13s 1s/step - loss: 36.3161 - val_loss: 326.0863\n","\n","Epoch 00152: val_loss did not improve from 229.00464\n","Epoch 153/500\n","12/12 [==============================] - 13s 1s/step - loss: 35.7334 - val_loss: 318.3786\n","\n","Epoch 00153: val_loss did not improve from 229.00464\n","Epoch 154/500\n","12/12 [==============================] - 13s 1s/step - loss: 34.7035 - val_loss: 307.0441\n","\n","Epoch 00154: val_loss did not improve from 229.00464\n","Epoch 155/500\n","12/12 [==============================] - 13s 1s/step - loss: 33.7761 - val_loss: 317.6361\n","\n","Epoch 00155: val_loss did not improve from 229.00464\n","Epoch 156/500\n","12/12 [==============================] - 13s 1s/step - loss: 32.6313 - val_loss: 329.8673\n","\n","Epoch 00156: val_loss did not improve from 229.00464\n","Epoch 157/500\n","12/12 [==============================] - 13s 1s/step - loss: 31.6903 - val_loss: 310.7409\n","\n","Epoch 00157: val_loss did not improve from 229.00464\n","Epoch 158/500\n","12/12 [==============================] - 13s 1s/step - loss: 30.5102 - val_loss: 315.6836\n","\n","Epoch 00158: val_loss did not improve from 229.00464\n","Epoch 159/500\n","12/12 [==============================] - 13s 1s/step - loss: 29.5505 - val_loss: 337.5780\n","\n","Epoch 00159: val_loss did not improve from 229.00464\n","Epoch 160/500\n","12/12 [==============================] - 13s 1s/step - loss: 28.3286 - val_loss: 323.0754\n","\n","Epoch 00160: val_loss did not improve from 229.00464\n","Epoch 161/500\n","12/12 [==============================] - 13s 1s/step - loss: 27.3175 - val_loss: 308.2580\n","\n","Epoch 00161: val_loss did not improve from 229.00464\n","Epoch 162/500\n","12/12 [==============================] - 13s 1s/step - loss: 26.3895 - val_loss: 310.1637\n","\n","Epoch 00162: val_loss did not improve from 229.00464\n","Epoch 163/500\n","12/12 [==============================] - 13s 1s/step - loss: 25.0965 - val_loss: 327.5758\n","\n","Epoch 00163: val_loss did not improve from 229.00464\n","Epoch 164/500\n","12/12 [==============================] - 13s 1s/step - loss: 24.3974 - val_loss: 312.4882\n","\n","Epoch 00164: val_loss did not improve from 229.00464\n","Epoch 165/500\n","12/12 [==============================] - 13s 1s/step - loss: 23.6393 - val_loss: 302.9964\n","\n","Epoch 00165: val_loss did not improve from 229.00464\n","Epoch 166/500\n","12/12 [==============================] - 13s 1s/step - loss: 22.8911 - val_loss: 305.0504\n","\n","Epoch 00166: val_loss did not improve from 229.00464\n","Epoch 167/500\n","12/12 [==============================] - 13s 1s/step - loss: 21.8742 - val_loss: 318.4291\n","\n","Epoch 00167: val_loss did not improve from 229.00464\n","Epoch 168/500\n","12/12 [==============================] - 13s 1s/step - loss: 21.0034 - val_loss: 318.2795\n","\n","Epoch 00168: val_loss did not improve from 229.00464\n","Epoch 169/500\n","12/12 [==============================] - 13s 1s/step - loss: 19.8050 - val_loss: 309.2757\n","\n","Epoch 00169: val_loss did not improve from 229.00464\n","Epoch 170/500\n","12/12 [==============================] - 13s 1s/step - loss: 18.9977 - val_loss: 299.6585\n","\n","Epoch 00170: val_loss did not improve from 229.00464\n","Epoch 171/500\n","12/12 [==============================] - 13s 1s/step - loss: 17.8178 - val_loss: 301.9282\n","\n","Epoch 00171: val_loss did not improve from 229.00464\n","Epoch 172/500\n","12/12 [==============================] - 13s 1s/step - loss: 16.8955 - val_loss: 308.7791\n","\n","Epoch 00172: val_loss did not improve from 229.00464\n","Epoch 173/500\n","12/12 [==============================] - 13s 1s/step - loss: 16.2640 - val_loss: 312.9311\n","\n","Epoch 00173: val_loss did not improve from 229.00464\n","Epoch 174/500\n","12/12 [==============================] - 13s 1s/step - loss: 15.5002 - val_loss: 312.9556\n","\n","Epoch 00174: val_loss did not improve from 229.00464\n","Epoch 175/500\n","12/12 [==============================] - 13s 1s/step - loss: 14.6872 - val_loss: 312.5048\n","\n","Epoch 00175: val_loss did not improve from 229.00464\n","Epoch 176/500\n","12/12 [==============================] - 13s 1s/step - loss: 14.1726 - val_loss: 314.1845\n","\n","Epoch 00176: val_loss did not improve from 229.00464\n","Epoch 177/500\n","12/12 [==============================] - 13s 1s/step - loss: 13.4824 - val_loss: 317.2686\n","\n","Epoch 00177: val_loss did not improve from 229.00464\n","Epoch 178/500\n","12/12 [==============================] - 13s 1s/step - loss: 12.9978 - val_loss: 317.6857\n","\n","Epoch 00178: val_loss did not improve from 229.00464\n","Epoch 179/500\n","12/12 [==============================] - 13s 1s/step - loss: 12.6988 - val_loss: 301.9579\n","\n","Epoch 00179: val_loss did not improve from 229.00464\n","Epoch 180/500\n","12/12 [==============================] - 13s 1s/step - loss: 12.2554 - val_loss: 309.8182\n","\n","Epoch 00180: val_loss did not improve from 229.00464\n","Epoch 181/500\n","12/12 [==============================] - 13s 1s/step - loss: 11.8992 - val_loss: 304.5934\n","\n","Epoch 00181: val_loss did not improve from 229.00464\n","Epoch 182/500\n","12/12 [==============================] - 13s 1s/step - loss: 11.3260 - val_loss: 313.9569\n","\n","Epoch 00182: val_loss did not improve from 229.00464\n","Epoch 183/500\n","12/12 [==============================] - 13s 1s/step - loss: 11.0614 - val_loss: 309.0141\n","\n","Epoch 00183: val_loss did not improve from 229.00464\n","Epoch 184/500\n","12/12 [==============================] - 13s 1s/step - loss: 10.7526 - val_loss: 309.2833\n","\n","Epoch 00184: val_loss did not improve from 229.00464\n","Epoch 185/500\n","12/12 [==============================] - 13s 1s/step - loss: 10.5086 - val_loss: 309.0501\n","\n","Epoch 00185: val_loss did not improve from 229.00464\n","Epoch 186/500\n","12/12 [==============================] - 13s 1s/step - loss: 10.0384 - val_loss: 316.2573\n","\n","Epoch 00186: val_loss did not improve from 229.00464\n","Epoch 187/500\n","12/12 [==============================] - 13s 1s/step - loss: 9.6344 - val_loss: 310.1259\n","\n","Epoch 00187: val_loss did not improve from 229.00464\n","Epoch 188/500\n","12/12 [==============================] - 13s 1s/step - loss: 9.4106 - val_loss: 309.7780\n","\n","Epoch 00188: val_loss did not improve from 229.00464\n","Epoch 189/500\n","12/12 [==============================] - 13s 1s/step - loss: 9.0695 - val_loss: 313.6974\n","\n","Epoch 00189: val_loss did not improve from 229.00464\n","Epoch 190/500\n","12/12 [==============================] - 13s 1s/step - loss: 8.8226 - val_loss: 304.8994\n","\n","Epoch 00190: val_loss did not improve from 229.00464\n","Epoch 191/500\n","12/12 [==============================] - 13s 1s/step - loss: 8.6157 - val_loss: 310.7455\n","\n","Epoch 00191: val_loss did not improve from 229.00464\n","Epoch 192/500\n","12/12 [==============================] - 13s 1s/step - loss: 8.4293 - val_loss: 318.7971\n","\n","Epoch 00192: val_loss did not improve from 229.00464\n","Epoch 193/500\n","12/12 [==============================] - 13s 1s/step - loss: 8.2751 - val_loss: 310.7433\n","\n","Epoch 00193: val_loss did not improve from 229.00464\n","Epoch 194/500\n","12/12 [==============================] - 13s 1s/step - loss: 7.9541 - val_loss: 318.9822\n","\n","Epoch 00194: val_loss did not improve from 229.00464\n","Epoch 195/500\n","12/12 [==============================] - 13s 1s/step - loss: 7.8728 - val_loss: 320.3682\n","\n","Epoch 00195: val_loss did not improve from 229.00464\n","Epoch 196/500\n","12/12 [==============================] - 13s 1s/step - loss: 7.4965 - val_loss: 320.7817\n","\n","Epoch 00196: val_loss did not improve from 229.00464\n","Epoch 197/500\n","12/12 [==============================] - 13s 1s/step - loss: 7.3548 - val_loss: 320.4216\n","\n","Epoch 00197: val_loss did not improve from 229.00464\n","Epoch 198/500\n","12/12 [==============================] - 13s 1s/step - loss: 7.2378 - val_loss: 315.1105\n","\n","Epoch 00198: val_loss did not improve from 229.00464\n","Epoch 199/500\n","12/12 [==============================] - 13s 1s/step - loss: 7.0628 - val_loss: 307.5857\n","\n","Epoch 00199: val_loss did not improve from 229.00464\n","Epoch 200/500\n","12/12 [==============================] - 13s 1s/step - loss: 6.9129 - val_loss: 316.7397\n","\n","Epoch 00200: val_loss did not improve from 229.00464\n","Epoch 201/500\n","12/12 [==============================] - 13s 1s/step - loss: 6.8262 - val_loss: 314.3818\n","\n","Epoch 00201: val_loss did not improve from 229.00464\n","Epoch 202/500\n","12/12 [==============================] - 13s 1s/step - loss: 6.6784 - val_loss: 309.6893\n","\n","Epoch 00202: val_loss did not improve from 229.00464\n","Epoch 203/500\n","12/12 [==============================] - 13s 1s/step - loss: 6.5164 - val_loss: 311.4884\n","\n","Epoch 00203: val_loss did not improve from 229.00464\n","Epoch 204/500\n","12/12 [==============================] - 13s 1s/step - loss: 6.3854 - val_loss: 311.9577\n","\n","Epoch 00204: val_loss did not improve from 229.00464\n","Epoch 205/500\n","12/12 [==============================] - 13s 1s/step - loss: 6.3292 - val_loss: 310.0211\n","\n","Epoch 00205: val_loss did not improve from 229.00464\n","Epoch 206/500\n","12/12 [==============================] - 13s 1s/step - loss: 6.0669 - val_loss: 307.0085\n","\n","Epoch 00206: val_loss did not improve from 229.00464\n","Epoch 207/500\n","12/12 [==============================] - 13s 1s/step - loss: 6.0066 - val_loss: 303.6983\n","\n","Epoch 00207: val_loss did not improve from 229.00464\n","Epoch 208/500\n","12/12 [==============================] - 13s 1s/step - loss: 5.9019 - val_loss: 305.9591\n","\n","Epoch 00208: val_loss did not improve from 229.00464\n","Epoch 209/500\n","12/12 [==============================] - 13s 1s/step - loss: 5.7821 - val_loss: 301.4188\n","\n","Epoch 00209: val_loss did not improve from 229.00464\n","Epoch 210/500\n","12/12 [==============================] - 13s 1s/step - loss: 5.6813 - val_loss: 302.8071\n","\n","Epoch 00210: val_loss did not improve from 229.00464\n","Epoch 211/500\n","12/12 [==============================] - 13s 1s/step - loss: 5.6094 - val_loss: 299.2294\n","\n","Epoch 00211: val_loss did not improve from 229.00464\n","Epoch 212/500\n","12/12 [==============================] - 13s 1s/step - loss: 5.5768 - val_loss: 302.9068\n","\n","Epoch 00212: val_loss did not improve from 229.00464\n","Epoch 213/500\n","12/12 [==============================] - 13s 1s/step - loss: 5.4588 - val_loss: 299.6726\n","\n","Epoch 00213: val_loss did not improve from 229.00464\n","Epoch 214/500\n","12/12 [==============================] - 13s 1s/step - loss: 5.3683 - val_loss: 298.5389\n","\n","Epoch 00214: val_loss did not improve from 229.00464\n","Epoch 215/500\n","12/12 [==============================] - 13s 1s/step - loss: 5.1994 - val_loss: 302.2889\n","\n","Epoch 00215: val_loss did not improve from 229.00464\n","Epoch 216/500\n","12/12 [==============================] - 13s 1s/step - loss: 5.1143 - val_loss: 304.6844\n","\n","Epoch 00216: val_loss did not improve from 229.00464\n","Epoch 217/500\n","12/12 [==============================] - 13s 1s/step - loss: 5.0727 - val_loss: 305.2131\n","\n","Epoch 00217: val_loss did not improve from 229.00464\n","Epoch 218/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.9687 - val_loss: 298.7834\n","\n","Epoch 00218: val_loss did not improve from 229.00464\n","Epoch 219/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.9245 - val_loss: 297.2375\n","\n","Epoch 00219: val_loss did not improve from 229.00464\n","Epoch 220/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.8856 - val_loss: 301.4065\n","\n","Epoch 00220: val_loss did not improve from 229.00464\n","Epoch 221/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.8073 - val_loss: 307.9113\n","\n","Epoch 00221: val_loss did not improve from 229.00464\n","Epoch 222/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.7311 - val_loss: 303.5702\n","\n","Epoch 00222: val_loss did not improve from 229.00464\n","Epoch 223/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.7045 - val_loss: 296.7640\n","\n","Epoch 00223: val_loss did not improve from 229.00464\n","Epoch 224/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.5937 - val_loss: 295.2342\n","\n","Epoch 00224: val_loss did not improve from 229.00464\n","Epoch 225/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.5555 - val_loss: 296.6454\n","\n","Epoch 00225: val_loss did not improve from 229.00464\n","Epoch 226/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.4582 - val_loss: 294.3763\n","\n","Epoch 00226: val_loss did not improve from 229.00464\n","Epoch 227/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.4290 - val_loss: 296.9398\n","\n","Epoch 00227: val_loss did not improve from 229.00464\n","Epoch 228/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.4393 - val_loss: 295.4854\n","\n","Epoch 00228: val_loss did not improve from 229.00464\n","Epoch 229/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.3832 - val_loss: 294.0312\n","\n","Epoch 00229: val_loss did not improve from 229.00464\n","Epoch 230/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.3096 - val_loss: 294.9883\n","\n","Epoch 00230: val_loss did not improve from 229.00464\n","Epoch 231/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.2634 - val_loss: 294.1814\n","\n","Epoch 00231: val_loss did not improve from 229.00464\n","Epoch 232/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.2644 - val_loss: 289.9011\n","\n","Epoch 00232: val_loss did not improve from 229.00464\n","Epoch 233/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.1923 - val_loss: 289.3940\n","\n","Epoch 00233: val_loss did not improve from 229.00464\n","Epoch 234/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.1431 - val_loss: 290.3367\n","\n","Epoch 00234: val_loss did not improve from 229.00464\n","Epoch 235/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.0953 - val_loss: 291.8470\n","\n","Epoch 00235: val_loss did not improve from 229.00464\n","Epoch 236/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.1029 - val_loss: 291.2889\n","\n","Epoch 00236: val_loss did not improve from 229.00464\n","Epoch 237/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.0099 - val_loss: 285.4267\n","\n","Epoch 00237: val_loss did not improve from 229.00464\n","Epoch 238/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.0314 - val_loss: 287.1815\n","\n","Epoch 00238: val_loss did not improve from 229.00464\n","Epoch 239/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.0290 - val_loss: 287.2430\n","\n","Epoch 00239: val_loss did not improve from 229.00464\n","Epoch 240/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.0437 - val_loss: 284.4619\n","\n","Epoch 00240: val_loss did not improve from 229.00464\n","Epoch 241/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.9485 - val_loss: 284.7234\n","\n","Epoch 00241: val_loss did not improve from 229.00464\n","Epoch 242/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.9183 - val_loss: 289.3633\n","\n","Epoch 00242: val_loss did not improve from 229.00464\n","Epoch 243/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.9090 - val_loss: 287.5646\n","\n","Epoch 00243: val_loss did not improve from 229.00464\n","Epoch 244/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.9088 - val_loss: 281.1631\n","\n","Epoch 00244: val_loss did not improve from 229.00464\n","Epoch 245/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.9519 - val_loss: 278.6365\n","\n","Epoch 00245: val_loss did not improve from 229.00464\n","Epoch 246/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.0245 - val_loss: 282.5852\n","\n","Epoch 00246: val_loss did not improve from 229.00464\n","Epoch 247/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.9719 - val_loss: 280.5154\n","\n","Epoch 00247: val_loss did not improve from 229.00464\n","Epoch 248/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.9478 - val_loss: 282.5977\n","\n","Epoch 00248: val_loss did not improve from 229.00464\n","Epoch 249/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.9518 - val_loss: 278.6013\n","\n","Epoch 00249: val_loss did not improve from 229.00464\n","Epoch 250/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.8412 - val_loss: 281.8818\n","\n","Epoch 00250: val_loss did not improve from 229.00464\n","Epoch 251/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.7496 - val_loss: 277.2645\n","\n","Epoch 00251: val_loss did not improve from 229.00464\n","Epoch 252/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.6397 - val_loss: 279.4860\n","\n","Epoch 00252: val_loss did not improve from 229.00464\n","Epoch 253/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.5792 - val_loss: 279.3673\n","\n","Epoch 00253: val_loss did not improve from 229.00464\n","Epoch 254/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.5301 - val_loss: 282.7454\n","\n","Epoch 00254: val_loss did not improve from 229.00464\n","Epoch 255/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.5452 - val_loss: 282.9030\n","\n","Epoch 00255: val_loss did not improve from 229.00464\n","Epoch 256/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.4472 - val_loss: 278.6682\n","\n","Epoch 00256: val_loss did not improve from 229.00464\n","Epoch 257/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.4476 - val_loss: 277.9465\n","\n","Epoch 00257: val_loss did not improve from 229.00464\n","Epoch 258/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.3850 - val_loss: 279.1749\n","\n","Epoch 00258: val_loss did not improve from 229.00464\n","Epoch 259/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.3316 - val_loss: 276.7192\n","\n","Epoch 00259: val_loss did not improve from 229.00464\n","Epoch 260/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.3526 - val_loss: 278.3716\n","\n","Epoch 00260: val_loss did not improve from 229.00464\n","Epoch 261/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.3096 - val_loss: 276.6891\n","\n","Epoch 00261: val_loss did not improve from 229.00464\n","Epoch 262/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.2942 - val_loss: 273.7422\n","\n","Epoch 00262: val_loss did not improve from 229.00464\n","Epoch 263/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.2629 - val_loss: 274.3657\n","\n","Epoch 00263: val_loss did not improve from 229.00464\n","Epoch 264/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.2267 - val_loss: 277.7852\n","\n","Epoch 00264: val_loss did not improve from 229.00464\n","Epoch 265/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.1791 - val_loss: 276.4784\n","\n","Epoch 00265: val_loss did not improve from 229.00464\n","Epoch 266/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.1670 - val_loss: 277.5014\n","\n","Epoch 00266: val_loss did not improve from 229.00464\n","Epoch 267/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.1480 - val_loss: 275.1255\n","\n","Epoch 00267: val_loss did not improve from 229.00464\n","Epoch 268/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.1111 - val_loss: 275.1592\n","\n","Epoch 00268: val_loss did not improve from 229.00464\n","Epoch 269/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.1244 - val_loss: 276.1406\n","\n","Epoch 00269: val_loss did not improve from 229.00464\n","Epoch 270/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.1189 - val_loss: 274.4214\n","\n","Epoch 00270: val_loss did not improve from 229.00464\n","Epoch 271/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.1168 - val_loss: 275.4033\n","\n","Epoch 00271: val_loss did not improve from 229.00464\n","Epoch 272/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.1224 - val_loss: 272.7407\n","\n","Epoch 00272: val_loss did not improve from 229.00464\n","Epoch 273/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.1405 - val_loss: 274.5754\n","\n","Epoch 00273: val_loss did not improve from 229.00464\n","Epoch 274/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.1529 - val_loss: 275.8242\n","\n","Epoch 00274: val_loss did not improve from 229.00464\n","Epoch 275/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.1208 - val_loss: 271.2518\n","\n","Epoch 00275: val_loss did not improve from 229.00464\n","Epoch 276/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.1342 - val_loss: 270.3183\n","\n","Epoch 00276: val_loss did not improve from 229.00464\n","Epoch 277/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.1845 - val_loss: 274.1662\n","\n","Epoch 00277: val_loss did not improve from 229.00464\n","Epoch 278/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.1080 - val_loss: 274.8297\n","\n","Epoch 00278: val_loss did not improve from 229.00464\n","Epoch 279/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.2650 - val_loss: 274.5251\n","\n","Epoch 00279: val_loss did not improve from 229.00464\n","Epoch 280/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.2244 - val_loss: 277.8602\n","\n","Epoch 00280: val_loss did not improve from 229.00464\n","Epoch 281/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.2004 - val_loss: 278.0468\n","\n","Epoch 00281: val_loss did not improve from 229.00464\n","Epoch 282/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.1178 - val_loss: 277.3528\n","\n","Epoch 00282: val_loss did not improve from 229.00464\n","Epoch 283/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.0036 - val_loss: 275.0557\n","\n","Epoch 00283: val_loss did not improve from 229.00464\n","Epoch 284/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.9774 - val_loss: 278.5048\n","\n","Epoch 00284: val_loss did not improve from 229.00464\n","Epoch 285/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.9278 - val_loss: 275.8830\n","\n","Epoch 00285: val_loss did not improve from 229.00464\n","Epoch 286/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.9222 - val_loss: 276.1348\n","\n","Epoch 00286: val_loss did not improve from 229.00464\n","Epoch 287/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.9453 - val_loss: 275.2986\n","\n","Epoch 00287: val_loss did not improve from 229.00464\n","Epoch 288/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.9130 - val_loss: 276.4922\n","\n","Epoch 00288: val_loss did not improve from 229.00464\n","Epoch 289/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.8827 - val_loss: 274.4664\n","\n","Epoch 00289: val_loss did not improve from 229.00464\n","Epoch 290/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.8499 - val_loss: 275.0693\n","\n","Epoch 00290: val_loss did not improve from 229.00464\n","Epoch 291/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.8046 - val_loss: 279.9224\n","\n","Epoch 00291: val_loss did not improve from 229.00464\n","Epoch 292/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.7947 - val_loss: 277.7218\n","\n","Epoch 00292: val_loss did not improve from 229.00464\n","Epoch 293/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.7644 - val_loss: 278.2727\n","\n","Epoch 00293: val_loss did not improve from 229.00464\n","Epoch 294/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.7413 - val_loss: 277.1257\n","\n","Epoch 00294: val_loss did not improve from 229.00464\n","Epoch 295/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.7170 - val_loss: 278.3864\n","\n","Epoch 00295: val_loss did not improve from 229.00464\n","Epoch 296/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.7095 - val_loss: 278.1977\n","\n","Epoch 00296: val_loss did not improve from 229.00464\n","Epoch 297/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6941 - val_loss: 275.5586\n","\n","Epoch 00297: val_loss did not improve from 229.00464\n","Epoch 298/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6954 - val_loss: 272.1000\n","\n","Epoch 00298: val_loss did not improve from 229.00464\n","Epoch 299/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6873 - val_loss: 275.0683\n","\n","Epoch 00299: val_loss did not improve from 229.00464\n","Epoch 300/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6577 - val_loss: 274.6898\n","\n","Epoch 00300: val_loss did not improve from 229.00464\n","Epoch 301/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6487 - val_loss: 277.6533\n","\n","Epoch 00301: val_loss did not improve from 229.00464\n","Epoch 302/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6391 - val_loss: 277.1489\n","\n","Epoch 00302: val_loss did not improve from 229.00464\n","Epoch 303/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6821 - val_loss: 274.6835\n","\n","Epoch 00303: val_loss did not improve from 229.00464\n","Epoch 304/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.7633 - val_loss: 268.1184\n","\n","Epoch 00304: val_loss did not improve from 229.00464\n","Epoch 305/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.7591 - val_loss: 273.3423\n","\n","Epoch 00305: val_loss did not improve from 229.00464\n","Epoch 306/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6775 - val_loss: 274.1841\n","\n","Epoch 00306: val_loss did not improve from 229.00464\n","Epoch 307/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6604 - val_loss: 271.2361\n","\n","Epoch 00307: val_loss did not improve from 229.00464\n","Epoch 308/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6083 - val_loss: 272.4400\n","\n","Epoch 00308: val_loss did not improve from 229.00464\n","Epoch 309/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6049 - val_loss: 272.9695\n","\n","Epoch 00309: val_loss did not improve from 229.00464\n","Epoch 310/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.5726 - val_loss: 272.3412\n","\n","Epoch 00310: val_loss did not improve from 229.00464\n","Epoch 311/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.5526 - val_loss: 273.8236\n","\n","Epoch 00311: val_loss did not improve from 229.00464\n","Epoch 312/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.5582 - val_loss: 274.9294\n","\n","Epoch 00312: val_loss did not improve from 229.00464\n","Epoch 313/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.5501 - val_loss: 273.7415\n","\n","Epoch 00313: val_loss did not improve from 229.00464\n","Epoch 314/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.5288 - val_loss: 274.9059\n","\n","Epoch 00314: val_loss did not improve from 229.00464\n","Epoch 315/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.5329 - val_loss: 273.4523\n","\n","Epoch 00315: val_loss did not improve from 229.00464\n","Epoch 316/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.5210 - val_loss: 271.3884\n","\n","Epoch 00316: val_loss did not improve from 229.00464\n","Epoch 317/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.5463 - val_loss: 272.2865\n","\n","Epoch 00317: val_loss did not improve from 229.00464\n","Epoch 318/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.5574 - val_loss: 274.6203\n","\n","Epoch 00318: val_loss did not improve from 229.00464\n","Epoch 319/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.5908 - val_loss: 269.2098\n","\n","Epoch 00319: val_loss did not improve from 229.00464\n","Epoch 320/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.7796 - val_loss: 274.7433\n","\n","Epoch 00320: val_loss did not improve from 229.00464\n","Epoch 321/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.8492 - val_loss: 276.1476\n","\n","Epoch 00321: val_loss did not improve from 229.00464\n","Epoch 322/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.0561 - val_loss: 290.9121\n","\n","Epoch 00322: val_loss did not improve from 229.00464\n","Epoch 323/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.8680 - val_loss: 278.7843\n","\n","Epoch 00323: val_loss did not improve from 229.00464\n","Epoch 324/500\n","12/12 [==============================] - 13s 1s/step - loss: 6.3740 - val_loss: 291.5192\n","\n","Epoch 00324: val_loss did not improve from 229.00464\n","Epoch 325/500\n","12/12 [==============================] - 13s 1s/step - loss: 11.7997 - val_loss: 297.7385\n","\n","Epoch 00325: val_loss did not improve from 229.00464\n","Epoch 326/500\n","12/12 [==============================] - 13s 1s/step - loss: 20.9042 - val_loss: 299.3223\n","\n","Epoch 00326: val_loss did not improve from 229.00464\n","Epoch 327/500\n","12/12 [==============================] - 13s 1s/step - loss: 31.5469 - val_loss: 355.2691\n","\n","Epoch 00327: val_loss did not improve from 229.00464\n","Epoch 328/500\n","12/12 [==============================] - 13s 1s/step - loss: 33.4868 - val_loss: 279.6015\n","\n","Epoch 00328: val_loss did not improve from 229.00464\n","Epoch 329/500\n","12/12 [==============================] - 13s 1s/step - loss: 30.7936 - val_loss: 296.3853\n","\n","Epoch 00329: val_loss did not improve from 229.00464\n","Epoch 330/500\n","12/12 [==============================] - 13s 1s/step - loss: 23.5738 - val_loss: 298.4108\n","\n","Epoch 00330: val_loss did not improve from 229.00464\n","Epoch 331/500\n","12/12 [==============================] - 13s 1s/step - loss: 18.8597 - val_loss: 318.4507\n","\n","Epoch 00331: val_loss did not improve from 229.00464\n","Epoch 332/500\n","12/12 [==============================] - 13s 1s/step - loss: 13.4856 - val_loss: 292.5039\n","\n","Epoch 00332: val_loss did not improve from 229.00464\n","Epoch 333/500\n","12/12 [==============================] - 13s 1s/step - loss: 9.4499 - val_loss: 297.1277\n","\n","Epoch 00333: val_loss did not improve from 229.00464\n","Epoch 334/500\n","12/12 [==============================] - 13s 1s/step - loss: 6.8986 - val_loss: 305.2322\n","\n","Epoch 00334: val_loss did not improve from 229.00464\n","Epoch 335/500\n","12/12 [==============================] - 13s 1s/step - loss: 5.5915 - val_loss: 298.2105\n","\n","Epoch 00335: val_loss did not improve from 229.00464\n","Epoch 336/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.5967 - val_loss: 299.6521\n","\n","Epoch 00336: val_loss did not improve from 229.00464\n","Epoch 337/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.2235 - val_loss: 299.6420\n","\n","Epoch 00337: val_loss did not improve from 229.00464\n","Epoch 338/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.8824 - val_loss: 296.1831\n","\n","Epoch 00338: val_loss did not improve from 229.00464\n","Epoch 339/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.6788 - val_loss: 298.1630\n","\n","Epoch 00339: val_loss did not improve from 229.00464\n","Epoch 340/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.5286 - val_loss: 297.6644\n","\n","Epoch 00340: val_loss did not improve from 229.00464\n","Epoch 341/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.4251 - val_loss: 297.8834\n","\n","Epoch 00341: val_loss did not improve from 229.00464\n","Epoch 342/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.3393 - val_loss: 295.0617\n","\n","Epoch 00342: val_loss did not improve from 229.00464\n","Epoch 343/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.2361 - val_loss: 295.8203\n","\n","Epoch 00343: val_loss did not improve from 229.00464\n","Epoch 344/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.1686 - val_loss: 295.1772\n","\n","Epoch 00344: val_loss did not improve from 229.00464\n","Epoch 345/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.0588 - val_loss: 296.3910\n","\n","Epoch 00345: val_loss did not improve from 229.00464\n","Epoch 346/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.0468 - val_loss: 295.7948\n","\n","Epoch 00346: val_loss did not improve from 229.00464\n","Epoch 347/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.9793 - val_loss: 294.0600\n","\n","Epoch 00347: val_loss did not improve from 229.00464\n","Epoch 348/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.9274 - val_loss: 291.3115\n","\n","Epoch 00348: val_loss did not improve from 229.00464\n","Epoch 349/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.8721 - val_loss: 291.4815\n","\n","Epoch 00349: val_loss did not improve from 229.00464\n","Epoch 350/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.8435 - val_loss: 292.2034\n","\n","Epoch 00350: val_loss did not improve from 229.00464\n","Epoch 351/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.8130 - val_loss: 293.5656\n","\n","Epoch 00351: val_loss did not improve from 229.00464\n","Epoch 352/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.7697 - val_loss: 290.1077\n","\n","Epoch 00352: val_loss did not improve from 229.00464\n","Epoch 353/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.7183 - val_loss: 288.1725\n","\n","Epoch 00353: val_loss did not improve from 229.00464\n","Epoch 354/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.7119 - val_loss: 289.2776\n","\n","Epoch 00354: val_loss did not improve from 229.00464\n","Epoch 355/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6882 - val_loss: 286.4230\n","\n","Epoch 00355: val_loss did not improve from 229.00464\n","Epoch 356/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6553 - val_loss: 285.8640\n","\n","Epoch 00356: val_loss did not improve from 229.00464\n","Epoch 357/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6389 - val_loss: 284.6130\n","\n","Epoch 00357: val_loss did not improve from 229.00464\n","Epoch 358/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6514 - val_loss: 285.8078\n","\n","Epoch 00358: val_loss did not improve from 229.00464\n","Epoch 359/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6011 - val_loss: 285.8602\n","\n","Epoch 00359: val_loss did not improve from 229.00464\n","Epoch 360/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6025 - val_loss: 286.8541\n","\n","Epoch 00360: val_loss did not improve from 229.00464\n","Epoch 361/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.5739 - val_loss: 284.0860\n","\n","Epoch 00361: val_loss did not improve from 229.00464\n","Epoch 362/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.5515 - val_loss: 282.6200\n","\n","Epoch 00362: val_loss did not improve from 229.00464\n","Epoch 363/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.5316 - val_loss: 282.4000\n","\n","Epoch 00363: val_loss did not improve from 229.00464\n","Epoch 364/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.5154 - val_loss: 281.8973\n","\n","Epoch 00364: val_loss did not improve from 229.00464\n","Epoch 365/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.5052 - val_loss: 282.3864\n","\n","Epoch 00365: val_loss did not improve from 229.00464\n","Epoch 366/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.4753 - val_loss: 282.2254\n","\n","Epoch 00366: val_loss did not improve from 229.00464\n","Epoch 367/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.4645 - val_loss: 282.3987\n","\n","Epoch 00367: val_loss did not improve from 229.00464\n","Epoch 368/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.4506 - val_loss: 280.6043\n","\n","Epoch 00368: val_loss did not improve from 229.00464\n","Epoch 369/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.4400 - val_loss: 279.0967\n","\n","Epoch 00369: val_loss did not improve from 229.00464\n","Epoch 370/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.4146 - val_loss: 278.8586\n","\n","Epoch 00370: val_loss did not improve from 229.00464\n","Epoch 371/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.4138 - val_loss: 278.8930\n","\n","Epoch 00371: val_loss did not improve from 229.00464\n","Epoch 372/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.4025 - val_loss: 278.6215\n","\n","Epoch 00372: val_loss did not improve from 229.00464\n","Epoch 373/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.3722 - val_loss: 278.5136\n","\n","Epoch 00373: val_loss did not improve from 229.00464\n","Epoch 374/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.3644 - val_loss: 277.9278\n","\n","Epoch 00374: val_loss did not improve from 229.00464\n","Epoch 375/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.3569 - val_loss: 277.9538\n","\n","Epoch 00375: val_loss did not improve from 229.00464\n","Epoch 376/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.3514 - val_loss: 277.3718\n","\n","Epoch 00376: val_loss did not improve from 229.00464\n","Epoch 377/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.3470 - val_loss: 277.4950\n","\n","Epoch 00377: val_loss did not improve from 229.00464\n","Epoch 378/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.3315 - val_loss: 276.8798\n","\n","Epoch 00378: val_loss did not improve from 229.00464\n","Epoch 379/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.3115 - val_loss: 276.5148\n","\n","Epoch 00379: val_loss did not improve from 229.00464\n","Epoch 380/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.3205 - val_loss: 277.2213\n","\n","Epoch 00380: val_loss did not improve from 229.00464\n","Epoch 381/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.3049 - val_loss: 277.2171\n","\n","Epoch 00381: val_loss did not improve from 229.00464\n","Epoch 382/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.3108 - val_loss: 276.1598\n","\n","Epoch 00382: val_loss did not improve from 229.00464\n","Epoch 383/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.2945 - val_loss: 276.1181\n","\n","Epoch 00383: val_loss did not improve from 229.00464\n","Epoch 384/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.2814 - val_loss: 275.8563\n","\n","Epoch 00384: val_loss did not improve from 229.00464\n","Epoch 385/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.2770 - val_loss: 275.9852\n","\n","Epoch 00385: val_loss did not improve from 229.00464\n","Epoch 386/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.2622 - val_loss: 276.0591\n","\n","Epoch 00386: val_loss did not improve from 229.00464\n","Epoch 387/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.2536 - val_loss: 275.6225\n","\n","Epoch 00387: val_loss did not improve from 229.00464\n","Epoch 388/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.2365 - val_loss: 275.7120\n","\n","Epoch 00388: val_loss did not improve from 229.00464\n","Epoch 389/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.2372 - val_loss: 273.7953\n","\n","Epoch 00389: val_loss did not improve from 229.00464\n","Epoch 390/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.2393 - val_loss: 274.0924\n","\n","Epoch 00390: val_loss did not improve from 229.00464\n","Epoch 391/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.2291 - val_loss: 274.8124\n","\n","Epoch 00391: val_loss did not improve from 229.00464\n","Epoch 392/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.2286 - val_loss: 274.9807\n","\n","Epoch 00392: val_loss did not improve from 229.00464\n","Epoch 393/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.2100 - val_loss: 275.3269\n","\n","Epoch 00393: val_loss did not improve from 229.00464\n","Epoch 394/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.2080 - val_loss: 274.9753\n","\n","Epoch 00394: val_loss did not improve from 229.00464\n","Epoch 395/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.1891 - val_loss: 274.5474\n","\n","Epoch 00395: val_loss did not improve from 229.00464\n","Epoch 396/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.1798 - val_loss: 273.9337\n","\n","Epoch 00396: val_loss did not improve from 229.00464\n","Epoch 397/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.1829 - val_loss: 273.9758\n","\n","Epoch 00397: val_loss did not improve from 229.00464\n","Epoch 398/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.1716 - val_loss: 275.0600\n","\n","Epoch 00398: val_loss did not improve from 229.00464\n","Epoch 399/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.1621 - val_loss: 274.9174\n","\n","Epoch 00399: val_loss did not improve from 229.00464\n","Epoch 400/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.1564 - val_loss: 274.6818\n","\n","Epoch 00400: val_loss did not improve from 229.00464\n","Epoch 401/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.1605 - val_loss: 274.0700\n","\n","Epoch 00401: val_loss did not improve from 229.00464\n","Epoch 402/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.1455 - val_loss: 273.7435\n","\n","Epoch 00402: val_loss did not improve from 229.00464\n","Epoch 403/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.1415 - val_loss: 273.9532\n","\n","Epoch 00403: val_loss did not improve from 229.00464\n","Epoch 404/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.1409 - val_loss: 273.9380\n","\n","Epoch 00404: val_loss did not improve from 229.00464\n","Epoch 405/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.1244 - val_loss: 273.2587\n","\n","Epoch 00405: val_loss did not improve from 229.00464\n","Epoch 406/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.1291 - val_loss: 272.9213\n","\n","Epoch 00406: val_loss did not improve from 229.00464\n","Epoch 407/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.1129 - val_loss: 274.2137\n","\n","Epoch 00407: val_loss did not improve from 229.00464\n","Epoch 408/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.1155 - val_loss: 274.8933\n","\n","Epoch 00408: val_loss did not improve from 229.00464\n","Epoch 409/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.1143 - val_loss: 274.5963\n","\n","Epoch 00409: val_loss did not improve from 229.00464\n","Epoch 410/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0963 - val_loss: 274.6978\n","\n","Epoch 00410: val_loss did not improve from 229.00464\n","Epoch 411/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0930 - val_loss: 274.3137\n","\n","Epoch 00411: val_loss did not improve from 229.00464\n","Epoch 412/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0852 - val_loss: 275.1956\n","\n","Epoch 00412: val_loss did not improve from 229.00464\n","Epoch 413/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0746 - val_loss: 275.6166\n","\n","Epoch 00413: val_loss did not improve from 229.00464\n","Epoch 414/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0755 - val_loss: 275.9586\n","\n","Epoch 00414: val_loss did not improve from 229.00464\n","Epoch 415/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0681 - val_loss: 276.0547\n","\n","Epoch 00415: val_loss did not improve from 229.00464\n","Epoch 416/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0639 - val_loss: 275.7617\n","\n","Epoch 00416: val_loss did not improve from 229.00464\n","Epoch 417/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0551 - val_loss: 275.6922\n","\n","Epoch 00417: val_loss did not improve from 229.00464\n","Epoch 418/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0557 - val_loss: 274.3521\n","\n","Epoch 00418: val_loss did not improve from 229.00464\n","Epoch 419/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0389 - val_loss: 272.9363\n","\n","Epoch 00419: val_loss did not improve from 229.00464\n","Epoch 420/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0427 - val_loss: 272.4693\n","\n","Epoch 00420: val_loss did not improve from 229.00464\n","Epoch 421/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0314 - val_loss: 272.9749\n","\n","Epoch 00421: val_loss did not improve from 229.00464\n","Epoch 422/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0257 - val_loss: 273.9774\n","\n","Epoch 00422: val_loss did not improve from 229.00464\n","Epoch 423/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0208 - val_loss: 275.3818\n","\n","Epoch 00423: val_loss did not improve from 229.00464\n","Epoch 424/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0207 - val_loss: 276.2065\n","\n","Epoch 00424: val_loss did not improve from 229.00464\n","Epoch 425/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0090 - val_loss: 276.4436\n","\n","Epoch 00425: val_loss did not improve from 229.00464\n","Epoch 426/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0093 - val_loss: 276.5310\n","\n","Epoch 00426: val_loss did not improve from 229.00464\n","Epoch 427/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.9969 - val_loss: 276.0887\n","\n","Epoch 00427: val_loss did not improve from 229.00464\n","Epoch 428/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.9895 - val_loss: 276.7076\n","\n","Epoch 00428: val_loss did not improve from 229.00464\n","Epoch 429/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.9979 - val_loss: 276.7971\n","\n","Epoch 00429: val_loss did not improve from 229.00464\n","Epoch 430/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.9988 - val_loss: 277.0207\n","\n","Epoch 00430: val_loss did not improve from 229.00464\n","Epoch 431/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.9847 - val_loss: 277.1879\n","\n","Epoch 00431: val_loss did not improve from 229.00464\n","Epoch 432/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.9894 - val_loss: 276.6279\n","\n","Epoch 00432: val_loss did not improve from 229.00464\n","Epoch 433/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.9797 - val_loss: 277.9046\n","\n","Epoch 00433: val_loss did not improve from 229.00464\n","Epoch 434/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0245 - val_loss: 279.3041\n","\n","Epoch 00434: val_loss did not improve from 229.00464\n","Epoch 435/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.0277 - val_loss: 280.1952\n","\n","Epoch 00435: val_loss did not improve from 229.00464\n","Epoch 436/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.9668 - val_loss: 280.4475\n","\n","Epoch 00436: val_loss did not improve from 229.00464\n","Epoch 437/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.9531 - val_loss: 279.8952\n","\n","Epoch 00437: val_loss did not improve from 229.00464\n","Epoch 438/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.9568 - val_loss: 280.1536\n","\n","Epoch 00438: val_loss did not improve from 229.00464\n","Epoch 439/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.9353 - val_loss: 281.3417\n","\n","Epoch 00439: val_loss did not improve from 229.00464\n","Epoch 440/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.9231 - val_loss: 282.1411\n","\n","Epoch 00440: val_loss did not improve from 229.00464\n","Epoch 441/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.9165 - val_loss: 282.6949\n","\n","Epoch 00441: val_loss did not improve from 229.00464\n","Epoch 442/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.9053 - val_loss: 282.9689\n","\n","Epoch 00442: val_loss did not improve from 229.00464\n","Epoch 443/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.8981 - val_loss: 282.8439\n","\n","Epoch 00443: val_loss did not improve from 229.00464\n","Epoch 444/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.9014 - val_loss: 284.5498\n","\n","Epoch 00444: val_loss did not improve from 229.00464\n","Epoch 445/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.9010 - val_loss: 287.5532\n","\n","Epoch 00445: val_loss did not improve from 229.00464\n","Epoch 446/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.8889 - val_loss: 288.6307\n","\n","Epoch 00446: val_loss did not improve from 229.00464\n","Epoch 447/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.8788 - val_loss: 287.0953\n","\n","Epoch 00447: val_loss did not improve from 229.00464\n","Epoch 448/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.8635 - val_loss: 287.6644\n","\n","Epoch 00448: val_loss did not improve from 229.00464\n","Epoch 449/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.8575 - val_loss: 288.4306\n","\n","Epoch 00449: val_loss did not improve from 229.00464\n","Epoch 450/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.8398 - val_loss: 288.6420\n","\n","Epoch 00450: val_loss did not improve from 229.00464\n","Epoch 451/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.8290 - val_loss: 289.6885\n","\n","Epoch 00451: val_loss did not improve from 229.00464\n","Epoch 452/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.8288 - val_loss: 291.5474\n","\n","Epoch 00452: val_loss did not improve from 229.00464\n","Epoch 453/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.8162 - val_loss: 293.4942\n","\n","Epoch 00453: val_loss did not improve from 229.00464\n","Epoch 454/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.8046 - val_loss: 294.7618\n","\n","Epoch 00454: val_loss did not improve from 229.00464\n","Epoch 455/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.7838 - val_loss: 296.7032\n","\n","Epoch 00455: val_loss did not improve from 229.00464\n","Epoch 456/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.7834 - val_loss: 299.1724\n","\n","Epoch 00456: val_loss did not improve from 229.00464\n","Epoch 457/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.7608 - val_loss: 302.0282\n","\n","Epoch 00457: val_loss did not improve from 229.00464\n","Epoch 458/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.7532 - val_loss: 304.0065\n","\n","Epoch 00458: val_loss did not improve from 229.00464\n","Epoch 459/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.7381 - val_loss: 303.1146\n","\n","Epoch 00459: val_loss did not improve from 229.00464\n","Epoch 460/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.7194 - val_loss: 305.8620\n","\n","Epoch 00460: val_loss did not improve from 229.00464\n","Epoch 461/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.7000 - val_loss: 313.0557\n","\n","Epoch 00461: val_loss did not improve from 229.00464\n","Epoch 462/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.6935 - val_loss: 316.2072\n","\n","Epoch 00462: val_loss did not improve from 229.00464\n","Epoch 463/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.6805 - val_loss: 316.7338\n","\n","Epoch 00463: val_loss did not improve from 229.00464\n","Epoch 464/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.6500 - val_loss: 315.1442\n","\n","Epoch 00464: val_loss did not improve from 229.00464\n","Epoch 465/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.6291 - val_loss: 319.7108\n","\n","Epoch 00465: val_loss did not improve from 229.00464\n","Epoch 466/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.6052 - val_loss: 323.0128\n","\n","Epoch 00466: val_loss did not improve from 229.00464\n","Epoch 467/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.5779 - val_loss: 323.9590\n","\n","Epoch 00467: val_loss did not improve from 229.00464\n","Epoch 468/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.5641 - val_loss: 320.6501\n","\n","Epoch 00468: val_loss did not improve from 229.00464\n","Epoch 469/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.5028 - val_loss: 325.7322\n","\n","Epoch 00469: val_loss did not improve from 229.00464\n","Epoch 470/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.4659 - val_loss: 330.2464\n","\n","Epoch 00470: val_loss did not improve from 229.00464\n","Epoch 471/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.4164 - val_loss: 339.2360\n","\n","Epoch 00471: val_loss did not improve from 229.00464\n","Epoch 472/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.3849 - val_loss: 343.8900\n","\n","Epoch 00472: val_loss did not improve from 229.00464\n","Epoch 473/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.3052 - val_loss: 354.9320\n","\n","Epoch 00473: val_loss did not improve from 229.00464\n","Epoch 474/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.3195 - val_loss: 336.4232\n","\n","Epoch 00474: val_loss did not improve from 229.00464\n","Epoch 475/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.3669 - val_loss: 337.7345\n","\n","Epoch 00475: val_loss did not improve from 229.00464\n","Epoch 476/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.4085 - val_loss: 346.3434\n","\n","Epoch 00476: val_loss did not improve from 229.00464\n","Epoch 477/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.2498 - val_loss: 350.5024\n","\n","Epoch 00477: val_loss did not improve from 229.00464\n","Epoch 478/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.3543 - val_loss: 353.4442\n","\n","Epoch 00478: val_loss did not improve from 229.00464\n","Epoch 479/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.3005 - val_loss: 335.0939\n","\n","Epoch 00479: val_loss did not improve from 229.00464\n","Epoch 480/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.1858 - val_loss: 346.1895\n","\n","Epoch 00480: val_loss did not improve from 229.00464\n","Epoch 481/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.2368 - val_loss: 346.1343\n","\n","Epoch 00481: val_loss did not improve from 229.00464\n","Epoch 482/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.0678 - val_loss: 339.5093\n","\n","Epoch 00482: val_loss did not improve from 229.00464\n","Epoch 483/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.1706 - val_loss: 347.4343\n","\n","Epoch 00483: val_loss did not improve from 229.00464\n","Epoch 484/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.4426 - val_loss: 332.5260\n","\n","Epoch 00484: val_loss did not improve from 229.00464\n","Epoch 485/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.7276 - val_loss: 353.1285\n","\n","Epoch 00485: val_loss did not improve from 229.00464\n","Epoch 486/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.2387 - val_loss: 372.8518\n","\n","Epoch 00486: val_loss did not improve from 229.00464\n","Epoch 487/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.0264 - val_loss: 345.5206\n","\n","Epoch 00487: val_loss did not improve from 229.00464\n","Epoch 488/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.2293 - val_loss: 362.8276\n","\n","Epoch 00488: val_loss did not improve from 229.00464\n","Epoch 489/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.9107 - val_loss: 376.7189\n","\n","Epoch 00489: val_loss did not improve from 229.00464\n","Epoch 490/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.6128 - val_loss: 418.0134\n","\n","Epoch 00490: val_loss did not improve from 229.00464\n","Epoch 491/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.8515 - val_loss: 390.9590\n","\n","Epoch 00491: val_loss did not improve from 229.00464\n","Epoch 492/500\n","12/12 [==============================] - 13s 1s/step - loss: 5.2093 - val_loss: 444.6477\n","\n","Epoch 00492: val_loss did not improve from 229.00464\n","Epoch 493/500\n","12/12 [==============================] - 13s 1s/step - loss: 5.3990 - val_loss: 410.9374\n","\n","Epoch 00493: val_loss did not improve from 229.00464\n","Epoch 494/500\n","12/12 [==============================] - 13s 1s/step - loss: 5.5348 - val_loss: 369.5976\n","\n","Epoch 00494: val_loss did not improve from 229.00464\n","Epoch 495/500\n","12/12 [==============================] - 13s 1s/step - loss: 4.5808 - val_loss: 389.5099\n","\n","Epoch 00495: val_loss did not improve from 229.00464\n","Epoch 496/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.9126 - val_loss: 394.3335\n","\n","Epoch 00496: val_loss did not improve from 229.00464\n","Epoch 497/500\n","12/12 [==============================] - 13s 1s/step - loss: 3.1083 - val_loss: 397.1644\n","\n","Epoch 00497: val_loss did not improve from 229.00464\n","Epoch 498/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.6099 - val_loss: 383.6825\n","\n","Epoch 00498: val_loss did not improve from 229.00464\n","Epoch 499/500\n","12/12 [==============================] - 13s 1s/step - loss: 2.2906 - val_loss: 386.8438\n","\n","Epoch 00499: val_loss did not improve from 229.00464\n","Epoch 500/500\n","12/12 [==============================] - 13s 1s/step - loss: 1.9051 - val_loss: 388.1605\n","\n","Epoch 00500: val_loss did not improve from 229.00464\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KlHsiifdy5uv","colab":{"base_uri":"https://localhost:8080/","height":598},"executionInfo":{"status":"ok","timestamp":1614692805767,"user_tz":-360,"elapsed":3039,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"960bf945-6dcc-4ea6-fc93-eb42d62e09d5"},"source":["# Get the prediction model by extracting layers till the output layer\r\n","prediction_model = keras.models.Model(\r\n","    model.get_layer(name=\"image\").input, model.get_layer(name=\"dense3\").output\r\n",")\r\n","prediction_model.summary()\r\n","\r\n","# A utility function to decode the output of the network\r\n","def decode_batch_predictions(pred):\r\n","    input_len = np.ones(pred.shape[0]) * pred.shape[1]\r\n","    # Use greedy search. For complex tasks, you can use beam search\r\n","    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=False, beam_width=100)[0][0][\r\n","        :, :max_length\r\n","    ]\r\n","    # Iterate over the results and get back the text\r\n","    output_text = []\r\n","    for res in results:\r\n","        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\r\n","        output_text.append(res)\r\n","    return output_text\r\n","\r\n","for batch in validation_dataset.take(1):\r\n","    batch_images = batch[\"image\"]\r\n","    batch_labels = batch[\"label\"]\r\n","\r\n","    preds = prediction_model.predict(batch_images)\r\n","    pred_texts = decode_batch_predictions(preds)\r\n","    print(pred_texts)\r\n","    for i in range(1):\r\n","        img = (batch_images[i] * 255).numpy().astype(\"uint8\")\r\n","        # print(img.shape) # (200, 50, 1)\r\n","        plt.imshow(img[:, :, 0].T, cmap=\"gray\")\r\n","        plt.axis(\"off\")\r\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","image (InputLayer)           [(None, 2452, 144, 1)]    0         \n","_________________________________________________________________\n","Conv1 (Conv2D)               (None, 2452, 144, 32)     320       \n","_________________________________________________________________\n","pool1 (MaxPooling2D)         (None, 1226, 72, 32)      0         \n","_________________________________________________________________\n","Conv2 (Conv2D)               (None, 1226, 72, 64)      18496     \n","_________________________________________________________________\n","pool2 (MaxPooling2D)         (None, 613, 36, 64)       0         \n","_________________________________________________________________\n","reshape (Reshape)            (None, 613, 2304)         0         \n","_________________________________________________________________\n","dense1 (Dense)               (None, 613, 512)          1180160   \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 613, 512)          0         \n","_________________________________________________________________\n","bidirectional_12 (Bidirectio (None, 613, 1024)         4198400   \n","_________________________________________________________________\n","bidirectional_13 (Bidirectio (None, 613, 1024)         6295552   \n","_________________________________________________________________\n","dense2 (Dense)               (None, 613, 99)           101475    \n","=================================================================\n","Total params: 11,794,403\n","Trainable params: 11,794,403\n","Non-trainable params: 0\n","_________________________________________________________________\n","[' [UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]']\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"-gRufb3jy5uv","colab":{"base_uri":"https://localhost:8080/","height":598},"executionInfo":{"status":"ok","timestamp":1615458751432,"user_tz":-360,"elapsed":3530,"user":{"displayName":"Townim Faisal Chowdhury","photoUrl":"","userId":"17782929784728513740"}},"outputId":"a02f9a4a-f881-4993-b9ce-95901e837dc9"},"source":["test_datafile = open('./Data/ICBOCR-D4/custom-images/Groundtruth.txt', encoding='utf8')\r\n","test_lines = [line.rstrip() for line in test_datafile]\r\n","\r\n","test_image_dir = \"./Data/ICBOCR-D4/custom-images/Line_images/\"\r\n","test_image_paths = [os.path.join(test_image_dir, line.split('@')[0]) for line in test_lines]\r\n","test_captions = [line.split('@')[1].lstrip() for line in test_lines]\r\n","\r\n","\r\n","x_test, _, y_test, _ = split_data(np.array(test_image_paths), np.array(test_captions), train_size=1.0, shuffle=True)\r\n","\r\n","batch_size = 1\r\n","test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\r\n","test_dataset = test_dataset.map(\r\n","        encode_single_sample, num_parallel_calls=tf.data.experimental.AUTOTUNE\r\n","    ).batch(batch_size)#.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n","\r\n","# Get the prediction model by extracting layers till the output layer\r\n","prediction_model = keras.models.Model(\r\n","    model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output\r\n",")\r\n","prediction_model.summary()\r\n","\r\n","# A utility function to decode the output of the network\r\n","def decode_batch_predictions(pred):\r\n","    input_len = np.ones(pred.shape[0]) * pred.shape[1]\r\n","    # Use greedy search. For complex tasks, you can use beam search\r\n","    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=False, beam_width=100)[0][0][\r\n","        :, :max_length\r\n","    ]\r\n","    # Iterate over the results and get back the text\r\n","    output_text = []\r\n","    for res in results:\r\n","        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\r\n","        output_text.append(res)\r\n","    return output_text\r\n","\r\n","for batch in test_dataset.take(1):\r\n","    batch_images = batch[\"image\"]\r\n","    batch_labels = batch[\"label\"]\r\n","\r\n","    preds = prediction_model.predict(batch_images)\r\n","    pred_texts = decode_batch_predictions(preds)\r\n","    print(pred_texts)\r\n","    for i in range(1):\r\n","        img = (batch_images[i] * 255).numpy().astype(\"uint8\")\r\n","        # print(img.shape) # (200, 50, 1)\r\n","        plt.imshow(img[:, :, 0].T, cmap=\"gray\")\r\n","        plt.axis(\"off\")\r\n","plt.show()\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","image (InputLayer)           [(None, 2452, 144, 1)]    0         \n","_________________________________________________________________\n","Conv1 (Conv2D)               (None, 2452, 144, 32)     320       \n","_________________________________________________________________\n","pool1 (MaxPooling2D)         (None, 1226, 72, 32)      0         \n","_________________________________________________________________\n","Conv2 (Conv2D)               (None, 1226, 72, 64)      18496     \n","_________________________________________________________________\n","pool2 (MaxPooling2D)         (None, 613, 36, 64)       0         \n","_________________________________________________________________\n","reshape (Reshape)            (None, 613, 2304)         0         \n","_________________________________________________________________\n","dense1 (Dense)               (None, 613, 512)          1180160   \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 613, 512)          0         \n","_________________________________________________________________\n","bidirectional_2 (Bidirection (None, 613, 1024)         4198400   \n","_________________________________________________________________\n","bidirectional_3 (Bidirection (None, 613, 1024)         6295552   \n","_________________________________________________________________\n","dense2 (Dense)               (None, 613, 103)          105575    \n","=================================================================\n","Total params: 11,798,503\n","Trainable params: 11,798,503\n","Non-trainable params: 0\n","_________________________________________________________________\n","['খন\"[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]']\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"Pte4vaRCy5uv"},"source":[""],"execution_count":null,"outputs":[]}]}